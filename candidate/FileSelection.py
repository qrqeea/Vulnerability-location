import os
import tqdm
import difflib
from util.io_util import load_pickle, save_text, save_json, save_pickle, load_json
from util.general_util import rule_based_filtering
from util.similarity_util import *


class FileSelection:

    def __init__(self, module_root_path: str, cve_data_all: dict, repo_file_list: dict):
        self.module_root_path = module_root_path
        self.cve_data_all = cve_data_all
        self.repo_file_list = repo_file_list

        os.makedirs(self.module_root_path, exist_ok = True)

        cve_list_path = f'{self.module_root_path}/cve_list'
        if os.path.exists(f'{cve_list_path}.pkl'):
            self.cve_list = load_pickle(f'{cve_list_path}.pkl')
        else:
            self.cve_list = [
                cve for cve in self.cve_data_all if
                'collected_commit' in self.cve_data_all[cve] and
                'File' in self.cve_data_all[cve]['components']
            ]
            save_text(cve_list_path, self.cve_list)
            save_pickle(f'{cve_list_path}.pkl', self.cve_list)
        print(len(self.cve_list))    # 4886


    def retrieve_by_file_component(self, k: int, similarity_threshold: float, similarity_algorithm = difflib.SequenceMatcher):
        
        os.makedirs(f'{self.module_root_path}/candidate_list', exist_ok = True)
        res_path = f'{self.module_root_path}/candidate_list/candidates_{len(self.cve_list)}_{k}_{similarity_threshold}_{similarity_algorithm.__name__}.json'
        if os.path.exists(res_path):
            return load_json(res_path)
        
        candidates_100_path = f'{self.module_root_path}/candidate_list/candidates_{len(self.cve_list)}_100_{similarity_algorithm.__name__}.json'
        
        candidates_100 = load_json(candidates_100_path) if os.path.exists(candidates_100_path) else {}
        updated = False
        for cve in tqdm.tqdm(self.cve_list):
            if cve in candidates_100:
                continue
            file_components = self.cve_data_all[cve].get('components').get('File')
            collected_commit = self.cve_data_all[cve].get('collected_commit')

            candidates_100[cve] = {}
            for repo, commit in collected_commit:
                repo_file_list = self.repo_file_list[repo][commit]
                related_files = self.find_related_files(file_components, repo_file_list, 100, similarity_algorithm)
                candidates_100[cve][repo] = related_files
            updated = True
        if updated:
            save_json(candidates_100_path, candidates_100)
        
        print('start reduce data')
        multi_repo_cnt = 0
        multi_candidate_cnt= 0
        reduced_data = {}
        for cve, v in candidates_100.items():
            # if not self.cve_data_all[cve]['collected_repo_correction']: continue
            # if not self.cve_data_all[cve]['collected_commit_correction']: continue
            reduced_data[cve] = {}
            if len(v) > 1:
                multi_repo_cnt += 1
            flag = False
            for repo, triple_list in v.items():
                tp = []
                for index, triple in enumerate(triple_list):
                    if index >= k and triple[2] != triple_list[index - 1][2]:
                        if index > 1:
                            flag = True
                        break
                    if triple[2] < similarity_threshold:    # 相似度低于阈值大概率是组件提取不对或repo、commit有误
                        break
                    tp.append(triple[1])
                if tp:
                    reduced_data[cve][repo] = tp
            if flag:
                multi_candidate_cnt += 1
            if not reduced_data[cve]:
                del reduced_data[cve]
        
        print(f'{multi_repo_cnt} cve has multi repo')
        print(f'{multi_candidate_cnt} cve has multi candidate')
        
        save_json(res_path, reduced_data)
        
        print('end reduce data')
        return reduced_data


    def find_related_files(self, keywords: list, files: list, k: int, similarity_algorithm):
        # components中包含'/'则匹配完整路径，否则只匹配文件名
        res = []
        for keyword in keywords:
            for full_path, isdir in files:
                if isdir: continue
                file_lower = full_path.lower()
                file_name = file_lower.split('/')[-1]
                file_path = '/'.join(file_lower.split('/')[:-1])
                if not rule_based_filtering(file_name, file_path):
                    continue

                keyword_lower = keyword.lower()
                if '/' not in keyword:
                    keyword_lower = keyword_lower.split('/')[-1]
                    file_lower = file_name
                
                if similarity_algorithm.__name__ == 'SequenceMatcher':
                    similarity = difflib.SequenceMatcher(None, keyword_lower, file_lower).ratio()
                elif similarity_algorithm.__name__ == 'ngram_similarity':
                    similarity = ngram_similarity(keyword_lower, file_lower, 2)
                else:
                    similarity = similarity_algorithm(keyword_lower, file_lower)
                res.append((keyword, full_path, similarity))
        sorted_res = sorted(res, key = lambda x: x[2], reverse = (similarity_algorithm.__name__ != 'levenshtein_distance'))
        # sorted_res = sorted(res, key = lambda x: x[2])
        return sorted_res[:k]
    

    # def generate_manual_check_cve_list(self):
    #     cve_list = set(load_pickle(f'{self.module_root_path}/incorrrect_cve_list_488.pkl')) & set(load_pickle(f'{self.module_root_path}/incorrrect_cve_list_462.pkl'))
    #     print(len(cve_list))      # 252
        
    #     df_new = pd.DataFrame({
    #         'cve': [],
    #         'cve url': [],
    #         'cnadidate': [],
    #         'ans': [],
    #         'component': [],
    #         'component in original': [],
    #         'component in complete': [],
    #         'reason': [],
    #         # 'original_description': [],
    #         # 'complete_description': [],
    #     })
    #     candidates = load_json(f'{self.candidate_list_dir}/candidates_4423_1_0_SequenceMatcher.json')
    #     df = pd.read_csv(f'{self.module_root_path}/file_incorrect_reason.csv')
    #     for cve in tqdm.tqdm(cve_list):
    #         reason = 'TODO'
    #         tp = df[df['cve'] == cve]
    #         if not tp.empty and tp.reason.values[0] != 'TODO':
    #             reason = tp.reason.values[0]
    #         df_new.loc[len(df_new)] = [
    #             cve,
    #             'https://nvd.nist.gov/vuln/detail/' + cve,
    #             ',  '.join([file for v in candidates[cve].values() for file in v]),
    #             ',  '.join(self.cve_data_all[cve]['file_list']),
    #             ',  '.join(self.cve_data_all[cve]['components']['File']),
    #             any(item in self.cve_data_all[cve]['original_description'] for item in self.cve_data_all[cve]['components']['File']),
    #             any(item in self.cve_data_all[cve]['complete_description'] for item in self.cve_data_all[cve]['components']['File']) if 'complete_description' in self.cve_data_all[cve] else '',
    #             reason,
    #             # self.cve_data_all[cve]['original_description'],
    #             # self.cve_data_all[cve]['complete_description'] if 'complete_description' in self.cve_data_all[cve] else ''
    #         ]
    #     df_new = df_new.sort_values(by = 'reason').reset_index(drop = True)
    #     # df_new = df_new.sort_values('', ascending = False)
    #     df_new.to_csv(
    #         f'{self.module_root_path}/file_incorrect_reason_new.csv',
    #         index = True, 
    #         quotechar = '"',
    #         quoting = csv.QUOTE_ALL
    #     )