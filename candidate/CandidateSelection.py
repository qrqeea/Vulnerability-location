import os
import sys
import csv
import tqdm
import random
import difflib
import pandas as pd
from util.io_util import *
from util.similarity_util import *
from util.github_util import github_tokens, get_file_content, get_file_list

class CandidateSelection:

    def __init__(self, project_root_path: str, module_root_path: str, gt_content_path: str, cve_data_all: dict, repo_file_list: dict):
        self.project_root_path = project_root_path
        self.module_root_path = module_root_path
        self.gt_content_path = gt_content_path
        self.cve_data_all = cve_data_all
        self.repo_file_list = repo_file_list

        # 文件名相同
        self.filter_file = [
            'changelog', 'news', 'changes', 'version', 'readme', 'license', 'authors', 'todo', 'history', 'copying', 'relnotes', 'thanks', 'notice', 'whatsnew', 'notes', 'release_notes', 'testlist', 'testsuite', 'test'
        ]
        # 以这些后缀结尾的文件
        self.filter_suffix = [
            '.md', '.txt', '.docx', '.pdf', '.rst', '.changes', '.rdoc', '.mdown',
            '.command', '.out', '.err', '.stderr', '.stdout', '.test',
            '.jpg', '.png', '.svg', '.mp4', '.gif', '.exr',
            '.csv', '.rdf',
            '.ttf', '.otf', '.woff', '.woff2',
            '.mock', '.stub', '.fake',
            '.pptx', '.key',
            '.bak', '.zip', '.gz', '.rar',
            '.gitignore',
            '.lib', '.jpeg', '.ppt', '.xlsx', '.xls', '.doc' '.ico', '.bmp', '.tar.gz', '.tgz', '.css', '.cygport'
        ]
        # 路径中包含
        self.filter_path = [
            'note', 'license', 'test'
        ]

        os.makedirs(self.module_root_path, exist_ok = True)

        self.file_component_dir = f'{self.module_root_path}/file_component'
        os.makedirs(self.file_component_dir, exist_ok = True)

        self.codebert_dir = f'{self.module_root_path}/codebert'
        os.makedirs(self.codebert_dir, exist_ok = True)


    def start(self):
        res = self.select_by_file_component(1, 0.6)
        # self.check_accuracy_by_repo_and_file_name(res)
        self.check_accuracy(res)
        # self.generate_manual_check_cve_list()

        # rest_cve_list = {cve for cve, v in self.cve_data_all.items() if 'collected_commit' in v} - set(res.keys())
        # print(f'{len(rest_cve_list)} enter CodeBERT')   # 8338 - 4146 = 4192
        # save_text(f'{self.module_root_path}/cve_list/cve_list_codebert', list(rest_cve_list))
        # save_pickle(f'{self.module_root_path}/cve_list/cve_list_codebert.pkl', list(rest_cve_list))


    def select_by_file_component(self, k: int, similarity_threshold: float, cve_list: list = None, similarity_algorithm = difflib.SequenceMatcher):
        if not cve_list:
            cve_list_path = f'{self.file_component_dir}/cve_list'
            if not os.path.exists(f'{cve_list_path}.pkl'):
                cve_list = [
                    cve for cve in self.cve_data_all if
                    'collected_commit' in self.cve_data_all[cve] and
                    'components' in self.cve_data_all[cve] and
                    'File' in self.cve_data_all[cve]['components']
                ]
                save_text(f'{cve_list_path}', cve_list)
                save_pickle(f'{cve_list_path}.pkl', cve_list)
            else:
                cve_list = load_pickle(f'{cve_list_path}.pkl')
            # print(len(cve_list))    # 4375
        
        os.makedirs(f'{self.file_component_dir}/candidate_list', exist_ok = True)
        res_path = f'{self.file_component_dir}/candidate_list/candidates_{len(cve_list)}_{k}_{similarity_threshold}_{similarity_algorithm.__name__}.json'
        if os.path.exists(res_path):
            return load_json(res_path)
        
        candidates_100_path = f'{self.file_component_dir}/candidate_list/candidates_{len(cve_list)}_100_{similarity_algorithm.__name__}.json'
        if os.path.exists(candidates_100_path):
            candidates_100 = load_json(candidates_100_path)
        else:
            candidates_100 = {}
            for cve in tqdm.tqdm(cve_list):
            # for cve in random.sample(cve_list, 50):
                file_components = self.cve_data_all[cve].get('components').get('File')
                collected_commit = self.cve_data_all[cve].get('collected_commit')

                candidates_100[cve] = {}
                for repo, commit in collected_commit:
                    repo_file_list = self.repo_file_list[repo][commit]
                    related_files = self.find_related_files(file_components, repo_file_list, 100, similarity_algorithm)
                    candidates_100[cve][repo] = related_files
            save_json(candidates_100_path, candidates_100)
        
        print('start reduce data')
        multi_repo_cnt = 0
        multi_candidate_cnt= 0
        reduced_data = {}
        for cve, v in candidates_100.items():
            # if not self.cve_data_all[cve]['collected_repo_correction']: continue
            # if not self.cve_data_all[cve]['collected_commit_correction']: continue
            reduced_data[cve] = {}
            if len(v) > 1:
                multi_repo_cnt += 1
            flag = False
            for repo, triple_list in v.items():
                tp = []
                for index, triple in enumerate(triple_list):
                    if index >= k and triple[2] != triple_list[index - 1][2]:
                        if index > 1:
                            flag = True
                        break
                    if triple[2] < similarity_threshold:    # 相似度低于阈值大概率是组件提取不对或repo、commit有误
                        break
                    tp.append(triple[1])
                if tp:
                    reduced_data[cve][repo] = tp
            if flag:
                multi_candidate_cnt += 1
            if not reduced_data[cve]:
                del reduced_data[cve]
        
        print(f'{multi_repo_cnt} cve has multi repo')
        print(f'{multi_candidate_cnt} cve has multi candidate')
        
        save_json(res_path, reduced_data)
        
        print('end reduce data')
        return reduced_data
    
    
    
    def check_accuracy_by_repo_and_file_name(self, data: dict):
        print('start check by repo and file name')
        
        incorrect_cve_list = []
        incorrect_cve_list_strict = []
        for cve, v in data.items():
            flag = False
            flag_strict = False
            for repo_candidate, file_candidates in v.items():
                if flag and flag_strict:
                    break
                for repo_ans, files_ans in self.cve_data_all[cve]['vulnerability_files'].items():
                    if repo_candidate.lower() == repo_ans.lower() and any(
                        file_ans.lower() == file_candidate.lower()
                        for file_candidate in file_candidates
                        for file_ans in files_ans
                    ):
                        flag_strict = True
                    if any(
                        file_ans.lower() in file_candidate.lower() or file_candidate.lower() in file_ans.lower() for file_candidate in file_candidates
                        for file_ans in files_ans
                    ):
                        flag = True
            if not flag:
                incorrect_cve_list.append(cve)
            if not flag_strict:
                incorrect_cve_list_strict.append(cve)
        
        total_count = len(data)
        correct_cnt = total_count - len(incorrect_cve_list)
        correct_cnt_strict = total_count - len(incorrect_cve_list_strict)
        print('{:.2f}%'.format(correct_cnt / total_count * 100), f'({correct_cnt}/{total_count})')
        print('strict: {:.2f}%'.format(correct_cnt_strict / total_count * 100), f'({correct_cnt_strict}/{total_count})')
        
        print('end check by repo and file name')
        return incorrect_cve_list
        # return incorrect_cve_list_strict


    def check_accuracy(self, data: dict):
        print('start check accuracy')
        incorrect_cve_list = self.check_accuracy_by_repo_and_file_name(data)
        
        print('start generate to_scrapy_list')
        to_scrapy_list = []
        for cve in incorrect_cve_list:
            for repo_candidate, file_candidates in data[cve].items():
                sha = list(filter(lambda x: x[0] == repo_candidate, self.cve_data_all[cve]['collected_commit']))[0][1]
                if not sha:
                    print('error, sha')
                    sys.exit()
                for file_candidate in file_candidates:
                    to_scrapy_list.append((cve, repo_candidate, sha, file_candidate))
        print(f'end generate to_scrapy_list, size: {len(to_scrapy_list)}')

        print('start scrapy to_scrapy_list')
        os.makedirs(f'{self.file_component_dir}/candidate_content', exist_ok = True)
        
        def get_candidate_content(to_scrapy_list_sub: list, token: str):
            for cve, repo, sha, file in tqdm.tqdm(to_scrapy_list_sub):
                dir = f'{self.file_component_dir}/candidate_content/{cve}'
                tp1 = repo.replace('/', '-')
                tp2 = file.replace('/', '-')
                save_path = f'{dir}/{tp1}_{tp2}'
                if os.path.exists(save_path):
                    continue
                res = get_file_content(repo, sha, file, token)
                if res:
                    os.makedirs(dir, exist_ok = True)
                    save_text(save_path, res)
        
        multi_thread(to_scrapy_list, get_candidate_content, tokens = github_tokens)
        print(f'end scrapy to_scrapy_list')
        
        print('start check by content')
        gt_not_found_list = []
        candidate_not_found_list = []
        
        similarity_table_path = f'{self.module_root_path}/similarity_table'
        if os.path.exists(f'{similarity_table_path}.pkl'):
            similarity_table = load_pickle(f'{similarity_table_path}.pkl')
        else:
            similarity_table = {}
        
        for cve in tqdm.tqdm(incorrect_cve_list.copy()):
            if cve not in similarity_table:
                gt_content_path = f'{self.gt_content_path}/{cve}'
                file_content_path = f'{self.file_component_dir}/candidate_content/{cve}'
                if not os.path.exists(gt_content_path):
                    gt_not_found_list.append(cve)
                    continue
                if not os.path.exists(file_content_path):
                    # TODO 处理一下
                    # print(f'error, {cve} content not found')
                    # 这里大概是因为经过启发式规则筛选后没有候选文件，如CVE-2019-8424
                    # 还有是因为候选文件不是文本文件，如CVE-2018-19504
                    candidate_not_found_list.append(cve)
                    continue
                file_ans_content = [
                    load_file(f'{gt_content_path}/{file}')
                    for file in os.listdir(gt_content_path) if file not in ['.DS_Store']
                ]
                file_candidate_content = [
                    load_file(f'{file_content_path}/{file}')
                    for file in os.listdir(file_content_path) if file not in ['.DS_Store']
                ]
                max_simi = 0
                for candidate_content in file_candidate_content:
                    for gt_content in file_ans_content:
                        simi = difflib.SequenceMatcher(None, candidate_content, gt_content).ratio()
                        if simi > max_simi:
                            max_simi = simi
                similarity_table[cve] = max_simi
            
            if similarity_table[cve] > 0.8:
                incorrect_cve_list.remove(cve)
        
        total_cnt = len(data)
        correct_cnt = total_cnt - len(incorrect_cve_list)
        print(f'{len(gt_not_found_list) + len(candidate_not_found_list)} lack content')
        print('{:.2f}%'.format(correct_cnt / total_cnt * 100), f'({correct_cnt}/{total_cnt})')

        save_json(f'{similarity_table_path}.json', similarity_table)
        save_pickle(f'{similarity_table_path}.pkl', similarity_table)
        
        save_text(f'{self.file_component_dir}/incorrrect_list', incorrect_cve_list)
        if gt_not_found_list:
            save_text(f'{self.file_component_dir}/gt_not_found_cve_list', gt_not_found_list)
        if candidate_not_found_list:
            save_text(f'{self.file_component_dir}/candidate_not_found_list', candidate_not_found_list)
        
        print('end check by content')


    def find_related_files(self, keywords: list, files: list, k: int, similarity_algorithm):
        # components中包含/则匹配完整路径，否则只匹配文件名
        res = []
        for keyword in keywords:
            for file, isdir in files:
                if isdir: continue
                file_lower = file.lower()
                keyword_lower = keyword.lower()
                path = '/'.join(file_lower.split('/')[:-1])
                if any(file_lower.split('/')[-1] == item for item in self.filter_file): continue
                if any(file_lower.endswith(suffix) for suffix in self.filter_suffix): continue
                if any(item in path for item in self.filter_path): continue

                if '/' not in keyword:
                    keyword_lower = keyword.split('/')[-1]
                    file_lower = file.split('/')[-1]
                
                if similarity_algorithm.__name__ == 'SequenceMatcher':
                    similarity = difflib.SequenceMatcher(None, keyword_lower, file_lower).ratio()
                elif similarity_algorithm.__name__ == 'ngram_similarity':
                    similarity = ngram_similarity(keyword_lower, file_lower, 2)
                else:
                    similarity = similarity_algorithm(keyword_lower, file_lower)
                res.append((keyword, file, similarity))
        sorted_res = sorted(res, key = lambda x: x[2], reverse = (similarity_algorithm.__name__ != 'levenshtein_distance'))
        # sorted_res = sorted(res, key = lambda x: x[2])
        return sorted_res[:k]