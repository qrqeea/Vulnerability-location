import os
import sys
import ast
import tqdm
import random
from util.io_util import *
from util.general_util import *
from util.similarity_util import *
from .ComponentsFilter import ComponentsFilter


class CandidateSelection:

    def __init__(self, module_root_path: str, specified_repo_path: str, cve_data_all: dict, repo_file_list: dict, correct_commits: dict):
        self.module_root_path = module_root_path
        self.specified_repo_path = specified_repo_path
        self.cve_data_all = cve_data_all
        self.repo_file_list = repo_file_list
        self.correct_commits = correct_commits

        os.makedirs(self.module_root_path, exist_ok = True)

        self.components_dir = f'{self.module_root_path}/components'
        os.makedirs(self.components_dir, exist_ok = True)

        self.prompt_components = f'{self.components_dir}/prompt_top10'
        os.makedirs(self.prompt_components, exist_ok = True)

        self.result_components = f'{self.components_dir}/result_top10'
        os.makedirs(self.result_components, exist_ok = True)

        self.full_file_dir = f'{self.module_root_path}/full_file'
        os.makedirs(self.full_file_dir, exist_ok = True)

        self.prompt_full_file = f'{self.full_file_dir}/prompt_top10'
        os.makedirs(self.prompt_full_file, exist_ok = True)

        self.result_full_file = f'{self.full_file_dir}/result_top10'
        os.makedirs(self.result_full_file, exist_ok = True)


    def start(self):
        # TODO gpt给的回答不是输入的文件，手动处理一下这些异常case，再看看有没有回复为空的情况
        # self.handle_components()
        # self.handle_full_file()
        # self.generate_final_candidates()
        self.check_candidates_recall(load_pickle(f'{self.module_root_path}/candidates.pkl'))
    

    def handle_full_file(self):
        self.get_full_file_candidates()
        self.check_candidates_recall(load_pickle(f'{self.full_file_dir}/candidates.pkl'))


    def handle_components(self):
        # componentsFilter = ComponentsFilter(
        #     self.components_dir,
        #     self.specified_repo_path,
        #     self.cve_data_all,
        #     self.repo_file_list,
        #     self.correct_commits
        # )
        # componentsFilter.start()
        self.get_components_candidates()
        self.check_candidates_recall(load_pickle(f'{self.components_dir}/candidates.pkl'))
    
    
    def get_components_candidates(self):
        def generate_prompt():
            prompt_template = load_file(f'{self.components_dir}/prompt_template')
            filtered_files_list = [
                load_pickle(f'{self.components_dir}/file/filtered_files_0.8.pkl'),
                load_pickle(f'{self.components_dir}/function/filtered_files.pkl'),
                load_pickle(f'{self.components_dir}/module/filtered_files.pkl')
            ]
            type_list = ['file', 'function', 'module']
            for index, filtered_files in enumerate(filtered_files_list):
                component_type = type_list[index]
                for cve, v in filtered_files.items():
                    for repo, files in v.items():
                        if len(files) == 1:     # 一个就不需要选了
                            continue
                        desc = self.cve_data_all[cve]['complete_description'] if 'complete_description' in self.cve_data_all[cve] else self.cve_data_all[cve]['original_description']
                        prompt = prompt_template.replace(
                            '{vulnerability description}', 
                            desc).replace(
                            '{file list}',
                            str(files)
                        )
                        repo_updated = repo.replace('/', '—')
                        save_text(f'{self.prompt_components}/{component_type}__{cve}__{repo_updated}__{len(files)}', prompt)

        # generate_prompt()
        self.prompt_dir = self.prompt_components
        self.result_dir = self.result_components
        # self.count_total_token()

        prompt_files = os.listdir(f'{self.prompt_dir}')
        if '.DS_Store' in prompt_files:
            prompt_files.remove('.DS_Store')
        # multi_thread(prompt_files, self.query_gpt_sub, chunk_size = 400)

        candidates = {}
        for file in os.listdir(self.result_dir):
            if file in ['.DS_Store', 'error_list']:
                continue
            cve = file.split('__')[1]
            repo = file.split('__')[2].replace('—', '/')
            res_path = f'{self.result_dir}/{file}'
            res = load_file(res_path)
            try:
                res = res.replace('```python', '')
                res = res.replace('```json', '')
                res = res.replace('```', '')
                res = ast.literal_eval(res)
                res = list(set(res))
            except Exception as e:
                # print(res_path, e, '\n\n')
                res = []
                with open(res_path, 'r') as f:
                    data = f.readlines()
                    for line in data:
                        line = line.strip()
                        for pattern in ['```plaintext', '```json', '```', '[', ']', '\',', '\'', '",' '"', '{', '}']:
                            line = line.replace(pattern, '')
                        if '. ' in line:
                            line = line[line.find('. ') + 2:]
                        if line:
                            res.append(line)
            if cve not in candidates:
                candidates[cve] = {}
            candidates[cve][repo] = res

        filtered_files = load_pickle(f'{self.components_dir}/filtered_files.pkl')
        for cve, v in filtered_files.items():
            for repo, files in v.items():
                if len(files) != 1:
                    continue
                if cve not in candidates:
                    candidates[cve] = {}
                candidates[cve][repo] = files
        save_json(f'{self.components_dir}/candidates.json', candidates)
        save_pickle(f'{self.components_dir}/candidates.pkl', candidates)


    def query_gpt_sub(self, prompt_files_sub: list):
        for file in tqdm.tqdm(prompt_files_sub):
        # for file in tqdm.tqdm(random.sample(prompt_files_sub, 2)):
            if os.path.exists(f'{self.result_dir}/{file}'):
                continue
            prompt = load_file(f'{self.prompt_dir}/{file}')
            # if calc_token(prompt) > 16385:
            #     continue
            try:
                res = query_openai(prompt, model = 'gpt-4o-2024-05-13')
                save_text(f'{self.result_dir}/{file}', res)
            except Exception as e:
                save_text(f'{self.result_dir}/error_list', f'{file}\n\n{e}', 'a')


    def get_full_file_candidates(self):
        if not os.path.exists(f'{self.full_file_dir}/cve_list.pkl'):
            cve_list_done = list(load_pickle(f'{self.components_dir}/filtered_files.pkl').keys())
            cve_list = [cve
                for cve, v in self.cve_data_all.items()
                if 'collected_commit' in v and cve not in cve_list_done
            ]
            save_text(f'{self.full_file_dir}/cve_list', cve_list)
            save_pickle(f'{self.full_file_dir}/cve_list.pkl', cve_list)
        else:
            cve_list = load_pickle(f'{self.full_file_dir}/cve_list.pkl')

        def generate_prompt():
            os.makedirs(f'{self.prompt_full_file}/file', exist_ok = True)
            os.makedirs(f'{self.prompt_full_file}/dir+file', exist_ok = True)
            os.makedirs(f'{self.result_full_file}/file', exist_ok = True)
            os.makedirs(f'{self.result_full_file}/dir+file', exist_ok = True)

            prompt_file_template = load_file(f'{self.full_file_dir}/prompt_file_template')
            prompt_dir_template = load_file(f'{self.full_file_dir}/prompt_dir_template')
            # 分两种，文件数量小于1000的给全部文件直接问，剩下的先选dir再选文件
            for cve in tqdm.tqdm(cve_list):
                desc = self.cve_data_all[cve]['complete_description'] if 'complete_description' in self.cve_data_all[cve] else self.cve_data_all[cve]['original_description']
                
                for repo, sha in self.cve_data_all[cve]['collected_commit']:
                    repo_updated = repo.replace('/', '—')
                    files = [
                        file
                        for file, isdir in self.repo_file_list[repo][sha]
                        if not isdir and rule_based_filtering(file)
                    ]
                    if len(files) <= 1000:       # 1000个file以内直接问，如果效果不好就降低数量
                        # prompt = prompt_file_template.replace('{vulnerability description}', desc).replace('{file list}', str(files))
                        # save_text(f'{self.prompt_full_file}/file/{cve}__{repo_updated}__{len(files)}', prompt)
                        continue
                    
                    root_files = [
                        file
                        for file, isdir in self.repo_file_list[repo][sha]
                        if not isdir and '/' not in file and rule_based_filtering(file)
                    ]
                    dirs = [
                        dir
                        for dir, isdir in self.repo_file_list[repo][sha]
                        if isdir and dir[0] != '.' and not any(item in dir.lower().split('/') for item in ['test', 'docs', 'tests'])
                    ]
                    if len(dirs) == 0 or len(dirs) > 1000:
                        continue
                    prompt = prompt_dir_template.replace(
                        '{vulnerability description}',
                        self.cve_data_all[cve]['complete_description'] if 'complete_description' in self.cve_data_all[cve] else self.cve_data_all[cve]['original_description']
                        ).replace(
                            '{directory list}',
                            str(dirs)
                        ).replace(
                            '{file list}',
                            str(root_files)
                        )
                    repo_updated = repo.replace('/', '—')
                    save_text(f'{self.prompt_full_file}/dir+file/{cve}__{repo_updated}__{len(dirs)}__{len(root_files)}', prompt)
        
        def handle_file(target: str):
            self.prompt_dir = f'{self.prompt_full_file}/{target}'
            self.result_dir = f'{self.result_full_file}/{target}'
            # self.count_total_token()
            
            # prompt_files = os.listdir(f'{self.prompt_dir}')
            # if '.DS_Store' in prompt_files:
            #     prompt_files.remove('.DS_Store')
            # multi_thread(prompt_files, self.query_gpt_sub, chunk_size = 150)

            candidates_path = f'{self.full_file_dir}/candidates.pkl'
            candidates = load_pickle(candidates_path) if os.path.exists(candidates_path) else {}
            for file in os.listdir(self.result_dir):
                if file in ['.DS_Store', 'error_list']:
                    continue
                cve = file.split('__')[0]
                repo = file.split('__')[1].replace('—', '/')
                # file_cnt = int(file.split('__')[2])
                res_path = f'{self.result_dir}/{file}'
                res = load_file(res_path)
                try:
                    res = res.replace('```python', '')
                    res = res.replace('```json', '')
                    res = res.replace('```', '')
                    res = ast.literal_eval(res)
                    res = list(set(res))
                except Exception as e:
                    # print(cve, res_path)
                    res = []
                    with open(res_path, 'r') as f:
                        data = f.readlines()
                        for line in data:
                            line = line.strip()
                            for pattern in ['```plaintext', '```json', '```', '[', ']', '\',', '\'', '",' '"', '{', '}']:
                                line = line.replace(pattern, '')
                            if '. ' in line:
                                line = line[line.find('. ') + 2:]
                            if line:
                                res.append(line)
                            data = f.readlines()
                if cve not in candidates:
                    candidates[cve] = {}
                candidates[cve][repo] = res

            save_json(f'{self.full_file_dir}/candidates.json', candidates)
            save_pickle(f'{self.full_file_dir}/candidates.pkl', candidates)
        
        def handle_dir_file():
            self.prompt_dir = f'{self.prompt_full_file}/dir+file'
            self.result_dir = f'{self.result_full_file}/dir+file'
            # self.count_total_token()
            
            prompt_files = os.listdir(f'{self.prompt_dir}')
            if '.DS_Store' in prompt_files:
                prompt_files.remove('.DS_Store')
            multi_thread(prompt_files, self.query_gpt_sub, chunk_size = 200)

            total_cves = set()
            correct_cves = set()
            for file in os.listdir(self.result_dir):
                if file in ['.DS_Store', 'error_list']:
                    continue
                cve = file.split('__')[0]
                repo = file.split('__')[1].replace('—', '/')
                res_path = f'{self.result_dir}/{file}'
                res = load_file(res_path)
                try:
                    res = ast.literal_eval(res)
                    res = list(set(res))
                except Exception as e:
                    # print(f'{self.result_dir}/{file}')
                    # continue
                    with open(res_path, 'r') as f:
                        data = f.readlines()
                        res = [i.strip() for i in data]
                total_cves.add(cve)
                if not (cve in self.correct_commits and repo in self.correct_commits[cve]):
                    continue
                vul_file = self.correct_commits[cve][repo]
                for dir in res:
                    if dir == '/' and '/' not in vul_file:
                        correct_cves.add(cve)
                        break
                    elif vul_file.startswith(dir) and '/' not in vul_file[len(dir) + 1:]:
                        # print(vul_file, '/'.join(vul_file.split('/')[:-1]))
                        correct_cves.add(cve)
                        break
            print('{:.2f}%'.format(len(correct_cves) / len(total_cves) * 100), f'({len(correct_cves)}/{len(total_cves)})')
        
        # generate_prompt()
        handle_file('file')
        # handle_dir_file()
        handle_file('file_772')


    def count_total_token(self):
        total = 0
        for file in tqdm.tqdm(os.listdir(self.prompt_dir)):
            if file in ['.DS_Store']: continue
            token = calc_token(load_file(f'{self.prompt_dir}/{file}'))
            # if token > 16385:
            #     print(file, token)
            total += token
        token_M = int(total / 1000000)
        print(f'total token: {token_M}M, gpt-3 price: {token_M * 0.5}$')
        print(f'total token: {token_M}M, gpt-4 price: {token_M * 5}$')


    def check_candidates_recall(self, candidates: dict):
        single_candidate_recall = 0
        multi_candidate_recall = 0
        single_candidate_cnt = 0
        multi_candidate_cnt = 0
        tp = []
        for cve, v in tqdm.tqdm(candidates.items()):
            f = False
            correct = False
            for repo, files in v.items():
                if len(files) == 1:
                    f = True
                if not (cve in self.correct_commits and repo in self.correct_commits[cve]):
                    continue
                vul_file = self.correct_commits[cve][repo].lower()
                if len(files) == 1:
                    if files[0].lower() == vul_file:
                        correct = True
                else:
                    if any(vul_file in candidate.lower() or candidate.lower() in vul_file for candidate in files):
                        correct = True
            if f:
                single_candidate_cnt += 1
                if correct:
                    single_candidate_recall += 1
                tp.append(cve)
            else:
                multi_candidate_cnt += 1
                if correct:
                    multi_candidate_recall += 1
        save_text(f'{self.module_root_path}/cve_list_single_candidate', tp)
        save_pickle(f'{self.module_root_path}/cve_list_single_candidate.pkl', tp)
            

        total_cnt = len(candidates)
        # recall += 433
        # total_cnt += 433
        print('single: {:.2f}%'.format(
            single_candidate_recall / single_candidate_cnt * 100), 
            f'({single_candidate_recall}/{single_candidate_cnt})'
        )
        print('multi: {:.2f}%'.format(
            multi_candidate_recall / multi_candidate_cnt * 100),
            f'({multi_candidate_recall}/{multi_candidate_cnt})'
        )
        recall = single_candidate_recall + multi_candidate_recall
        print('total: {:.2f}%'.format(recall / total_cnt * 100), f'({recall}/{total_cnt})')
        

    def generate_final_candidates(self):
        data1 = load_pickle(f'{self.components_dir}/candidates.pkl')
        data2 = load_pickle(f'{self.full_file_dir}/candidates.pkl')
        print(len(data1))   # 6272
        print(len(data2))   # 2040
        res = {}
        for data in [data1, data2]:
            for cve, v in data.items():
                if cve in res:
                    print(cve, 'error')
                    sys.exit()
                res[cve] = {}
                for repo, files in v.items():
                    if repo in res[cve]:
                        print(cve, repo, 'error')
                        sys.exit()
                    res[cve][repo] = files
        print(len(res))
        
        # 373/433
        for cve, v in self.cve_data_all.items():
            if 'collected_commit' in v and cve not in res:
                res[cve] = {}
                for repo, sha in v['collected_commit']:
                    if cve in self.correct_commits and repo in self.correct_commits[cve]:
                        res[cve][repo] = [self.correct_commits[cve][repo]]
                    else:
                        while True:
                            file, isdir = random.sample(self.repo_file_list[repo][sha], 1)[0]
                            if not isdir and rule_based_filtering(file):
                                res[cve][repo] = [file]
                                break

        save_json(f'{self.module_root_path}/candidates.json', res)
        save_pickle(f'{self.module_root_path}/candidates.pkl', res)