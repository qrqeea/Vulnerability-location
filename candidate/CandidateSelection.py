import os
import sys
import csv
import tqdm
import random
import difflib
import pandas as pd
from util.io_util import *
from util.general_util import *
from util.similarity_util import *
from util.github_util import github_tokens, get_file_content


class CandidateSelection:

    def __init__(self, project_root_path: str, module_root_path: str, gt_content_path: str, repo_path: str, cve_data_all: dict, repo_file_list: dict):
        self.project_root_path = project_root_path
        self.module_root_path = module_root_path
        self.gt_content_path = gt_content_path
        self.repo_path = repo_path
        self.cve_data_all = cve_data_all
        self.repo_file_list = repo_file_list

        os.makedirs(self.module_root_path, exist_ok = True)

        self.file_component_dir = f'{self.module_root_path}/file'
        os.makedirs(self.file_component_dir, exist_ok = True)

        self.function_component_dir = f'{self.module_root_path}/function'
        os.makedirs(self.function_component_dir, exist_ok = True)

        self.module_component_dir = f'{self.module_root_path}/module'
        os.makedirs(self.module_component_dir, exist_ok = True)

        self.codebert_dir = f'{self.module_root_path}/codebert'
        os.makedirs(self.codebert_dir, exist_ok = True)


    def start(self):
        res = self.retrieve_by_file_component(1, 0.6)
        # self.check_accuracy_by_repo_and_file_name(res)
        # self.check_accuracy(res)
        # self.generate_manual_check_cve_list()
        
        cve_list_file = list(res.keys())
        cve_list_function = self.retrieve_by_function(cve_list_file)

        self.retrieve_by_module(cve_list_file + cve_list_function)

        # self.codebert(set(res.keys()))


    def retrieve_by_file_component(self, k: int, similarity_threshold: float, similarity_algorithm = difflib.SequenceMatcher):
        cve_list_path = f'{self.file_component_dir}/cve_list'
        if os.path.exists(f'{cve_list_path}.pkl'):
            cve_list = load_pickle(f'{cve_list_path}.pkl')
        else:
            cve_list = [
                cve for cve in self.cve_data_all if
                'collected_commit' in self.cve_data_all[cve] and
                'File' in self.cve_data_all[cve]['components']
            ]
            save_text(cve_list_path, cve_list)
            save_pickle(f'{cve_list_path}.pkl', cve_list)
        # print(len(cve_list))    # 4884
        
        os.makedirs(f'{self.file_component_dir}/candidate_list', exist_ok = True)
        res_path = f'{self.file_component_dir}/candidate_list/candidates_{len(cve_list)}_{k}_{similarity_threshold}_{similarity_algorithm.__name__}.json'
        if os.path.exists(res_path):
            return load_json(res_path)
        
        candidates_100_path = f'{self.file_component_dir}/candidate_list/candidates_{len(cve_list)}_100_{similarity_algorithm.__name__}.json'
        if os.path.exists(candidates_100_path):
            candidates_100 = load_json(candidates_100_path)
        else:
            candidates_100 = {}
            for cve in tqdm.tqdm(cve_list):
            # for cve in random.sample(cve_list, 50):
                file_components = self.cve_data_all[cve].get('components').get('File')
                collected_commit = self.cve_data_all[cve].get('collected_commit')

                candidates_100[cve] = {}
                for repo, commit in collected_commit:
                    repo_file_list = self.repo_file_list[repo][commit]
                    related_files = self.find_related_files(file_components, repo_file_list, 100, similarity_algorithm)
                    candidates_100[cve][repo] = related_files
            save_json(candidates_100_path, candidates_100)
        
        print('start reduce data')
        multi_repo_cnt = 0
        multi_candidate_cnt= 0
        reduced_data = {}
        for cve, v in candidates_100.items():
            # if not self.cve_data_all[cve]['collected_repo_correction']: continue
            # if not self.cve_data_all[cve]['collected_commit_correction']: continue
            reduced_data[cve] = {}
            if len(v) > 1:
                multi_repo_cnt += 1
            flag = False
            for repo, triple_list in v.items():
                tp = []
                for index, triple in enumerate(triple_list):
                    if index >= k and triple[2] != triple_list[index - 1][2]:
                        if index > 1:
                            flag = True
                        break
                    if triple[2] < similarity_threshold:    # 相似度低于阈值大概率是组件提取不对或repo、commit有误
                        break
                    tp.append(triple[1])
                if tp:
                    reduced_data[cve][repo] = tp
            if flag:
                multi_candidate_cnt += 1
            if not reduced_data[cve]:
                del reduced_data[cve]
        
        print(f'{multi_repo_cnt} cve has multi repo')
        print(f'{multi_candidate_cnt} cve has multi candidate')
        
        save_json(res_path, reduced_data)
        
        print('end reduce data')
        return reduced_data
    
    
    def retrieve_by_function(self, cve_list_done: list):
        res_path = f'{self.function_component_dir}/filtered_files'
        if os.path.exists(f'{res_path}.pkl'):
            return list(load_pickle(f'{res_path}.pkl').keys())

        cve_list_path = f'{self.function_component_dir}/cve_list'
        if os.path.exists(f'{cve_list_path}.pkl'):
            cve_list = load_pickle(f'{cve_list_path}.pkl')
            keywords = load_json(f'{self.function_component_dir}/keywords.json')
        else:
            cve_list = [
                cve for cve in self.cve_data_all if
                'collected_commit' in self.cve_data_all[cve] and
                'Function' in self.cve_data_all[cve]['components'] and
                cve not in cve_list_done
            ]
            keywords = {}
            for cve in tqdm.tqdm(cve_list):
                functions = self.cve_data_all[cve]['components']['Function']
                tp = list({
                    keyword.lower()
                    for func in functions
                    for keyword in func.replace('::', ' ').replace(':', ' ').split(' ')
                    if keyword.lower() not in ['driver', 'implementation', 'function', 'api', 'xml', 'check', 'tensor', 'control', 'component', 'server', 'inference', 'code', 'key', 'and', 'http', 'condition', 'race', 'connect', 'release', 'std', 'move', 'version', 'shape', 'scsi', 'generic', 'kernel', 'management', 'password', 'interface', 'designer', 'form', 'mysql', 'client', 'dim', 'xsd', 'processor', 'data', 'html', 'node', 'uri', 'destruction', 'variable', 'routines', 'new', 'file', 'step', 'hash', 'call', 'search', 'test', 'where', 'net', 'remote', 'build'] and len(keyword) > 2
                })
                if tp:
                    keywords[cve] = tp
            cve_list = list(keywords.keys())
            save_text(cve_list_path, cve_list)
            save_pickle(f'{cve_list_path}.pkl', cve_list)
            save_json(f'{self.function_component_dir}/keywords.json', keywords)
        
        print(len(cve_list))
        
        res = {}
        os.makedirs(f'{self.function_component_dir}/filtered_files', exist_ok = True)
        for cve in tqdm.tqdm(cve_list):
            # print(cve)
            if os.path.exists(f'{self.function_component_dir}/filtered_files/{cve}.json'):
                res[cve] = load_json(f'{self.function_component_dir}/filtered_files/{cve}.json')
                continue
            res[cve] = {}
            for repo, _ in self.cve_data_all[cve]['collected_commit']:
                res[cve][repo] = []
                repo_updated = repo.replace('/', '—')
                repo_path = f'{self.repo_path}/{cve}/{repo_updated}'
                if not os.path.exists(repo_path):
                    print(f'{cve} {repo} not found')
                    continue
                for root_path, _, files in os.walk(repo_path):
                    for file in files:
                        if not rule_based_filtering(file.lower(), root_path.lower()):
                            continue
                        full_path = f'{root_path}/{file}'
                        try:
                            content = load_file(full_path)
                            if any(keyword in content.lower() for keyword in keywords[cve]):
                                res[cve][repo].append(full_path[len(repo_path) + 1:])
                        except Exception as e:
                            pass
                if not res[cve][repo]:
                    del res[cve][repo]
            if not res[cve]:
                del res[cve]
            else:
                save_json(f'{self.function_component_dir}/filtered_files/{cve}.json', res[cve])
        
        print(f'{len(res)}/{len(cve_list)} cve found candidates')
        save_json(f'{res_path}.json', res)
        save_pickle(f'{res_path}.pkl', res)
        
        return list(res.keys())


    def retrieve_by_module(self, cve_list_done: list):
        cve_list_path = f'{self.module_component_dir}/cve_list'
        if os.path.exists(f'{cve_list_path}.pkl'):
            cve_list = load_pickle(f'{cve_list_path}.pkl')
        else:
            cve_list = [
                cve for cve in self.cve_data_all if
                'collected_commit' in self.cve_data_all[cve] and
                'components' in self.cve_data_all[cve] and
                'Module' in self.cve_data_all[cve]['components'] and
                cve not in cve_list_done
            ]
            save_text(cve_list_path, cve_list)
            save_pickle(f'{cve_list_path}.pkl', cve_list)
        print(len(cve_list))



    def check_accuracy_by_repo_and_file_name(self, data: dict):
        print('start check by repo and file name')
        
        incorrect_cve_list = []
        incorrect_cve_list_strict = []
        for cve, v in data.items():
            flag = False
            flag_strict = False
            for repo_candidate, file_candidates in v.items():
                if flag and flag_strict:
                    break
                for repo_ans, files_ans in self.cve_data_all[cve]['vulnerability_files'].items():
                    if repo_candidate.lower() == repo_ans.lower() and any(
                        file_ans.lower() == file_candidate.lower()
                        for file_candidate in file_candidates
                        for file_ans in files_ans
                    ):
                        flag_strict = True
                    if any(
                        file_ans.lower() in file_candidate.lower() or file_candidate.lower() in file_ans.lower() for file_candidate in file_candidates
                        for file_ans in files_ans
                    ):
                        flag = True
            if not flag:
                incorrect_cve_list.append(cve)
            if not flag_strict:
                incorrect_cve_list_strict.append(cve)
        
        total_count = len(data)
        correct_cnt = total_count - len(incorrect_cve_list)
        correct_cnt_strict = total_count - len(incorrect_cve_list_strict)
        print('{:.2f}%'.format(correct_cnt / total_count * 100), f'({correct_cnt}/{total_count})')
        print('strict: {:.2f}%'.format(correct_cnt_strict / total_count * 100), f'({correct_cnt_strict}/{total_count})')
        
        print('end check by repo and file name')
        return incorrect_cve_list
        # return incorrect_cve_list_strict


    def get_candidate_content(self, to_scrapy_list_sub: list, token: str):
        for cve, repo, sha, file in tqdm.tqdm(to_scrapy_list_sub):
            dir = f'{self.candidate_content}/{cve}'
            tp1 = repo.replace('/', '\\')
            tp2 = file.replace('/', '\\')
            save_path = f'{dir}/{tp1}____{tp2}'
            if os.path.exists(save_path):
                continue
            res = get_file_content(repo, sha, file, token)
            if res:
                os.makedirs(dir, exist_ok = True)
                save_text(save_path, res)


    def check_accuracy(self, data: dict):
        print('start check accuracy')
        incorrect_cve_list = self.check_accuracy_by_repo_and_file_name(data)
        
        print('start generate to_scrapy_list')
        to_scrapy_list = []
        for cve in incorrect_cve_list:
            for repo_candidate, file_candidates in data[cve].items():
                sha = list(filter(lambda x: x[0] == repo_candidate, self.cve_data_all[cve]['collected_commit']))[0][1]
                if not sha:
                    print('error, sha')
                    sys.exit()
                for file_candidate in file_candidates:
                    to_scrapy_list.append((cve, repo_candidate, sha, file_candidate))
        print(f'end generate to_scrapy_list, size: {len(to_scrapy_list)}')

        print('start scrapy to_scrapy_list')
        os.makedirs(f'{self.file_component_dir}/candidate_content', exist_ok = True)
        self.candidate_content = f'{self.file_component_dir}/candidate_content'
        multi_thread(to_scrapy_list, self.get_candidate_content, tokens = github_tokens)
        print(f'end scrapy to_scrapy_list')

        self.check_by_file_content(len(data), incorrect_cve_list, self.file_component_dir)
        

    def check_by_file_content(self, total_cnt: int, incorrect_cve_list: list, root_path: str):
        print('start check by file content')
        gt_not_found_list = []
        candidate_not_found_list = []
        
        similarity_table_path = f'{self.module_root_path}/similarity_table'
        if os.path.exists(f'{similarity_table_path}.pkl'):
            similarity_table = load_pickle(f'{similarity_table_path}.pkl')
        else:
            similarity_table = {}
        
        positive_label_list = []
        for cve in tqdm.tqdm(incorrect_cve_list.copy()):
            if cve not in similarity_table or not similarity_table[cve]:
                similarity_table[cve] = {}
                gt_content_path = f'{self.gt_content_path}/{cve}'
                file_content_path = f'{root_path}/candidate_content/{cve}'
                if not os.path.exists(gt_content_path):
                    gt_not_found_list.append(cve)
                    continue
                if not os.path.exists(file_content_path):
                    # TODO 处理一下
                    # print(f'error, {cve} content not found')
                    # 这里大概是因为经过启发式规则筛选后没有候选文件，如CVE-2019-8424
                    # 还有是因为候选文件不是文本文件，如CVE-2018-19504
                    candidate_not_found_list.append(cve)
                    continue
                file_ans_list = [
                    file
                    for file in os.listdir(gt_content_path) if file not in ['.DS_Store']
                ]
                file_candidate_list = [
                    file
                    for file in os.listdir(file_content_path) if file not in ['.DS_Store']
                ]
                max_simi = 0
                selected_file = ''
                for file_candidate in file_candidate_list:
                    for file_ans in file_ans_list:
                        candidate_content = load_file(f'{file_content_path}/{file_candidate}')
                        gt_content = load_file(f'{gt_content_path}/{file_ans}')
                        simi = difflib.SequenceMatcher(None, candidate_content, gt_content).ratio()
                        if simi > max_simi:
                            max_simi = simi
                            selected_file = file_candidate
                similarity_table[cve]['simi'] = max_simi
                similarity_table[cve]['repo'] = selected_file.split('____')[0].replace('\\', '/')
                similarity_table[cve]['file'] = selected_file.split('____')[1].replace('\\', '/')
            
            if similarity_table[cve]['simi'] > 0.8:
                incorrect_cve_list.remove(cve)
                if root_path == self.codebert_dir:
                    positive_label_list.append((cve, similarity_table[cve]['repo'], similarity_table[cve]['file']))
        
        correct_cnt = total_cnt - len(incorrect_cve_list)
        print(f'{len(gt_not_found_list) + len(candidate_not_found_list)} lack content')
        print('{:.2f}%'.format(correct_cnt / total_cnt * 100), f'({correct_cnt}/{total_cnt})')

        save_json(f'{similarity_table_path}.json', similarity_table)
        save_pickle(f'{similarity_table_path}.pkl', similarity_table)
        
        save_text(f'{root_path}/incorrrect_list', incorrect_cve_list)
        if gt_not_found_list:
            save_text(f'{root_path}/gt_not_found_cve_list', gt_not_found_list)
        if candidate_not_found_list:
            save_text(f'{root_path}/candidate_not_found_list', candidate_not_found_list)
        
        print('end check by content')
        
        if root_path == self.codebert_dir:
            return positive_label_list


    def find_related_files(self, keywords: list, files: list, k: int, similarity_algorithm):
        # components中包含'/'则匹配完整路径，否则只匹配文件名
        res = []
        for keyword in keywords:
            for full_path, isdir in files:
                if isdir: continue
                file_lower = full_path.lower()
                file_name = file_lower.split('/')[-1]
                file_path = '/'.join(file_lower.split('/')[:-1])
                if not rule_based_filtering(file_name, file_path):
                    continue

                keyword_lower = keyword.lower()
                if '/' not in keyword:
                    keyword_lower = keyword_lower.split('/')[-1]
                    file_lower = file_name
                
                if similarity_algorithm.__name__ == 'SequenceMatcher':
                    similarity = difflib.SequenceMatcher(None, keyword_lower, file_lower).ratio()
                elif similarity_algorithm.__name__ == 'ngram_similarity':
                    similarity = ngram_similarity(keyword_lower, file_lower, 2)
                else:
                    similarity = similarity_algorithm(keyword_lower, file_lower)
                res.append((keyword, full_path, similarity))
        sorted_res = sorted(res, key = lambda x: x[2], reverse = (similarity_algorithm.__name__ != 'levenshtein_distance'))
        # sorted_res = sorted(res, key = lambda x: x[2])
        return sorted_res[:k]
    

    def codebert(self, cve_list_done: set):
        cve_list_path = f'{self.codebert_dir}/cve_list'
        if not os.path.exists(f'{cve_list_path}.pkl'):
            cve_list = {cve for cve, v in self.cve_data_all.items() if 'collected_commit' in v} - cve_list_done
            cve_list = list(cve_list)
            save_text(cve_list_path, cve_list)
            save_pickle(f'{cve_list_path}.pkl', cve_list)
        else:
            cve_list = load_pickle(f'{cve_list_path}.pkl')
        print(f'{len(cve_list)}')   # 4176

        print('start check if vulnerability file in collected commit')
        incorrect_cve_list = []
        positive_label_list = []
        for cve in tqdm.tqdm(cve_list):
            flag = False
            for repo, sha in self.cve_data_all[cve]['collected_commit']:
                if flag: break
                if repo in self.cve_data_all[cve]['vulnerability_files']:   # repo相同，file相同
                    file_ans_list = self.cve_data_all[cve]['vulnerability_files'][repo]
                else:
                    file_ans_list = self.cve_data_all[cve]['file_list']     # repo不同，file相同
                for file_ans in file_ans_list:
                    file_ans = file_ans.lower()
                    for file_candidate, _ in self.repo_file_list[repo][sha]:
                        if file_ans.lower() == file_candidate.lower():
                            flag = True
                            positive_label_list.append((cve, repo, file_candidate))
                            break
            if not flag:    # 没有同名文件，repo可能相同也可能不相同，选出相似度最高的文件，计算内容相似度
                incorrect_cve_list.append(cve)
        
        print(f'{len(cve_list) - len(incorrect_cve_list)} cve correct, {len(incorrect_cve_list)} cve to check by content')
        print(f'positive_label_list size: {len(positive_label_list)}')

        if os.path.exists(f'{self.codebert_dir}/to_scrapy_list.pkl'):
            to_scrapy_list = load_pickle(f'{self.codebert_dir}/to_scrapy_list.pkl')
        else:
            to_scrapy_list = []
            print('start generate to_scrapy_list')
            for cve in tqdm.tqdm(incorrect_cve_list):
                for repo, sha in self.cve_data_all[cve]['collected_commit']:
                    file = ''
                    max_simi = 0
                    for file_candidate, isdir in self.repo_file_list[repo][sha]:
                        if isdir: continue
                        file_candidate_lower = file_candidate.lower()
                        file_name = file_candidate_lower.split('/')[-1]
                        path = '/'.join(file_candidate_lower.split('/')[:-1])
                        if not rule_based_filtering(file_name, path):
                            continue

                        for file_ans in self.cve_data_all[cve]['file_list']:
                            file_ans_lower = file_ans.lower()
                            simi = difflib.SequenceMatcher(None, file_candidate_lower, file_ans_lower).ratio()
                            if simi > max_simi:
                                max_simi = simi
                                file = file_candidate
                    if file:
                        to_scrapy_list.append((cve, repo, sha, file))
            save_text(f'{self.codebert_dir}/to_scrapy_list', to_scrapy_list)
            save_pickle(f'{self.codebert_dir}/to_scrapy_list.pkl', to_scrapy_list)
            print(f'end generate to_scrapy_list, size: {len(to_scrapy_list)}')

        print('start scrapy to_scrapy_list')
        os.makedirs(f'{self.codebert_dir}/candidate_content', exist_ok = True)
        self.candidate_content = f'{self.codebert_dir}/candidate_content'
        multi_thread(to_scrapy_list, self.get_candidate_content, tokens = github_tokens)
        print(f'end scrapy to_scrapy_list')

        positive_label_list += self.check_by_file_content(len(cve_list), incorrect_cve_list, self.codebert_dir)
        print(f'positive_label_list size: {len(positive_label_list)}')
        save_text(f'{self.codebert_dir}/positive_label_list', positive_label_list)
        save_pickle(f'{self.codebert_dir}/positive_label_list.pkl', positive_label_list)