import os
import re
import sys
import json
import tqdm
import time
import pytz
import requests
from datetime import datetime
from get_cpe_product import get_cpe
from util import *

github_tokens = [
    
]

class RepositoryCollection:

    def __init__(self, root_path: str, cve_json_path: str, cve_list: list):
        self.root_path = root_path
        self.cve_json_path = cve_json_path
        self.cve_list = cve_list


    def start(self):
        self.search_reference_url()
        self.check_reference_url_result()

        # self.search_from_github(rest_cve_list)
        # rest_cve_list = self.check_github_search_result()
        # print(len(rest_cve_list))


    def search_reference_url(self):
        # 搜索cve的reference url, 如果有github url则提取repo
        
        print('start search reference url')
        res = {}
        rest_cve_list = []
        for cve in tqdm.tqdm(self.cve_list):
            filePath = f'{self.cve_json_path}/{cve}.json'
            reference_list = [dic['url'] for dic in load_json(filePath)['containers']['cna']['references']]
            flag =  False
            for url in reference_list:
                if get_domain(url) == 'github.com':
                    flag = True
                    if cve in res.keys():
                        res[cve].add(('/'.join(url.split('/')[3:5]), url))
                    else:
                        res[cve] = {('/'.join(url.split('/')[3:5]), url)}
            if not flag:
                rest_cve_list.append(cve)
                

        save_text(f'{self.root_path}/repo_from_reference_url', res)
        save_pickle(f'{self.root_path}/repo_from_reference_url.pkl', res)
        save_text(f'{self.root_path}/cve_after_search_reference_url', rest_cve_list)
        save_pickle(f'{self.root_path}/cve_after_search_reference_url.pkl', rest_cve_list)

        print(f'found {len(res)}/{len(self.cve_list)}, {len(self.cve_list) - len(res)} rest')
        print('end search reference url\n')


    def check_reference_url_result(self):
        print('start check repo from reference url')
        data = load_pickle(f'{self.root_path}/repo_from_reference_url.pkl')
        ground_truth = load_pickle('/Volumes/NVD/experiment_data/all/cleaned_gt_commit_single_v2.pkl').keys()
        ground_truth = {(cve, repo.lower()) for (cve, repo) in ground_truth}
        res = []
        for cve, repos in data.items():
            flag = False
            for repo in repos:
                if (cve, repo[0].lower()) in ground_truth:
                    flag = True
                    break
            if not flag:
                res.append((cve, [repo[1] for repo in repos]))
                # print(cve)
        save_text(f'{self.root_path}/incorrect_repo_from_reference_url', res)
        print('accuracy: {:.2f}%,'.format((len(data) - len(res)) / len(data) * 100), f'{len(res)} rest')
        print('end check repo from reference url')
        

    def check_github_search_result(self):
        print('start check repo from github search')
        data = load_pickle(f'{self.root_path}/repo_from_github_search.pkl')
        ground_truth = load_pickle('/Volumes/NVD/experiment_data/all/cleaned_gt_commit_single_v2.pkl').keys()
        ground_truth = {(cve, repo.lower()) for (cve, repo) in ground_truth}
        res = []
        for cve, repos in data.items():
            flag = False
            for repo in repos:
                if (cve, repo.lower()) in ground_truth:
                    flag = True
                    break
            if not flag:
                res.append((cve, [repo for repo in repos]))
                # print(cve)
        save_text(f'{self.root_path}/incorrect_repo_from_github_search', res)
        print('end check repo from github search')
        correct_count = len(data) - len(res)
        print(f'accuracy: {correct_count}/{len(data)}, ', '{:.2f}%,'.format(correct_count / len(data) * 100), f'{len(res)} rest')
        
        return [item[0] for item in res]


    def search_from_github(self, cve_list: list):
        url = 'https://api.github.com/search/repositories'

        def check_api_limit(headers):
            remaining = headers['X-RateLimit-Remaining']
            reset_time = int(headers['X-RateLimit-Reset'])
            if int(remaining) == 0:
                reset_time_utc = datetime.utcfromtimestamp(reset_time)
                beijing_timezone = pytz.timezone('Asia/Shanghai')
                reset_time_beijing = reset_time_utc.replace(tzinfo=pytz.utc).astimezone(beijing_timezone)
                now_beijing = datetime.now(beijing_timezone)
                time_to_reset = (reset_time_beijing - now_beijing).total_seconds() + 2
                # print(f'剩余重置时间：{time_to_reset} seconds')
                if time_to_reset < 0:
                    time_to_reset = 1
                time.sleep(time_to_reset)
            
        def search(cve_list_sub: list, token: str):
            headers = {'Authorization': f'token {token}'}
            for cve in tqdm.tqdm(cve_list_sub):
                tp = set()
                for product in cpe_product[cve]:
                    try:
                        # params = {'q': product, 'per_page': 10, 'sort': 'forks'}
                        # response = requests.get(url, headers = headers, params = params)
                        # if response.ok:
                        #     data = response.json()
                        #     repositories = data["items"]
                        #     # repo["html_url"]
                        #     tp = tp.union({repo['full_name'] for repo in repositories})
                        #     check_api_limit(response.headers)

                        # params = {'q': product, 'per_page': 10, 'sort': 'stars'}
                        # response = requests.get(url, headers = headers, params = params)
                        # if response.ok:
                        #     data = response.json()
                        #     repositories = data["items"]
                        #     tp = tp.union({repo['full_name'] for repo in repositories})
                        #     check_api_limit(response.headers)

                        params = {'q': product, 'per_page': 10}
                        response = requests.get(url, headers = headers, params = params)
                        if response.ok:
                            data = response.json()
                            repositories = data["items"]
                            tp = tp.union({repo['full_name'] for repo in repositories})
                            check_api_limit(response.headers)
                        # else:
                            # print('token:', token)
                            # print(response.text)
                            # check_api_limit(response.headers)
                    except Exception as e:
                        # print('token:', token)
                        print(e)
                res[cve] = tp

        print('start search github')

        res = {}
        cpe_product_path = f'{self.root_path}/cpe_product.pkl'
        if os.path.exists(cpe_product_path):
            cpe_product = load_pickle(cpe_product_path)
        else:
            cpe_product = get_cpe(cve_list, '/Volumes/NVD/experiment_data/all/cpe_json')
            save_pickle(cpe_product_path, cpe_product)

        multi_thread(cve_list, search, github_tokens)

        save_text(f'{self.root_path}/repo_from_github_search', res)
        save_pickle(f'{self.root_path}/repo_from_github_search.pkl', res)

        print('end search github')
        print(f'found {len(res)}/{len(cve_list)}, {len(cve_list) - len(res)} rest\n')



def get_product(s):
    pattern = r'(?<=:)([^:]+):([^:]+):([^:]+):([^:]+):([^:]+):'
    match = re.search(pattern, s)
    if match:
        return match.group(4)
    else:
        return None


def get_cpe_products(cve_list: list, cpe_json_path: str):
    # 遍历cpe_json文件，遇到目标cve，先查看reference url中是否有github链接，若有则提取repo；否则提取cpe_product
    # 返回两个dict
    # res_repo: {cve: {full_repo_name}}
    # res_cpe_product: {cve: {cpe_product}}
    
    res_repo = {}
    res_cpe_product = {}
    files = os.listdir(cpe_json_path)
    for file in tqdm.tqdm(files):
        if file in ['.DS_Store']:
            continue
        
        # print(f"current file: {file}")
        fileName = f'{cpe_json_path}/{file}'
        with open(fileName, 'r', encoding='utf-8', errors='ignore') as f:
            data = json.load(f)['CVE_Items']
        for cve_dic in data:
            cve_id = cve_dic['cve']['CVE_data_meta']['ID']
            if cve_id not in cve_list: continue
            
            repos = set()
            for item in cve_dic['cve']['references']['reference_data']:
                url = item['url']
                if get_domain(url) == 'github.com':
                    repos.add('/'.join(url.split('/')[3:5]))
            if repos:
                res_repo[cve_id] = repos
                continue

            res_cpe_product[cve_id] = set()
            cpe_list = cve_dic['configurations']['nodes'][0]['cpe_match']
            if not cpe_list:
                # CVE-2021-31810
                # CVE-2014-9390
                # CVE-2022-29526
                cpe_list = cpe_list = cve_dic['configurations']['nodes'][0]['children'][0]['cpe_match']
            for cpe_dic in cpe_list:
                cpe = cpe_dic['cpe23Uri']
                res_cpe_product[cve_id].add(get_product(cpe))
    return (res_repo, res_cpe_product)


def search_github_repositories(keyword, count = 3):
    base_url = "https://api.github.com/"
    search_endpoint = "search/repositories"
    params = {
        "q": keyword,
        "per_page": count
    }
    
    response = requests.get(base_url + search_endpoint, params=params)
    if response.status_code == 200:
        data = response.json()
        repositories = data["items"]
        # for repo in repositories:
        #     print(repo["html_url"])
        return [repo["html_url"] for repo in repositories]
    else:
        print("Error occurred: Status code", response.status_code)
        return None


def extract_repo_name(repo_url):
    if repo_url.startswith("https://github.com/"):
        path_components: list = repo_url[len("https://github.com/"):].split('/')
        path_components = [item for item in path_components if item != '']  # 去除空元素
        # 如果路径包含用户名和仓库名，则认为是仓库地址
        if len(path_components) >= 2:
            return f"{path_components[0]}/{path_components[1]}"
    return None


def get_product_url(cpe_product_dic: dict):
    products = set()
    for _, value in cpe_product_dic.items():
        products |= value
    # print(len(products))
        
    prompt_general = 'Give me the GitHub URL of #, answer within 100 tokens. Only output the URL, do not output prompt information.'
    product_repo_url = {}    # {product: [(repo_full_name, repo_url)]}
    
    tmp_file = 'repository_tmp.pkl'
    if os.path.exists(tmp_file):
        product_repo_url = load_pickle(tmp_file)
        print(len(product_repo_url))

    for product in tqdm.tqdm(products):
        if product_repo_url.get(product):
            continue

        prompt = prompt_general.replace('#', product)
        # print(prompt)
        try:
            # print('querying openai')
            url = query_openai(prompt, 'gpt-4-turbo')
            repo_full_name = extract_repo_name(url)
            if repo_full_name and requests.get(url).ok:
                product_repo_url[product] = [(repo_full_name, url)]
            else:
                urls = search_github_repositories(product)
                if urls:
                    tp = []
                    for url in urls:
                        repo_full_name = extract_repo_name(url)
                        if repo_full_name and requests.get(url).ok:
                            tp.append((repo_full_name, url))
                    product_repo_url[product] = tp
        except Exception:
            try:
                # print('second querying openai')
                url = query_openai(prompt)
                repo_full_name = extract_repo_name(url)
                if repo_full_name and requests.get(url).ok:
                    product_repo_url[product] = [(repo_full_name, url)]
                else:
                    urls = search_github_repositories(product)
                    if urls:
                        tp = []
                        for url in urls:
                            repo_full_name = extract_repo_name(url)
                            if repo_full_name and requests.get(url).ok:
                                tp.append((repo_full_name, url))
                        product_repo_url[product] = tp
            except Exception as e:
                # print(e)
                save_pickle(tmp_file, product_repo_url)

    return product_repo_url


def get_repository_by_product(cpe_product_dic: dict):
    # input: {cve: {cpe_products}}
    # 汇总所有product，依次询问gpt，然后验证是否有效，若无效通过github搜索，保留5个，{product: [(repo_full_name, repo_url)]}
    # 汇总输出：遍历input，对每个product，到2中查找，若存在，则输出{(cve, repo_full_name): url}
    
    product_repo_url = get_product_url(cpe_product_dic)
    # print(product_repo_url)
    res = {}
    for cve, products in cpe_product_dic.items():
        for product in products:
            if product_repo_url.get(product):
                for repo_full_name, url in product_repo_url[product]:
                    res[(cve, repo_full_name)] = url
    return res


def get_repository(cve_list: list, target_path: str):
    if os.path.exists(target_path):
        return load_pickle(f'{target_path}.pkl')

    cve_repo, cve_product = get_cpe_products(
        cve_list,
        '/Volumes/NVD/experiment_data/all/cpe_json'
    )
    
    # cve_product = {
    #     'CVE-2021-21996' :  {'salt'},
    #     'CVE-2021-22119' :  {'spring_security'},
    #     'CVE-2021-22696' :  {'cxf'},
    #     'CVE-2021-23450' :  {'dojo'},
    #     'CVE-2016-9459' :  {'owncloud', 'nextcloud_server'},
    #     'CVE-2016-9460' :  {'owncloud', 'nextcloud'}
    # }

    res = get_repository_by_product(cve_product)
    for cve, full_repo_name_set in cve_repo.items():
        for full_repo_name in full_repo_name_set:
            res[(cve, full_repo_name)] = 'https://github.com/' + full_repo_name
    
    save_text(target_path, res)
    save_pickle(f'{target_path}.pkl', res)

    return res


if __name__ == '__main__':
    cve_list = load_pickle('/Volumes/NVD/experiment_data/267/procedure_data/cve_list_267.pkl')
    get_repository(
        cve_list,
        '/Volumes/NVD/experiment_data/267/procedure_data/{(cve, repo_full_name): url}'
    )