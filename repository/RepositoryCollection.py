import os
import tqdm
import random
import requests
from datetime import datetime
from util.io_util import *
from util.github_util import *


class RepositoryCollection:

    def __init__(self, project_root_path: str, module_root_path: str, repo_data_path: str, cve_data_all: dict):
        self.project_root_path = project_root_path
        self.module_root_path = module_root_path
        self.repo_data_path = repo_data_path
        self.cve_data_all = cve_data_all
        
        self.cve_list = list(self.cve_data_all.keys())
        print(len(self.cve_list))

        os.makedirs(self.module_root_path, exist_ok = True)

        self.repo_from_gpt = f'{self.module_root_path}/repo_from_gpt'
        os.makedirs(self.repo_from_gpt, exist_ok = True)

        self.repo_from_url = f'{self.module_root_path}/repo_from_url'
        os.makedirs(self.repo_from_url, exist_ok = True)

        self.repo_from_github = f'{self.module_root_path}/repo_from_github'
        os.makedirs(self.repo_from_github, exist_ok = True)


    def start(self):
        # self.sync()

        self.search_reference_url()
        # self.check_result(self.repo_from_url)

        self.query_gpt()
        # self.check_result(self.repo_from_gpt)

        self.search_from_github()
        # self.check_result(self.repo_from_github)

        self.union_result()


    def sync(self):
        dir_list = [
            (f'{self.repo_from_gpt}/prompt', f'{self.module_root_path}/deprecated/repo_from_gpt/prompt'),
            (f'{self.repo_from_gpt}/result', f'{self.module_root_path}/deprecated/repo_from_gpt/result'),
        ]
        for source, dest in dir_list:
            file_list = os.listdir(source)
            if '.DS_Store' in file_list:
                file_list.remove('.DS_Store')
            for file in file_list:
                cve = file.split('.')[0]
                if cve not in self.cve_list:
                    copy_file(f'{source}/{file}', f'{dest}/{file}')
                    os.remove(f'{source}/{file}')


    def check_repo_existence(self, res: dict):
        
        def check_repo_existence_sub(cve_sub_list: list, token: str):
            for cve in tqdm.tqdm(cve_sub_list):
                repo_to_delete = []
                for repo in res[cve]:
                    if repo not in repo_existence:
                        repo_existence[repo] = check_repo_exist(repo, token)
                    if not repo_existence[repo]:
                        repo_to_delete.append(repo)
                for repo in repo_to_delete:
                    res[cve].remove(repo)
                if not res[cve]:
                    del res[cve]
                    # print('del', cve)

        # 检查提取到的repo是否存在
        print('start check if the repo exists')
        
        repo_existence_path = f'{self.repo_data_path}/repo_existence'
        repo_existence = load_pickle(f'{repo_existence_path}.pkl') if os.path.exists(f'{repo_existence_path}.pkl') else {}
        
        multi_thread(list(res.keys()), check_repo_existence_sub, tokens = github_tokens)

        save_json(f'{repo_existence_path}.json', repo_existence)
        save_pickle(f'{repo_existence_path}.pkl', repo_existence)
        
        print('end check if the repo exists')

        return res


    def search_reference_url(self):
        # 搜索cve的reference url, 如果有github url则提取repo
        print(f'start search reference url, cve count: {len(self.cve_list)}')
        
        res = {}
        for cve in tqdm.tqdm(self.cve_list):
            for url in self.cve_data_all[cve]['reference_list']:
                if get_domain(url) == 'github.com':
                    repo_full_name = '/'.join(url.split('/')[3:5])
                    if '#' in repo_full_name:
                        repo_full_name = repo_full_name[:repo_full_name.find('#')]
                    if cve in res:
                        res[cve].add(repo_full_name)
                    else:
                        res[cve] = {repo_full_name}
            if cve in res:
                res[cve] = list(res[cve])
        
        print(f'found {len(res)}/{len(self.cve_list)}, {len(self.cve_list) - len(res)} rest')
        print('end search reference url\n')

        res = self.check_repo_existence(res)
        
        print(f'found {len(res)}/{len(self.cve_list)}, {len(self.cve_list) - len(res)} rest')
        save_json(f'{self.repo_from_url}/collected_repos.json', res)
        save_pickle(f'{self.repo_from_url}/collected_repos.pkl', res)


    def check_result(self, target: str, update_to_cve_data_all = False):
        print('start check repo')

        incorrect_cve_dic = {}
        data = load_pickle(f'{target}/collected_repos.pkl')
        for cve, repos in tqdm.tqdm(data.items()):
            flag = False
            for repo in repos:
                if any(repo.lower() == ans.lower() for ans in self.cve_data_all[cve]['repo_list']):
                    flag = True
                    break
            if not flag:
                incorrect_cve_dic[cve] = repos
        print(f'first check: {len(incorrect_cve_dic)} cve incorrect')

        def get_latest_and_original_repo_name(repo_name: str, token: str):
            if repo_name not in repo_past_name:
                latest_repo_name = get_latest_repo_name(repo_name, token)
                original_repo_name = get_original_repo_name(repo_name, token)
                repo_past_name[repo_name] = {}
                repo_past_name[repo_name]['latest_name'] = latest_repo_name
                repo_past_name[repo_name]['original_name'] = original_repo_name
            return (
                repo_past_name[repo_name]['latest_name'],
                repo_past_name[repo_name]['original_name']
            )
        
        def check_repo_name(cve_list: list, token: str):
            for cve in tqdm.tqdm(cve_list):
                flag = False
                for repo in incorrect_cve_dic[cve]:
                    past_name = get_latest_and_original_repo_name(repo, token)
                    for repo_ans in self.cve_data_all[cve]['repo_list']:
                        ans_past_name = get_latest_and_original_repo_name(repo_ans, token)
                        if past_name[0].lower() == ans_past_name[0].lower() or past_name[1].lower() == ans_past_name[1].lower():
                            flag = True
                            break
                if flag:
                    del incorrect_cve_dic[cve]
        
        # 检查是否是改过名的或fork的repo
        if os.path.exists(f'{self.repo_data_path}/repo_past_name.json'):
            repo_past_name = load_json(f'{self.repo_data_path}/repo_past_name.json')
        else:
            repo_past_name = {}

        multi_thread([cve for cve, _ in incorrect_cve_dic.items()], check_repo_name, tokens = github_tokens)

        # if incorrect_cve_dic:
        #     save_json(f'{target}/incorrect_repo.json', incorrect_cve_dic)
        #     save_pickle(f'{target}/incorrect_repo.pkl', incorrect_cve_dic)
        
        save_json(f'{self.repo_data_path}/repo_past_name.json', repo_past_name)
        save_pickle(f'{self.repo_data_path}/repo_past_name.pkl', repo_past_name)

        if update_to_cve_data_all:
            for cve in data:
                self.cve_data_all[cve]['collected_repo_correction'] = cve not in incorrect_cve_dic
            save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
            save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)

        print('accuracy: {:.2f}%,'.format((len(data) - len(incorrect_cve_dic)) / len(data) * 100), 
              f'{len(data) - len(incorrect_cve_dic)}/{len(data)}, {len(incorrect_cve_dic)} rest')
        print('end check repo')


    def search_from_github(self):

        def search(product_list_sub: list, token: str):
            for product in tqdm.tqdm(product_list_sub):
                tp = set()
                # try:
                params = {'q': product, 'per_page': 1}
                search_res = search_repo(params, token)
                tp |= search_res
                    # params = {'q': product, 'per_page': 5, 'sort': 'forks'}
                    # search_res = search_repo(params, token)
                    # tp |= search_res
                    # params = {'q': product, 'per_page': 5, 'sort': 'stars'}
                    # search_res = search_repo(params, token)
                    # tp |= search_res
                # except Exception as e:
                    # print('token:', token)
                    # print(token, e)
                product_search_res[product] = tp
        
        rest_cve_list = list(
            set(self.cve_list) - set(load_pickle(f'{self.repo_from_url}/collected_repos.pkl').keys()) -
            set(load_pickle(f'{self.repo_from_gpt}/collected_repos.pkl').keys())
        )
        product_list = list({ product for cve in rest_cve_list for product in self.cve_data_all[cve]['cpe_product'] })
        
        res = {}
        product_search_res = {}
        
        print(f'start search github, rest_cve_list: {len(rest_cve_list)}, product_list: {len(product_list)}')
        multi_thread(product_list, search, tokens = github_tokens)
        
        for cve in rest_cve_list:
            tp = set()
            for product in self.cve_data_all[cve]['cpe_product']:
                tp |= product_search_res[product]
            if tp:
                res[cve] = list(tp)

        save_json(f'{self.repo_from_github}/collected_repos.json', res)
        save_pickle(f'{self.repo_from_github}/collected_repos.pkl', res)

        print(f'found {len(res)}/{len(rest_cve_list)}, {len(rest_cve_list) - len(res)} rest')
        print('end search github\n')


    def query_gpt(self):
        cve_list_done = list(load_pickle(f'{self.repo_from_url}/collected_repos.pkl').keys())
        cve_list_gpt = set(self.cve_list) - set(cve_list_done)
        rest_cve_list = list(cve_list_gpt - {cve for cve in os.listdir(f'{self.repo_from_gpt}/result') if cve not in ['.DS_Store', 'error_list']})
        cve_list_gpt = list(cve_list_gpt)

        os.makedirs(f'{self.repo_from_gpt}/prompt', exist_ok = True)
        os.makedirs(f'{self.repo_from_gpt}/result', exist_ok = True)
        
        def generate_prompt():
            print('start generate prompt')
            total_count = 0
            prompt_general = load_file(f'{self.repo_from_gpt}/prompt_query_repo')
            for cve in tqdm.tqdm(rest_cve_list):
                prompt = prompt_general.replace('{CVE}', cve)

                description = self.cve_data_all[cve]['complete_description'] if 'complete_description' in self.cve_data_all[cve] else self.cve_data_all[cve]['original_description']
                prompt = prompt.replace('{description}', description)

                cpe_uri = self.cve_data_all[cve]['cpe_uri'][0]
                prompt = prompt.replace('{CPE}', cpe_uri)

                save_text(f'{self.repo_from_gpt}/prompt/{cve}', prompt)
                total_count += calc_token(prompt, model = 'gpt-4-turbo')
            total_count_K = total_count / 1000
            print(f'end generate prompt, total token: {total_count_K}K, price: {0.005 * total_count_K}$')

        # generate_prompt()

        def query(cve_list_sub: list):
            for cve in tqdm.tqdm(cve_list_sub):
            # for cve in tqdm.tqdm(random.sample(cve_list_sub, 1)):
                res_path = f'{self.repo_from_gpt}/result/{cve}'
                if os.path.exists(res_path):
                    continue
                try:
                    prompt = load_file(f'{self.repo_from_gpt}/prompt/{cve}')
                    gpt_res = query_openai(prompt, model = 'gpt-4o')
                    save_text(res_path, gpt_res)
                except Exception as e:
                    save_text(f'{self.module_root_path}/result/error_list', f'{cve} {e}', 'a')
        
        if rest_cve_list:
            print(f'start query gpt, rest_cve_list: {len(rest_cve_list)}')
            multi_thread(rest_cve_list, query, chunk_size = 50)
            print('end query gpt')

        res = {}
        for cve in os.listdir(f'{self.repo_from_gpt}/result'):
            if cve in ['.DS_Store', 'error_list', 'CVE-2011-4619', 'CVE-2004-0791', 'CVE-2001-0596', 'CVE-2007-0046'] or cve in cve_list_done:
                continue
            res[cve] = [load_file(f'{self.repo_from_gpt}/result/{cve}').strip()]
        
        res = self.check_repo_existence(res)

        save_json(f'{self.repo_from_gpt}/collected_repos.json', res)
        save_pickle(f'{self.repo_from_gpt}/collected_repos.pkl', res)
        print(f'get {len(res)}/{len(cve_list_gpt)}, {len(cve_list_gpt) - len(res)} rest')


    def union_result(self):
        path_list = [self.repo_from_url, self.repo_from_gpt, self.repo_from_github]
        res = {}
        for path in path_list:
            full_path = f'{path}/collected_repos.json'
            data = load_json(full_path)
            print(len(data))
            res.update(data)

        print(len(res))
        save_json(f'{self.module_root_path}/collected_repos.json', res)
        save_pickle(f'{self.module_root_path}/collected_repos.pkl', res)
        
        # self.check_result(self.module_root_path)

        # for cve in self.cve_data_all:
        #     if cve not in res:
        #         print(cve)
        #     if 'collected_repo' in self.cve_data_all[cve]:
        #         del self.cve_data_all[cve]['collected_repo']

        for cve, repos in res.items():
            self.cve_data_all[cve]['collected_repo'] = repos
        save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
        save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)