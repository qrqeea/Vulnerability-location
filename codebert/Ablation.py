import os
import tqdm
import random
import pandas as pd
from util.io_util import *
from util.github_util import *
from transformers import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")

def count_tokens(text):
    tokens = tokenizer.encode(text)
    return len(tokens)


class CodeBertAblation:

    def __init__(self, module_root_path: str, repo_path: str, cve_data_all: dict, candidates: dict, repo_file_list: dict):
        self.module_root_path = module_root_path
        self.repo_path = repo_path
        self.cve_data_all = cve_data_all
        self.candidates = candidates
        self.repo_file_list = repo_file_list

        os.makedirs(module_root_path, exist_ok = True)

        # print(len(cve_list))
        self.positive_label_list = [
            (cve, self.cve_data_all[cve]['collected_commit'][0], self.cve_data_all[cve]['collected_commit'][2])
            for cve in candidates
        ]
        print(len(self.positive_label_list))


    def start(self):
        self.select_training_and_test_set()
        for i in range(5):
            print(f'round {i}')
            self.generate_data_set(
                load_pickle(f'{self.module_root_path}/tmp/training_set_{i}.pkl'),
                f'{self.module_root_path}/training_set_{i}.csv'
            )
            self.generate_data_set(
                load_pickle(f'{self.module_root_path}/tmp/test_set_{i}.pkl'),
                f'{self.module_root_path}/test_set_{i}.csv'
            )
        self.check_result()


    def select_training_and_test_set(self):
        print('start select training and test set')
        os.makedirs(f'{self.module_root_path}/tmp', exist_ok = True)
        res = self.random_split(self.positive_label_list, 5)
        for i in range(5):
            print(f'round {i}')
            # res[i]作为测试集
            test_set_pos = res[i]
            training_set_pos = []
            for j in range(5):
                if i != j:
                    training_set_pos += res[j]
            print(len(test_set_pos), len(training_set_pos))
            training_set_neg = set()
            print('start select training_set_neg')
            while len(training_set_neg) < len(training_set_pos):
                for cve, _, _ in training_set_pos:
                    rd = self.random_select_one_file(cve)
                    if rd:
                        training_set_neg.add(rd)
                        # if len(training_set_neg) == len(training_set_pos):
                        #     break
            print('end select training_set_neg')
            test_set_neg = set()
            tp = [cve for cve, _, _ in test_set_pos]
            print('start select test_set_neg')
            for cve in tp:
                repo = self.cve_data_all[cve]['collected_commit'][0]
                vul_file = self.cve_data_all[cve]['collected_commit'][2]
                for file in self.candidates[cve]:
                    if file != vul_file:
                        test_set_neg.add((cve, repo, file))
            print('end select test_set_neg')
            training_set_pos = [
                (cve, repo, file, 1)
                for cve, repo, file in training_set_pos
            ]
            training_set_neg = [
                (cve, repo, file, 0)
                for cve, repo, file in training_set_neg
            ]
            test_set_pos = [
                (cve, repo, file, 1)
                for cve, repo, file in test_set_pos
            ]
            test_set_neg = [
                (cve, repo, file, 0)
                for cve, repo, file in test_set_neg
            ]
            training_set = training_set_pos + training_set_neg
            test_set = test_set_pos + test_set_neg
            save_text(f'{self.module_root_path}/tmp/training_set_{i}', training_set)
            save_pickle(f'{self.module_root_path}/tmp/training_set_{i}.pkl', training_set)
            save_text(f'{self.module_root_path}/tmp/test_set_{i}', test_set)
            save_pickle(f'{self.module_root_path}/tmp/test_set_{i}.pkl', test_set)
        print('end select training and test set')


    def random_select_one_file(self, cve: str):
        repo = self.cve_data_all[cve]['collected_commit'][0]
        vul_file = self.cve_data_all[cve]['collected_commit'][2]
        repo_updated = repo.replace('/', '—')
        tp = self.candidates[cve].copy()
        tp.remove(vul_file)
        rd_file = random.choice(tp)
        file_path = f'{self.repo_path}/{cve}/{repo_updated}/{rd_file}'
        try:
            load_file(file_path)
            return (cve, repo, rd_file)
        except Exception:
            print(f'error, {file_path}')
            sys.exit()


    def random_split(self, data: list, parts: int):
        random.shuffle(data)
        block_size = int(len(data) / parts)
        res = []
        for i in range(parts):
            l = i * block_size
            r = l + block_size
            if i == parts - 1:
                r = len(data)
            res.append(data[l:r])
        return res


    def generate_data_set(self, data_set: list, target: str):
        df = pd.DataFrame({
            'sentence': [],
            'label': [],
        })
        for cve, repo, file, label in tqdm.tqdm(data_set):
            repo_updated = repo.replace('/', '—')
            repo_path = f'{self.repo_path}/{cve}/{repo_updated}'
            if not os.path.exists(repo_path):
                print(f'repo not exist: {cve} {repo}')
                continue
            
            file_path = file.split(f'{repo_updated}/')[-1]
            desc = self.cve_data_all[cve]['complete_description'] if 'complete_description' in self.cve_data_all[cve] else self.cve_data_all[cve]['original_description']
            desc = desc[:400]   # 大约100个token
            try:
                content = load_file(f'{self.repo_path}/{cve}/{repo_updated}/{file}')
            except Exception as e:
                print(f'{self.repo_path}/{cve}/{repo_updated}/{file}')
                print(e)
            content = clear_comment(format_text(content, ' '))
            tp = f'{repo} <|endoftext|> {file_path} <|endoftext|> {desc} <|endoftext|> '
            rest_token = 500 - count_tokens(tp)
            r = 800
            step = 50
            content_len = len(content)
            while count_tokens(content[:r]) > rest_token:
                r -= step
                if r < 0:
                    r = 0
                    break
            while count_tokens(content[:r + step]) < rest_token:
                r += step
                if r > content_len:
                    break
            # print(f'real r: {r}')
            content = content[:r]
            df.loc[len(df)] = [
                f'{tp}{content}',
                label
            ]
        
        df.to_csv(
            target,
            index = False,
        )

    
    def check_result(self):
        print('start check')
        pos_cnt1 = 0
        pos_cnt2 = 0

        for i in range(5):
            df_training = pd.read_csv(f'{self.module_root_path}/training_set_{i}.csv')
            df_test = pd.read_csv(f'{self.module_root_path}/test_set_{i}.csv')
            print(f'round {i}: training size: {len(df_training)}, test size: {len(df_test)}')
            if len(df_training.columns) != 2 or len(df_test.columns) != 2:
                print('error! column != 2')
                sys.exit()
            for index, row in df_test.iterrows():
                if row.label == 1:
                    pos_cnt1 += 1
                tokens = count_tokens(row.sentence)
                if tokens > 505:
                    print(f'test_set_{i} {index} {tokens}')
            for index, row in df_training.iterrows():
                if row.label == 1:
                    pos_cnt2 += 1
                tokens = count_tokens(row.sentence)
                if tokens > 505:
                    print(f'training_set_{i} {index} {tokens}')

        print(f'positive cnt1: {pos_cnt1}')     # 应该等于1763
        print(f'positive cnt2: {pos_cnt2}')     # 应该等于1763 * 4
        print('end check')