import os
import tqdm
import random
import pandas as pd
from util.io_util import *
from util.github_util import *
from transformers import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")

def count_tokens(text):
    tokens = tokenizer.encode(text)
    return len(tokens)


class TrainingSet:

    def __init__(self, module_root_path: str, repo_path: str, cve_data_all: dict, cve_list: list, repo_file_list: dict):
        self.module_root_path = module_root_path
        self.repo_path = repo_path
        self.cve_data_all = cve_data_all
        self.cve_list = cve_list
        self.repo_file_list = repo_file_list

        os.makedirs(module_root_path, exist_ok = True)

        # 文件名相同
        self.filter_file = [
            'changelog', 'news', 'changes', 'version', 'readme', 'license', 'authors', 'todo', 'history', 'copying', 'relnotes', 'thanks', 'notice', 'whatsnew', 'notes', 'release_notes', 'testlist', 'testsuite', 'test'
        ]
        # 以这些后缀结尾的文件
        self.filter_suffix = [
            '.md', '.txt', '.docx', '.pdf', '.rst', '.changes', '.rdoc', '.mdown',
            '.command', '.out', '.err', '.stderr', '.stdout', '.test',
            '.jpg', '.png', '.svg', '.mp4', '.gif', '.exr',
            '.csv', '.rdf',
            '.ttf', '.otf', '.woff', '.woff2',
            '.mock', '.stub', '.fake',
            '.pptx', '.key',
            '.bak', '.zip', '.gz', '.rar',
            '.gitignore',
            '.lib', '.jpeg', '.ppt', '.xlsx', '.xls', '.doc', '.ico', '.bmp', '.tar.gz', '.tgz', '.css', '.cygport',
            '.docs', '.wav'
        ]
        # 路径中包含
        self.filter_path = [
            'note', 'license', 'test'
        ]

        print(len(cve_list))
        self.positive_label_list = [
            (cve, self.cve_data_all[cve]['collected_commit'][0], self.cve_data_all[cve]['collected_commit'][2])
            for cve in cve_list
        ]
        print(len(self.positive_label_list))


    def start(self):
        self.select_training_and_test_set()
        for i in range(5):
            print(f'round {i}')
            self.generate_data_set(
                load_pickle(f'{self.module_root_path}/tmp/training_set_{i}.pkl'),
                f'{self.module_root_path}/training_set_{i}.csv'
            )
            self.generate_data_set(
                load_pickle(f'{self.module_root_path}/tmp/test_set_{i}.pkl'),
                f'{self.module_root_path}/test_set_{i}.csv'
            )
        self.check_result()


    def select_training_and_test_set(self):
        print('start select training and test set')
        os.makedirs(f'{self.module_root_path}/tmp', exist_ok = True)
        res = self.random_split(self.positive_label_list, 5)
        for i in range(5):
            print(f'round {i}')
            # res[i]作为测试集
            test_set_pos = res[i]
            training_set_pos = []
            for j in range(5):
                if i != j:
                    training_set_pos += res[j]
            print(len(test_set_pos), len(training_set_pos))
            training_set_neg = []
            print('start select training_set_neg')
            while len(training_set_neg) < len(training_set_pos):
                for cve, _, _ in training_set_pos:
                    rd = self.random_select_one_file(cve)
                    if rd:
                        training_set_neg.append(rd)
                        if len(training_set_neg) == len(training_set_pos):
                            break
            print('end select training_set_neg')
            test_set_neg = []
            # incorrect_cve = list(set(self.cve_list) - {cve for cve, _, _ in self.positive_label_list})
            # print(len(incorrect_cve))   # 534
            # tp = [cve for cve, _, _ in test_set_pos] + incorrect_cve
            tp = [cve for cve, _, _ in test_set_pos]
            print('start select test_set_neg')
            while len(test_set_neg) < len(test_set_pos) * 100:
                for cve in tp:
                    rd = self.random_select_one_file(cve)
                    if rd:
                        test_set_neg.append(rd)
                        if len(test_set_neg) == len(test_set_pos) * 100:
                            break
            print('end select test_set_neg')
            training_set_pos = [
                (cve, repo, file, 1)
                for cve, repo, file in training_set_pos
            ]
            training_set_neg = [
                (cve, repo, file, 0)
                for cve, repo, file in training_set_neg
            ]
            test_set_pos = [
                (cve, repo, file, 1)
                for cve, repo, file in test_set_pos
            ]
            test_set_neg = [
                (cve, repo, file, 0)
                for cve, repo, file in test_set_neg
            ]
            training_set = training_set_pos + training_set_neg
            test_set = test_set_pos + test_set_neg
            save_text(f'{self.module_root_path}/tmp/training_set_{i}', training_set)
            save_pickle(f'{self.module_root_path}/tmp/training_set_{i}.pkl', training_set)
            save_text(f'{self.module_root_path}/tmp/test_set_{i}', test_set)
            save_pickle(f'{self.module_root_path}/tmp/test_set_{i}.pkl', test_set)
        print('end select training and test set')


    def random_select_one_file(self, cve: str):
        for repo, sha in self.cve_data_all[cve]['collected_commit']:
            repo_updated = repo.replace('/', '—')
            # if not os.path.exists(f'{self.repo_path}/{cve}/{repo_updated}'):
            #     return None
            cnt = 0
            while True:
                # random.seed()
                file, _ = random.choice(self.repo_file_list[repo][sha])
                # print(file, isdir)
                file_lower = file.lower()
                path = '/'.join(file_lower.split('/')[:-1])
                real_file_path = f'{self.repo_path}/{cve}/{repo_updated}/{file}'
                if os.path.isfile(real_file_path) and not any(file_lower.split('/')[-1] == item for item in self.filter_file) and not any(file_lower.endswith(suffix) for suffix in self.filter_suffix) and not any(item in path for item in self.filter_path):
                    try:
                        load_file(real_file_path)
                        return (cve, repo, file)
                    except Exception:
                        pass
                        # print(f'{real_file_path} not exist')
                cnt += 1
                if cnt > 50:    # 经过规则筛选后没有文件了
                    return None


    def random_split(self, data: list, parts: int):
        random.shuffle(data)
        block_size = int(len(data) / parts)
        res = []
        for i in range(parts):
            l = i * block_size
            r = l + block_size
            if i == parts - 1:
                r = len(data)
            res.append(data[l:r])
        return res


    def generate_data_set(self, data_set: list, target: str):
        df = pd.DataFrame({
            'sentence': [],
            'label': [],
        })
        for cve, repo, file, label in tqdm.tqdm(data_set):
            repo_updated = repo.replace('/', '—')
            repo_path = f'{self.repo_path}/{cve}/{repo_updated}'
            if not os.path.exists(repo_path):
                print(f'repo not exist: {cve} {repo}')
                continue
            
            file_path = file.split(f'{repo_updated}/')[-1]
            desc = self.cve_data_all[cve]['complete_description'] if 'complete_description' in self.cve_data_all[cve] else self.cve_data_all[cve]['original_description']
            desc = desc[:400]   # 大约100个token
            try:
                content = load_file(f'{self.repo_path}/{cve}/{repo_updated}/{file}')
            except Exception as e:
                print(f'{self.repo_path}/{cve}/{repo_updated}/{file}')
                print(e)
            content = clear_comment(format_text(content, ' '))
            tp = f'{repo} <|endoftext|> {file_path} <|endoftext|> {desc} <|endoftext|> '
            rest_token = 500 - count_tokens(tp)
            r = 800
            step = 50
            content_len = len(content)
            while count_tokens(content[:r]) > rest_token:
                r -= step
                if r < 0:
                    r = 0
                    break
            while count_tokens(content[:r + step]) < rest_token:
                r += step
                if r > content_len:
                    break
            # print(f'real r: {r}')
            content = content[:r]
            df.loc[len(df)] = [
                f'{tp}{content}',
                label
            ]
        
        df.to_csv(
            target,
            index = False,
        )

    
    def check_result(self):
        print('start check')
        pos_cnt1 = 0
        pos_cnt2 = 0

        for i in range(5):
            df_training = pd.read_csv(f'{self.module_root_path}/training_set_{i}.csv')
            df_test = pd.read_csv(f'{self.module_root_path}/test_set_{i}.csv')
            print(f'round {i}: training size: {len(df_training)}, test size: {len(df_test)}')
            if len(df_training.columns) != 2 or len(df_test.columns) != 2:
                print('error! column != 2')
                sys.exit()
            for index, row in df_test.iterrows():
                if row.label == 1:
                    pos_cnt1 += 1
                tokens = count_tokens(row.sentence)
                if tokens > 505:
                    print(f'test_set_{i} {index} {tokens}')
            for index, row in df_training.iterrows():
                if row.label == 1:
                    pos_cnt2 += 1
                tokens = count_tokens(row.sentence)
                if tokens > 510:
                    print(f'training_set_{i} {index} {tokens}')

        print(f'positive cnt1: {pos_cnt1}')     # 应该等于3641
        print(f'positive cnt2: {pos_cnt2}')     # 应该等于3641 * 4
        print('end check')