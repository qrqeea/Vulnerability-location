import os

import tqdm
import tiktoken
import numpy as np
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv

from util import load_pickle, save_pickle, save_text

max_token = 8000

filter_list = [
    'changelog', 'news', 'changes', 'changelog', 'version', 'readme', 'makefile', 'license', 'authors', 'todo', 'TODO', 'history', 'copying', 'relnotes', 'thanks', 'notice','whatsnew', 'notes', 'release_notes', 'note', 'testlist', 'testsuite', 'test', '.gitignore', '.xlsx', '.xls', '.md', '.txt', '.doc', '.docx', '.pdf', '.rst', '.changes', '.rdoc', '.mdown', '.command', '.out', '.err', '.stderr', '.stdout', '.test', '.jpg', '.jpeg', '.png', '.svg', '.mp4', '.gif', '.exr', '.csv', '.rdf', '.ico', '.ttf', '.otf', '.woff', '.woff2', '.mock', '.stub', '.fake', '.ppt', '.pptx', '.key', '.bak', '.zip', '.gz', '.rar', '.bmp', '.yaml', '.yml', '.json', '.xml', '.ini', '.cfg', '.tar.gz', '.tgz', '.html', '.htm', '.css', '.cygport'
]

load_dotenv()
client = OpenAI()

def calctoken(input):
    enc = tiktoken.get_encoding("cl100k_base")
    enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
    return len(enc.encode(input))


def get_embedding(text, model = "text-embedding-3-small"):
   text = text.replace("\n", " ")
   return client.embeddings.create(input = [text], model=model).data[0].embedding


def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def rec(df, repo_name: str, full_path: str):
    file_name = full_path.split('/')[-1].lower()
    if os.path.isdir(full_path):
        for item in os.listdir(full_path):
            rec(df, repo_name, f'{full_path}/{item}')
    elif '.' not in file_name or 'test' in full_path or 'note' in full_path or 'license' in full_path or any(element in file_name for element in filter_list):
        pass
    else:
        try:
            with open(full_path, 'r') as f:
                content = f.read()
        except UnicodeDecodeError:      # 非文本文件
            return
        # TODO: 这里有bug，要重新写，可以用倍增？
        token_len = calctoken(content)
        if token_len > max_token:
            content_len = len(content)
            block_count = int(token_len / max_token) + 1
            block_size = int(content_len / block_count)
            cur = 0
            while cur + block_size < content_len:
                if content[cur : cur + block_size]:
                    df.loc[len(df)] = [repo_name, '/'.join(full_path.split('/')[7:]), content[cur : cur + block_size]]
                cur += block_size                
            else:
                if cur < content_len:
                    df.loc[len(df)] = [repo_name, '/'.join(full_path.split('/')[7:]), content[cur:]]
        elif content:
            df.loc[len(df)] = [repo_name, '/'.join(full_path.split('/')[7:]), content]


def split_file_content(cve: str):
    with open(f'/Volumes/NVD/experiment_data/267/descriptions/{cve}') as f:
        description = f.read()
    df = pd.DataFrame(
        {
            'repo_name': ['null'],
            'file_name': ['null'],
            'content': [description]
        }
    )
    path = f'/Users/wangtao/Downloads/target/{cve}'
    repos = os.listdir(path)
    for repo in tqdm.tqdm(repos):
        repo_name = repo.replace('—', '/')
        rec(df, repo_name, f'{path}/{repo}')
    df.to_csv(f'/Volumes/NVD/experiment_data/267/vector/{cve}.csv', index = False)
    return df


def text_to_vector(cve: str):
    df = split_file_content(cve)

    df['vector'] = df.content.apply(lambda x: get_embedding(x))
    description_vector = df.loc[0].vector
    df['similarities'] = df.vector.apply(lambda x: cosine_similarity(x, description_vector))
    df = df.sort_values('similarities', ascending = False)
    df.to_csv(f'/Volumes/NVD/experiment_data/267/similarities/{cve}.csv', index = False)
    return df


def select_file_candidates(k: int):
    cve_list = load_pickle('/Volumes/NVD/experiment_data/267/procedure_data/cve_list_205.pkl')
    # cve_list = [
    #     'CVE-2018-1000888', 'CVE-2015-8748', 'CVE-2019-11411', 'CVE-2019-11412', 'CVE-2017-15010', 'CVE-2021-28363', 'CVE-2020-26137', 'CVE-2019-20479', 'CVE-2022-23614', 'CVE-2019-1010305', 'CVE-2020-8203', 'CVE-2022-24724', 'CVE-2013-7436', 'CVE-2020-9274', 'CVE-2019-15026', 'CVE-2022-29869', 'CVE-2020-4067', 'CVE-2019-16235', 'CVE-2019-16236', 'CVE-2019-16237', 'CVE-2020-14399', 'CVE-2020-14400', 'CVE-2020-14401', 'CVE-2019-19204', 'CVE-2019-1010319', 'CVE-2019-1010317', 'CVE-2020-27783', 'CVE-2018-1002200', 'CVE-2022-30784', 'CVE-2018-15127', 'CVE-2018-20019'
    # ]
    res = {}
    for cve in tqdm.tqdm(cve_list):
        df = text_to_vector(cve)
        df.drop(df.index[0], inplace = True)
        
        res[cve] = []
        for _, row in df.iterrows():
            if (row.repo_name, row.file_name) not in res[cve]:
                res[cve].append((row.repo_name, row.file_name))
            if len(res[cve]) > k:
                break
    save_pickle('/Volumes/NVD/experiment_data/267/result.pkl_openai', res)
    save_text('/Volumes/NVD/experiment_data/267/result_openai', res)
    return res


if __name__ == '__main__':
    select_file_candidates(15)