import os
import csv
import ast
import tqdm
import math
import random
import pandas as pd
from dotenv import load_dotenv
from util.io_util import *


class Completion:
    
    def __init__(self, project_root_path: str, module_root_path: str, scrapy_result_dir: str, cve_data_all: dict, gpt_max_token: int):
        self.project_root_path = project_root_path
        self.module_root_path = module_root_path
        self.scrapy_result_dir = scrapy_result_dir
        self.cve_data_all = cve_data_all
        self.gpt_max_token = gpt_max_token
        
        self.cve_list = list(cve_data_all.keys())

        os.makedirs(self.module_root_path, exist_ok = True)

        self.embedding_text_dir = f'{self.module_root_path}/embedding_text'
        os.makedirs(self.embedding_text_dir, exist_ok = True)
            
        self.embedding_result_dir = f'{self.module_root_path}/embedding_result'
        os.makedirs(self.embedding_result_dir, exist_ok = True)

        self.completion_prompt_dir = f'{self.module_root_path}/completion_prompt'
        os.makedirs(self.completion_prompt_dir, exist_ok = True)

        self.completion_result_dir = f'{self.module_root_path}/completion_result'
        os.makedirs(self.completion_result_dir, exist_ok = True)


    def start(self):
        self.split_scrapy_res()
        self.count_embedding_total_token()
        self.generate_query()
        self.text_to_vector()
        self.calc_similarity()
        self.count_similarity([0.15, 0.4, 0.5, 0.6, 0.7, 0.8])
        self.generate_complete_prompt()
        self.count_prompt_total_token()
        self.complete_description()
        self.handle_result()
        

    def split_scrapy_res(self, block_max_token = 2048):
        cve_not_scrapy_count = 0
        cve_no_valid_scrapy_result_couint = 0
        for cve in tqdm.tqdm(self.cve_list):
        # for cve in tqdm.tqdm(list(random.sample(self.cve_list, 5))):
            csv_path = f'{self.scrapy_result_dir}/{cve}.csv'
            if not os.path.exists(csv_path):
                cve_not_scrapy_count += 1
                continue
            
            try:
                df = pd.read_csv(csv_path)
            except:
                print('error', cve)
            df_new = pd.DataFrame({
                'url': [],
                'token_len': [],
                'text': []
            })

            for _, row in df.iterrows():
                if not isinstance(row.text, str):
                    continue
                text_len = len(row.text)
                if 0 < text_len < 150 or row.state == 1:
                    continue
                # if 'BEGIN PGP SIGNATURE' in row.text or 'END PGP SIGNATURE' in row.text:
                #     # print(csv_path)
                #     l = row.text.find('BEGIN PGP SIGNATURE')
                #     r = row.text.find('END PGP SIGNATURE')
                #     if l != -1 and r != -1:
                #         index = (df['url'] == row.url)
                #         df.loc[index, 'text'] = row.text[l:r]
                #         df.to_csv(
                #             csv_path,
                #             index = False, 
                #             quotechar = '"',
                #             quoting = csv.QUOTE_ALL
                #         )
                #         print(csv_path, l ,r)
                #     break

                token_len = calc_token(row.text)
                if token_len > block_max_token:
                    block_size = math.ceil(token_len / block_max_token)
                    block_len = int(text_len / block_size)
                    l = 0
                    for i in range(1, block_size):
                        r = block_len * i
                        tp = 0
                        while r < text_len:
                            if not row.text[r].isprintable() or any(row.text[r] == char for char in [' ', '\n', ',', '.']):
                                break
                            r -= 1
                            tp += 1
                            if tp > 300:
                                print(csv_path, row.url)
                                break                             
                        if row.text[l:r]:
                            df_new.loc[len(df_new)] = [
                                row.url,
                                f'{calc_token(row.text[l:r])}/{token_len}',
                                row.text[l:r]
                            ]
                        l = r
                    if row.text[l:]:
                        df_new.loc[len(df_new)] = [
                            row.url,
                            f'{calc_token(row.text[l:])}/{token_len}',
                            row.text[l:]
                        ]
                else:
                    df_new.loc[len(df_new)] = [row.url, f'{token_len}/{token_len}', row.text]
            if len(df_new) > 0:
                df_new.to_csv(
                    f'{self.embedding_text_dir}/{cve}.csv',
                    index = False,
                    quotechar = '"',
                    quoting = csv.QUOTE_ALL
                )
            else:
                cve_no_valid_scrapy_result_couint += 1
        print('not scrapy cve count:', cve_not_scrapy_count)
        print('no valid scrapy result cve count:', cve_no_valid_scrapy_result_couint)


    def generate_query(self):
        
        print('start generate query_to_embedding.csv')

        new_cve_list = [file.split('.')[0] for file in os.listdir(self.embedding_text_dir) if file not in ['.DS_Store']]

        df = pd.DataFrame({
            'cve': [],
            'text': []
        })
        # for file in tqdm.tqdm(list(random.sample(new_cve_list, 5))):
        for file in tqdm.tqdm(new_cve_list):
            cve = file.split('.')[0]
            cpe_info = 'Affected Software: '
            for product in self.cve_data_all[cve]['cpe_product']:
                cpe_info += product + ' '
            original_description = self.cve_data_all[cve]['original_description']
            text = f'{cve}\nDescription\n{original_description}\n{cpe_info}'
            df.loc[len(df)] = [cve, text]
        df.to_csv(
            f'{self.module_root_path}/query_to_embedding.csv',
            index = False, 
            quotechar = '"',
            quoting = csv.QUOTE_ALL
        )
        print('end generate query_to_embedding.csv, size:', len(df))


    def text_to_vector(self):
        
        def get_embedding(text: str):
            return client.embeddings.create(input = [text], model = 'text-embedding-3-small').data[0].embedding
        
        def query_text_to_vector(cve_list: list):
            for cve in tqdm.tqdm(cve_list):
                row = df_query.index[df_query['cve'] == cve]
                tp = df_query.loc[row, 'vector']
                if tp.values == [None]:
                    df_query.loc[row, 'vector'] = str(get_embedding(str(df_query.loc[row].text)))
                else:
                    print('skip')
                

        print('start embedding')

        load_dotenv()
        client = OpenAI()

        path = f'{self.module_root_path}/query_embedding_result.csv'
        if not os.path.exists(path):
            df_query = pd.read_csv(f'{self.module_root_path}/query_to_embedding.csv')
            print('start embedding query')
            df_query['vector'] = None
            cve_list = list(df_query.cve)
            multi_thread(cve_list, query_text_to_vector, chunk_size = 700)

            df_query.to_csv(
                f'{self.module_root_path}/{path}',
                index = False, 
                quotechar = '"',
                quoting = csv.QUOTE_ALL
            )
            print('end embedding query')
        else:
            print('already embedding query')

        def cve_text_to_vector(cve_list: list):
            for cve in tqdm.tqdm(cve_list):
                target_path = f'{self.embedding_result_dir}/{cve}.csv'
                if os.path.exists(target_path):
                    continue

                df = pd.read_csv(f'{self.embedding_text_dir}/{cve}.csv')
                df['vector'] = df.text.apply(lambda x: get_embedding(x))
                df.to_csv(
                    target_path,
                    index = False, 
                    quotechar = '"',
                    quoting = csv.QUOTE_ALL
                )

        print('start embedding cve')

        rest_cve = list(
            {file.split('.')[0] for file in os.listdir(self.embedding_text_dir) if file not in ['.DS_Store']} -
            {file.split('.')[0] for file in os.listdir(self.embedding_result_dir) if file not in ['.DS_Store']}
        )
        print(len(rest_cve))
        multi_thread(rest_cve, cve_text_to_vector, chunk_size = 150)

        print('start embedding cve')
        print('end embedding')


    def calc_similarity(self):
        print('start calc similarity')

        df_query = pd.read_csv(f'{self.module_root_path}/query_embedding_result.csv')
        tp = {}
        for _, row in df_query.iterrows():
            tp[row.cve] = ast.literal_eval(row.vector)

        for file in tqdm.tqdm(os.listdir(self.embedding_result_dir)):
            if file in ['.DS_Store']: continue
            
            cve = file.split('.')[0]
            df = pd.read_csv(f'{self.embedding_result_dir}/{cve}.csv')
            if len(df.columns) == 5:
                continue
            query_vector = tp[cve]

            df['similarity'] = df.vector.apply(lambda x: cosine_similarity(ast.literal_eval(x), query_vector))
            df = df.sort_values('similarity', ascending = False)
            df.to_csv(
                f'{self.embedding_result_dir}/{cve}.csv',
                index = False, 
                quotechar = '"',
                quoting = csv.QUOTE_ALL
            )
        print('end calc similarity')


    def generate_complete_prompt(self):

        def select_knowledge_block(cve: str, token_used: int):
            res = ''
            df = pd.read_csv(f'{self.embedding_result_dir}/{cve}.csv')
            for _, row in df.iterrows():
                if row.similarity >= 0.458:
                    if calc_token(res + row.text, 'gpt-3.5-turbo') <= self.gpt_max_token - token_used:
                        res += row.text + '\n\n'
                    else:
                        break
            return res

        cnt = 0
        for file in tqdm.tqdm(os.listdir(self.embedding_result_dir)):
            if file in ['.DS_Store']: continue

            cve = file.split('.')[0]
            with open(f'{self.module_root_path}/prompt_complete_template', 'r') as f:
                prompt = f.read()
            prompt = prompt.replace('{cve}', cve)
            prompt = prompt.replace('{original description}', self.cve_data_all[cve]['original_description'])
            
            knowledge = select_knowledge_block(cve, calc_token(prompt, 'gpt-3.5-turbo'))
            if len(knowledge) < 75:
                continue
            cnt += 1
            prompt = prompt.replace('{reference information}', knowledge)
            save_text(f'{self.completion_prompt_dir}/{cve}', prompt)
        
        print(f'generate {cnt} cve completion prompt')


    def count_prompt_total_token(self):
        total = 0
        for cve in tqdm.tqdm(os.listdir(self.completion_prompt_dir)):
            if cve in ['.DS_Store']: continue
            with open(f'{self.completion_prompt_dir}/{cve}', 'r') as f:
                token = calc_token(f.read(), 'gpt-3.5-turbo')
                total += token
                if token > self.gpt_max_token:
                    print(cve, token)
        token_M = int(total / 1000000)
        print(f'prompt total token: {token_M}M, price: {token_M * 0.5}$')


    def complete_description(self):

        def complete(cve_list: list):
            for cve in tqdm.tqdm(cve_list):
                if os.path.exists(f'{self.completion_result_dir}/{cve}'):
                    continue
                with open(f'{self.completion_prompt_dir}/{cve}') as f:
                    prompt = f.read()
                try:
                    res = query_openai(prompt)
                    save_text(f'{self.completion_result_dir}/{cve}', res)
                except Exception as e:
                    save_text(f'{self.completion_result_dir}/error_list', f'{cve}\n\n{e}', 'a')
                
        rest_cve = list(
            {file.split('.')[0] for file in os.listdir(self.completion_prompt_dir) if file not in ['.DS_Store']} -
            {file.split('.')[0] for file in os.listdir(self.completion_result_dir) if file not in ['.DS_Store']}
        )
        print(len(rest_cve))
        multi_thread(rest_cve, complete, chunk_size = 2000)

    def handle_result(self):
        cve_list = [file.split('.')[0] for file in os.listdir(self.completion_result_dir) if file not in ['.DS_Store', 'error_list']]
        for cve in cve_list:
            self.cve_data_all[cve]['complete_description'] = load_file(f'{self.completion_result_dir}/{cve}')
        save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
        save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)


    def count_embedding_total_token(self):
        total = 0
        for cve in tqdm.tqdm(os.listdir(self.embedding_text_dir)):
            if cve in ['.DS_Store']: continue
            sum = 0
            df = pd.read_csv(f'{self.embedding_text_dir}/{cve}')
            for text in df.text:
                # if isinstance(text, str):
                sum += calc_token(text)
            total += sum
        token_M = int(total / 1000000)
        print(f'embedding total token: {token_M}M, price: {token_M * 0.02}$')


    def count_similarity(self, interval: list):
        similarity_all = []
        for file in os.listdir(self.embedding_result_dir):
        # for file in tqdm.tqdm(os.listdir(self.embedding_result_dir)):
            if file in ['.DS_Store']: continue

            df = pd.read_csv(f'{self.embedding_result_dir}/{file}')
            for _, row in df.iterrows():
                # if 0.1 <= row.similarity < 0.15:
                if 0.7 < row.similarity:
                    print(file, row.url)
            similarity = list(df.similarity)
            similarity_all += similarity
            # print(similarity)
        # print(len(similarity_all))
        print(count_range(similarity_all, interval))