import os
import json
import tqdm
import importlib
from util import query_openai, get_domain, copy_file, load_pickle, save_text, load_json, count_range, calc_token

max_token = 8192

config = {
    "particular_domain_list": [
        "lists.apache.org",
        "access.redhat.com",
        "lists.fedoraproject.org",
        "lists.debian.org",
        "www.debian.org",
        "lists.opensuse.org",
        "security.gentoo.org",
        "www.openwall.com",
        "bugzilla.redhat.com",
        "security.netapp.com",
        "huntr.dev",
        "seclists.org",
        "snyk.io",
        "packetstormsecurity.com",
        "www.redhat.com",
        "exchange.xforce.ibmcloud.com",
        "www.ubuntu.com",
        "jenkins.io",
        "marc.info",
        "www.exploit-db.com",
        "www.tenable.com"
    ],
    "not_handle_domain_list": [
        "ftp.caldera.com",
        "cert-portal.siemens.com"
    ]
}
modules = {}


# def complete_description_single(cve: str):
#     with open('prompt_template/prompt_complete_description', 'r') as f:
#         prompt_all = f.read()
#     prompt_all = prompt_all.replace('{#original description#}', get_original_description(cve)) 

#     data_dir = f'experiment_data/267/completion_description_data/{cve}'

#     prompt_list = generate_prompt_list(cve)
#     print(f"prompt_list_len: {len(prompt_list)}")
#     for index, prompt in enumerate(prompt_list):
#         if len(prompt) > 20000:     # 超出token上限先不处理
#             continue
#         print('querying openai...')
#         res = query_openai(prompt)
#         with open(f'{data_dir}/openai_res_{index}', 'w') as f:
#             print(res, file = f)
#         # 需要判断一下结果是不是'no result'或者内容长度太少
#         if len(res) > 180:
#             prompt_all += res + '\n\n'

#     with open(f'{data_dir}/prompt_all', 'w') as f:
#         print(prompt_all, file = f)
    
#     res = query_openai(prompt_all)
#     with open(f'{data_dir}/openai_res', 'w') as f:
#         print(res, file = f)
#     return res


# def gather_result():
#     dir = 'experiment_data/267/completion_description_data'
#     for cve in os.listdir(dir):
#         if 'CVE' not in cve:
#             continue
        # copy_file(f'{dir}/{cve}/openai_res', f'{dir}/descriptions/{cve}')


class Completion:
    
    def __init__(self, root_path: str):
        self.root_path = root_path
        self.cve_list = load_pickle(f'{self.root_path}/cve_list.pkl')

        dir = f'{self.root_path}/procedure_data'
        if not os.path.exists(dir):
            os.mkdir(dir)


    def get_reference_url_list(self, cve: str):
        filePath = f'{self.root_path}/cve_json/{cve}.json'
        reference_list = load_json(filePath)['containers']['cna']['references']
        return reference_list


    def count_url(self, interval: list):
        reference_len_list = [
            len(self.get_reference_url_list(cve))
            for cve in self.cve_list
        ]
        res = count_range(reference_len_list, interval)
        print(res)


    def get_original_description(self, cve: str):
        filePath = f'{self.root_path}/cve_json/{cve}.json'
        description = load_json(filePath)['containers']['cna']['descriptions'][0]['value']
        return description


    def count_descriptions(self, interval: list):
        description_len_list = [
            len(self.get_original_description(cve).split(' '))
            for cve in self.cve_list
        ]
        res = count_range(description_len_list, interval)
        print(res)


    def count_domain(self):
        url_list_dir = f'{self.root_path}/url_list'
        if not os.path.exists(url_list_dir):
            os.mkdir(url_list_dir)

        res = {}
        for cve in self.cve_list:
            url_list = self.get_reference_url_list(cve)
            for dic in url_list:
                url = dic['url']
                domain = get_domain(url)
                save_text(f'{url_list_dir}/{domain}', url, 'a')
                if domain in res.keys():
                    res[domain] += 1
                else:
                    res[domain] = 1
        # res = dict(sorted(res.items(), key = lambda item: item[1], reverse = True))
        # print(res)


    def complete_single(self, cve: str):

        def get_module_name(domain: str):
            module_name = domain.replace('.', '_').replace('-', '_')
            if module_name == 'www_debian_org':     # 同一个网站
                module_name = 'lists_debian_org'
            return module_name
        
        def split_scrapy_res(scrapy_res_list: list):
            res = []
            for content in scrapy_res_list:
                if calc_token(content) > max_token:
                    pass    # TODO split
                else:
                    res.append(content)
            return res

        dir = f'{self.root_path}/procedure_data/{cve}'
        os.mkdir(dir)
        os.mkdir(f'{dir}/scrapy_result')

        references = self.get_reference_url_list(cve)
        scrapy_res_list = []

        for dic in references:
            url = dic['url']
            domain = get_domain(url)
            if domain in config['particular_domain_list']:
                module_name = get_module_name(domain)
                if module_name not in modules.keys():
                    # TODO 可以改成相对导入？
                    modules[module_name] = importlib.import_module(f'prompt_generator.{module_name}')
                scrapy_res = modules[module_name].generate_prompt(url)
                if scrapy_res != None:
                    scrapy_res_list.append(scrapy_res)
                    save_text(f'{dir}/scrapy_result/{domain}', scrapy_res + '\n' + '_' * 50, 'a')
            elif domain in config['not_handle_domain_list']:
                pass
            else:       # 通用
                # print(f'common url: {url}')
                from prompt_generator import common
                scrapy_res = common.generate_prompt(url)
                if scrapy_res != None:
                    scrapy_res_list.append(scrapy_res)
                    save_text(f'{dir}/scrapy_result/{domain}', scrapy_res + '\n' + '_' * 50, 'a')
        
        scrapy_res_list = split_scrapy_res(scrapy_res_list)
        # TODO 用dataframe，保存为csv


    def complete_all(self):
        for cve in tqdm.tqdm(self.cve_list):
            # 已存在对应目录就认为已经完成补全
            if not os.path.exists(f'{self.root_path}/procedure_data/{cve}'):
                # print(f'current cve: {cve}')
                self.complete_single(cve)