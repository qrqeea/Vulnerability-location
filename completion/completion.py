import os
import sys
import csv
import tqdm
import random
import threading
import importlib
import pandas as pd
from util import *


config = {
    "particular_domain_list": [
        "lists.apache.org",
        "access.redhat.com",
        "lists.fedoraproject.org",
        "lists.debian.org",
        "www.debian.org",
        "lists.opensuse.org",
        "security.gentoo.org",
        "www.openwall.com",
        "bugzilla.redhat.com",
        "security.netapp.com",
        "huntr.dev",
        "seclists.org",
        "snyk.io",
        "www.gocd.org",
        "packetstormsecurity.com",
        "www.redhat.com",
        "exchange.xforce.ibmcloud.com",
        "www.ubuntu.com",
        "jenkins.io",
        "docs.saltstack.com",
        "marc.info",
        "www.exploit-db.com",
        "www.tenable.com",
        "github.com",
        "tanzu.vmware.com",
        "pivotal.io",
        "source.android.com",
        "support.hpe.com",
        "h20566.www2.hpe.com",
        "www.kb.cert.org",
        "metacpan.org",
        "tomcat.apache.org",
        "patchwork.kernel.org",
        "x-stream.github.io",
        "nokogiri.org",
        "borgbackup.readthedocs.io",
        "docs.zephyrproject.org",
        "tools.ietf.org",
        "mantisbt.org",
        "bugs.launchpad.net"
    ],
    "not_handle_domain_list": [
        "php.net"
        "sogo.nu",
        "nmap.org",
        "ftp.sco.com",
        "www.php.net",
        "openvpn.net",
        "www.ciac.org",
        "www.vupen.com",
        "www.phpbb.com",
        "kc.mcafee.com",
        "hackerone.com",
        "www.osvdb.org",
        "www.gitpod.io",
        "www.adobe.com",
        "git.kernel.org",
        "www.oracle.com",
        "rhn.redhat.com",
        "www.seebug.org",
        "ftp.netbsd.org",
        "blog.lizzie.io",
        "docs.jboss.org",
        "docs.gitlab.com",
        "nodesecurity.io",
        "ftp.freebsd.org",
        "ftp.FreeBSD.org",
        'help.rapid7.com',
        "docs.oracle.com",
        "ftp.caldera.com",
        "trac.ffmpeg.org",
        "spec.matrix.org",
        "www.mandriva.com",
        "cloud.google.com",
        "www.cloudscan.me",
        "www.paramiko.org",
        "httpd.apache.org",
        "groups.google.com",
        "support.apple.com",
        "bugs.chromium.org",
        "beanvalidation.org",
        "docs.microsoft.com",
        "www.rfc-editor.org",
        "blog.quarkslab.com",
        "www.nccgroup.trust",
        "h30499.www3.hp.com",
        "kb.pulsesecure.net",
        "h20000.www2.hp.com",
        "oval.cisecurity.org",
        "www.codeigniter.com",
        "developer.pidgin.im",
        "pgbouncer.github.io",
        "www.springsource.com",
        "news.ycombinator.com",
        "www.securityfocus.com",
        "firejail.wordpress.com",
        "docs.djangoproject.com",
        "archives.neohapsis.com",
        "cert-portal.siemens.com",
        "research.checkpoint.com",
        "www.securitytracker.com",
        "minerva.crocs.fi.muni.cz",
        "docs.python-requests.org",
        "marketplace.atlassian.com",
        "googleprojectzero.blogspot.com"
    ]
}

modules = {}


class Completion:
    
    def __init__(self, root_path: str, max_token: int):
        self.root_path = root_path
        self.max_token = max_token
        self.cve_list = load_pickle(f'{self.root_path}/cve_list.pkl')

        self.completion_dir = f'{self.root_path}/completion'
        if not os.path.exists(self.completion_dir):
            os.mkdir(self.completion_dir)

        self.url_list_dir = f'{self.root_path}/completion/url_list'
        if not os.path.exists(self.url_list_dir):
            os.mkdir(self.url_list_dir)

        self.scrapy_result_dir = f'{self.root_path}/completion/scrapy_result'
        if not os.path.exists(self.scrapy_result_dir):
            os.mkdir(self.scrapy_result_dir)

        self.embedding_result_dir = f'{self.root_path}/completion/embedding_result'
        if not os.path.exists(self.embedding_result_dir):
            os.mkdir(self.embedding_result_dir)

        self.completion_prompt_dir = f'{self.root_path}/completion/completion_prompt'
        if not os.path.exists(self.completion_prompt_dir):
            os.mkdir(self.completion_prompt_dir)

        self.completion_result_dir = f'{self.root_path}/completion/completion_result'
        if not os.path.exists(self.completion_result_dir):
            os.mkdir(self.completion_result_dir)

        self.error_list = f'{self.scrapy_result_dir}/error_list.csv'
        if not os.path.exists(self.error_list):
            pd.DataFrame({
                'cve': [],
                'url': [],
                'domain': [],
                'error': []
            }).to_csv(
                self.error_list,
                index = False,
                quotechar = '"',
                quoting = csv.QUOTE_ALL
            )


    def get_reference_url_list(self, cve: str):
        filePath = f'{self.root_path}/cve_json/{cve}.json'
        reference_list = [dic['url'] for dic in load_json(filePath)['containers']['cna']['references']]
        return reference_list


    def count_url(self, interval: list):
        reference_len_list = [
            len(self.get_reference_url_list(cve))
            for cve in self.cve_list
        ]
        res = count_range(reference_len_list, interval)
        print(res)


    def get_original_description(self, cve: str):
        filePath = f'{self.root_path}/cve_json/{cve}.json'
        description = load_json(filePath)['containers']['cna']['descriptions'][0]['value']
        return description


    def count_descriptions(self, interval: list):
        description_len_list = [
            len(self.get_original_description(cve).split(' '))
            for cve in self.cve_list
        ]
        res = count_range(description_len_list, interval)
        print(res)


    def count_domain(self):
        res = {}
        for cve in self.cve_list:
            url_list = self.get_reference_url_list(cve)
            for dic in url_list:
                url = dic['url']
                domain = get_domain(url)
                save_text(f'{self.url_list_dir}/{domain}', url, 'a')
                if domain in res.keys():
                    res[domain] += 1
                else:
                    res[domain] = 1
        # res = dict(sorted(res.items(), key = lambda item: item[1], reverse = True))
        # print(res)


    def get_module_name(self, domain: str):
        module_name = domain.replace('.', '_').replace('-', '_')
        if module_name == 'www_debian_org':
            module_name = 'lists_debian_org'
        elif module_name == 'h20566_www2_hpe_com':
            module_name = 'support_hpe_com'
        elif module_name in ['usn_ubuntu_com', 'ubuntu_com']:
            module_name = 'www_ubuntu_com'
            
        return module_name


    def scrapy_single_url(self, cve: str, url: str, retry = False, save_to_error_list = True):
        domain = get_domain(url)
        state = 0           # 0表示无异常
        try:
            if domain in config['particular_domain_list']:
                if domain == 'tomcat.apache.org':
                    url += f'#{cve}'
                module_name = self.get_module_name(domain)
                # print(module_name)
                if module_name not in modules.keys():
                    modules[module_name] = importlib.import_module(f'completion.scrapy.{module_name}')
                res = modules[module_name].scrapy(url)
            elif domain in config['not_handle_domain_list']:
                return [url, domain, 0, '']
            else:
                # print(f'common url: {url}')
                from completion.scrapy import common
                res = common.scrapy(url)
        except Exception as e:
            if hasattr(e, 'status') and e.status == 403 and not retry:     # 网络异常且是第一次尝试
                res = self.scrapy_single_url(cve, url, True)
                return res
            else:
                res = e
                state = 1
                if save_to_error_list:
                    df = pd.read_csv(self.error_list)
                    df.loc[len(df)] = [cve, url, domain, e]
                    df.to_csv(
                        self.error_list,
                        index = False, 
                        quotechar = '"',
                        quoting = csv.QUOTE_ALL
                    )
        return [url, domain, state, res]


    def scrapy_all_url(self):

        def scrapy_sub_url(cve_list: list):
            for cve in tqdm.tqdm(cve_list):
                if os.path.exists(f'{self.scrapy_result_dir}/{cve}_raw.csv'):
                    continue
                # print('cve:', cve)
                url_list = self.get_reference_url_list(cve)
                df = pd.DataFrame({
                    'url': [],
                    'domain': [],
                    'state': [],
                    'text': []
                })
                # for url in tqdm.tqdm(url_list):
                for url in url_list:
                    df.loc[len(df)] = self.scrapy_single_url(cve, url)
                df.to_csv(
                    f'{self.scrapy_result_dir}/{cve}_raw.csv',
                    index = False, 
                    quotechar = '"',
                    quoting = csv.QUOTE_ALL
                )
        
        # scrapy_sub_url(list(random.sample(self.cve_list, 5)))
        # scrapy_sub_url(['CVE-2004-0104'])
        

        chunk_size = 200
        chunks = [self.cve_list[i:i + chunk_size] for i in range(0, len(self.cve_list), chunk_size)]
        # print(len(chunks))

        threads = []
        for i in range(len(chunks)):
            thread = threading.Thread(target = scrapy_sub_url, args = (chunks[i],))
            threads.append(thread)
        
        for thread in threads:
            thread.start()

        for thread in threads:
            thread.join()
        
        # df = self.split_scrapy_res(df)
        # df.to_csv(f'{self.scrapy_result_dir}/{cve}.csv', index = False)
        

    def split_scrapy_res(self, df: pd.DataFrame):
        res = pd.DataFrame({
            'url': [],
            'domain': [],
            'text': []
        })
        block_len = int(self.max_token / 7)
        for line in df:
            # TODO 空文本
            if line.state == 1:     # TODO 错误处理
                continue
            token_len = calc_token(line.text)
            text_len = len(line.text)
            if token_len > block_len:
                l = 0
                r = 0
                while l < text_len:
                    while r < text_len and calc_token(line.text[l:r]) < block_len:
                        r += 200
                    else:
                        if r > text_len:
                            r = text_len
                        if line.text[l:r]:
                            res.loc[len(res)] = [line.url, line.text[l:r]]
                        l = r
            else:
                res.loc[len(res)] = [line.url, line.text]
        return res


    def complete_all(self):
        self.scrapy_all_url()
        for cve in tqdm.tqdm(self.cve_list):
            # 已存在对应目录就认为已经完成补全
            if not os.path.exists(f'{self.completion_result_dir}/{cve}'):
                # print(f'current cve: {cve}')
                # self.complete_single(cve)
                pass
    

    def handle_error_list(self):
        if not os.path.exists(self.error_list):
            return
        df_error = pd.read_csv(self.error_list)
        for _, row in tqdm.tqdm(df_error.iterrows()):
            df_cve = pd.read_csv(f'{self.scrapy_result_dir}/{row.cve}_raw.csv')
            res = self.scrapy_single_url(row.cve, row.url, save_to_error_list = False)
            if res[2] == 0:
                df_cve.loc[df_cve['url'] == row.url] = res
                df_cve.to_csv(
                    f'{self.scrapy_result_dir}/{row.cve}_raw.csv',
                    index = False, 
                    quotechar = '"',
                    quoting = csv.QUOTE_ALL
                )
                row.error = 'done'
                print(f'{row.cve}, {row.url} update success')
            else:
                print(f'{row.cve}, {row.url} updated failure')
        df_error = df_error.drop(df_error[df_error['error'] == 'done'].index)
        df_error.to_csv(
            self.error_list,
            index = False, 
            quotechar = '"',
            quoting = csv.QUOTE_ALL
        )


    def count_scrapy_result(self):
        for cve in self.cve_list:
            # print(cve)
            df = pd.read_csv(f'{self.scrapy_result_dir}(old)/{cve}_raw.csv')
            for _, row in df.iterrows():
                # print(row.url)
                if isinstance(row.text, str) and len(row.text) > 50000:
                    print(cve, row.url)


    def count_total_token(self):
        total = 0
        for cve in tqdm.tqdm(self.cve_list):
            sum = 0
            df = pd.read_csv(f'{self.scrapy_result_dir}/{cve}_raw.csv')
            for text in df.text:
                if isinstance(text, str):
                    sum += calc_token(text, 'text-embedding-3-small')
            # print(cve, sum)
            total += sum
        print(total)