import os
import csv
import ast
import tqdm
import math
import random
import importlib
import pandas as pd
from dotenv import load_dotenv
from util.io_util import *


config = {
    "particular_domain_list": [
        "lists.apache.org",
        "access.redhat.com",
        "lists.fedoraproject.org",
        "lists.debian.org",
        "www.debian.org",
        "lists.opensuse.org",
        "security.gentoo.org",
        "openwall.com",
        "www.openwall.com",
        "bugzilla.redhat.com",
        "security.netapp.com",
        "huntr.dev",
        "seclists.org",
        "snyk.io",
        "www.gocd.org",
        "packetstormsecurity.com",
        "www.redhat.com",
        "exchange.xforce.ibmcloud.com",
        "www.ubuntu.com",
        "usn.ubuntu.com",
        "ubuntu.com",
        "jenkins.io",
        "docs.saltstack.com",
        "marc.info",
        "www.exploit-db.com",
        "www.tenable.com",
        "github.com",
        "tanzu.vmware.com",
        "pivotal.io",
        "source.android.com",
        "support.hpe.com",
        "h20566.www2.hpe.com",
        "kb.cert.org",
        "www.kb.cert.org",
        "metacpan.org",
        "tomcat.apache.org",
        "patchwork.kernel.org",
        "x-stream.github.io",
        "nokogiri.org",
        "borgbackup.readthedocs.io",
        "docs.zephyrproject.org",
        "tools.ietf.org",
        "mantisbt.org",
        "bugs.launchpad.net",
        "bugs.debian.org",
        "mitogen.networkgenomics.com",
        "supervisord.org",
        "docs.pylonsproject.org",
        "www.facebook.com",
        "talosintelligence.com",
        "www.talosintelligence.com",
        "gstreamer.freedesktop.org",
        "bugzilla.suse.com",
        "launchpad.net"
    ],
    "not_handle_domain_list": [
        "php.net",
        "sogo.nu",
        "nmap.org",
        "java.net",
        "ffmpeg.org",
        "ftp.sco.com",
        "verneet.com",
        "www.php.net",
        "openvpn.net",
        "sumofpwn.nl",
        "git.php.net",
        "twitter.com",
        "share.ez.no",
        "www.lua.org",
        "www.ciac.org",
        "www.golem.de",
        "www.kame.net",
        "www.vupen.com",
        "www.opera.com",
        "lab.louiz.org",
        "securiteam.io",
        "www.phpbb.com",
        "kc.mcafee.com",
        "hackerone.com",
        "www.osvdb.org",
        "www.gitpod.io",
        "www.adobe.com",
        "www.xerox.com",
        "code.foxkit.us",
        "blog.redash.io",
        "git.kernel.org",
        "www.oracle.com",
        "rhn.redhat.com",
        "www.seebug.org",
        "ftp.netbsd.org",
        "blog.lizzie.io",
        "docs.jboss.org",
        "www.soluble.ai",
        "www.dulwich.io",
        "www.ffmpeg.org",
        "www.novell.com",
        "www.kernel.org",
        "www.caldera.com",
        "kb.bluecoat.com",
        "patches.sgi.com",
        "docs.gitlab.com",
        "nodesecurity.io",
        "ftp.freebsd.org",
        "ftp.FreeBSD.org",
        "www.ss-proj.org",
        'help.rapid7.com',
        "docs.oracle.com",
        "ftp.caldera.com",
        "trac.ffmpeg.org",
        "spec.matrix.org",
        "www.cigital.com",
        "www.spinics.net",
        "www.htbridge.ch",
        "tools.cisco.com",
        "blog.phusion.nl",
        "www.tcpdump.org",
        "www.youtube.com",
        "rt-solutions.de",
        "wiki.scn.sap.com",
        "issues.rpath.com",
        "www2.itrc.hp.com",
        "www.mandriva.com",
        "cloud.google.com",
        "www.cloudscan.me",
        "www.paramiko.org",
        "httpd.apache.org",
        "git.chromium.org",
        "blogs.akamai.com",
        "www.darkmatter.ae",
        "groups.google.com",
        "vsftpd.beasts.org",
        "support.apple.com",
        "bugs.chromium.org",
        "www.hashicorp.com",
        "www.ventuneac.net",
        "www.hackpuntes.com",
        "bugzilla.fedora.us",
        "elixir.bootlin.com",
        "beanvalidation.org",
        "bugzilla.libav.org",
        "docs.microsoft.com",
        "www.rfc-editor.org",
        "ssd-disclosure.com",
        "blog.quarkslab.com",
        "www.nccgroup.trust",
        "h30499.www3.hp.com",
        "kb.pulsesecure.net",
        "h20000.www2.hp.com",
        "cpansearch.perl.org",
        "ruia-ruia.github.io",
        "oval.cisecurity.org",
        "ffmpeg.mplayerhq.hu",
        "www.dnnsoftware.com",
        "hermes.opensuse.org",
        "www.codeigniter.com",
        "developer.pidgin.im",
        "pgbouncer.github.io",
        "jira.hyperledger.org",
        "www.bouncycastle.org",
        "pyyaml.docsforge.com",
        "www.bountysource.com",
        "git.dereferenced.org",
        "www.springsource.com",
        "news.ycombinator.com",
        "blog.forallsecure.com",
        "www.securityfocus.com",
        "help.ecostruxureit.com",
        "firejail.wordpress.com",
        "membership.backbox.org",
        "www.digitalmunition.me",
        "docs.djangoproject.com",
        "archives.neohapsis.com",
        "distro.conectiva.com.br",
        "esupport.trendmicro.com",
        "cert-portal.siemens.com",
        "research.checkpoint.com",
        "www.securitytracker.com",
        "bertjwregeer.keybase.pub",
        "minerva.crocs.fi.muni.cz",
        "docs.python-requests.org",
        "marketplace.atlassian.com",
        "www.portcullis-security.com",
        "news.dieweltistgarnichtso.net",
        "googleprojectzero.blogspot.com"
    ]
}

modules = {}


class Completion:
    
    def __init__(self, project_root_path: str, module_root_path: str, cve_data_all: dict, gpt_max_token: int):
        self.project_root_path = project_root_path
        self.module_root_path = module_root_path
        self.cve_data_all = cve_data_all
        self.gpt_max_token = gpt_max_token
        
        self.cve_list = list(cve_data_all.keys())

        self.url_list_dir = f'{self.module_root_path}/url_list'
        os.makedirs(self.url_list_dir, exist_ok = True)

        self.scrapy_result_dir = f'{self.module_root_path}/scrapy_result'
        os.makedirs(self.scrapy_result_dir, exist_ok = True)

        self.embedding_text_dir = f'{self.module_root_path}/embedding_text'
        os.makedirs(self.embedding_text_dir, exist_ok = True)
            
        self.embedding_result_dir = f'{self.module_root_path}/embedding_result'
        os.makedirs(self.embedding_result_dir, exist_ok = True)

        self.completion_prompt_dir = f'{self.module_root_path}/completion_prompt'
        os.makedirs(self.completion_prompt_dir, exist_ok = True)

        self.completion_result_dir = f'{self.module_root_path}/completion_result'
        os.makedirs(self.completion_result_dir, exist_ok = True)

        self.error_list = f'{self.scrapy_result_dir}/error_list.csv'
        if not os.path.exists(self.error_list):
            pd.DataFrame({
                'cve': [],
                'url': [],
                'domain': [],
                'text': []
            }).to_csv(
                self.error_list,
                index = False,
                quotechar = '"',
                quoting = csv.QUOTE_ALL
            )

        self.long_text = f'{self.scrapy_result_dir}/long_text.csv'
        self.short_text = f'{self.scrapy_result_dir}/short_text.csv'


    def start(self):
        # self.count_domain()
        self.scrapy_all_url()
        self.handle_error_list()
        self.count_scrapy_result()
        self.split_scrapy_res()
        self.count_total_token()
        self.generate_query()
        self.text_to_vector()
        self.calc_similarity()
        self.count_similarity([0.15, 0.4, 0.5, 0.6, 0.7, 0.8])
        self.generate_complete_prompt()
        self.complete_description()


    def count_url(self, interval: list):
        reference_len_list = [
            len(self.cve_data_all[cve]['reference_list'])
            for cve in self.cve_list
        ]
        res = count_range(reference_len_list, interval)
        print(res)


    def count_descriptions(self, interval: list):
        description_len_list = [
            len(self.cve_data_all[cve]['original_description'].split(' '))
            for cve in self.cve_list
        ]
        res = count_range(description_len_list, interval)
        print(res)


    def count_domain(self):
        res = {}
        for cve in tqdm.tqdm(self.cve_list):
            url_list = self.cve_data_all[cve]['reference_list']
            for url in url_list:
                domain = get_domain(url)
                save_text(f'{self.url_list_dir}/{domain}', url, 'a')
                if domain in res.keys():
                    res[domain] += 1
                else:
                    res[domain] = 1
        # res = dict(sorted(res.items(), key = lambda item: item[1], reverse = True))
        # print(res)


    def get_module_name(self, domain: str):
        module_name = domain.replace('.', '_').replace('-', '_')
        if module_name == 'www_debian_org':
            module_name = 'lists_debian_org'
        elif module_name == 'h20566_www2_hpe_com':
            module_name = 'support_hpe_com'
        elif module_name == 'kb_cert_org':
            module_name = 'www_kb_cert_org'
        elif module_name in ['usn_ubuntu_com', 'ubuntu_com']:
            module_name = 'www_ubuntu_com'
        elif module_name == 'www_talosintelligence_com':
            module_name = 'talosintelligence_com'
        elif module_name == 'openwall_com':
            module_name = 'www_openwall_com'
        elif module_name == 'bugzilla_suse_com':
            module_name = 'bugzilla_redhat_com'
        elif module_name == 'launchpad_net':
            module_name = 'bugs_launchpad_net'
            
        return module_name


    def scrapy_single_url(self, cve: str, url: str, retry = False, save_to_error_list = True):
        domain = get_domain(url)
        state = 0           # 0表示无异常
        try:
            if domain in config['particular_domain_list']:
                if domain in ['tomcat.apache.org', 'talosintelligence.com', 'www.talosintelligence.com']:
                    url += f'#{cve}'
                module_name = self.get_module_name(domain)
                # print(module_name)
                if module_name not in modules.keys():
                    modules[module_name] = importlib.import_module(f'completion.scrapy.{module_name}')
                res = modules[module_name].scrapy(url)
            elif domain in config['not_handle_domain_list']:
                return [url, domain, 0, '']
            else:
                # print(f'common url: {url}')
                from completion.scrapy import common
                res = common.scrapy(url)
        except Exception as e:
            if not retry:     # retry一次
                res = self.scrapy_single_url(cve, url, True)
                return res
            else:
                res = e
                state = 1
                if save_to_error_list:
                    df = pd.read_csv(self.error_list)
                    df.loc[len(df)] = [cve, url, domain, e]
                    df.to_csv(
                        self.error_list,
                        index = False, 
                        quotechar = '"',
                        quoting = csv.QUOTE_ALL
                    )
        return [url, domain, state, res]


    def scrapy_all_url(self):

        def scrapy_all_url_sub(cve_list: list):
            for cve in tqdm.tqdm(cve_list):
                if os.path.exists(f'{self.scrapy_result_dir}/{cve}.csv'):
                    continue
                # print('cve:', cve)
                url_list = self.cve_data_all[cve]['reference_list']
                df = pd.DataFrame({
                    'url': [],
                    'domain': [],
                    'state': [],
                    'text': []
                })
                # for url in tqdm.tqdm(url_list):
                for url in url_list:
                    df.loc[len(df)] = self.scrapy_single_url(cve, url)
                df.to_csv(
                    f'{self.scrapy_result_dir}/{cve}.csv',
                    index = False, 
                    quotechar = '"',
                    quoting = csv.QUOTE_ALL
                )
        
        # scrapy_all_url_sub(list(random.sample(self.cve_list, 5)))
        # scrapy_all_url_sub(['CVE-2004-0104'])
        cve_list_done = {
            file.split('.')[0] for file in os.listdir(self.scrapy_result_dir)
            if file not in ['.DS_Store', 'error_list.csv', 'long_text.csv', 'short_text.csv']
        }
        cve_list_todo = list(set(self.cve_list) - cve_list_done)
        multi_thread(cve_list_todo, scrapy_all_url_sub, chunk_size = 300)
        

    def split_scrapy_res(self, block_max_token = 2048):
        invalid = 0
        for cve in tqdm.tqdm(self.cve_list):
        # for cve in tqdm.tqdm(list(random.sample(self.cve_list, 5))):
            if os.path.exists(f'{self.embedding_text_dir}/{cve}.csv'):
                continue

            df = pd.read_csv(f'{self.scrapy_result_dir}/{cve}.csv')
            df_new = pd.DataFrame({
                'url': [],
                'token_len': [],
                'text': []
            })

            for _, row in df.iterrows():
                if not isinstance(row.text, str):
                    continue
                text_len = len(row.text)
                if 0 < text_len < 150:
                    continue

                token_len = calc_token(row.text)
                if token_len > block_max_token:
                    block_size = math.ceil(token_len / block_max_token)
                    block_len = int(text_len / block_size)
                    l = 0
                    for i in range(1, block_size):
                        r = block_len * i
                        tp = 0
                        while r < text_len:
                            if not row.text[r].isprintable() or any(row.text[r] == char for char in [' ', '\n', ',', '.']):
                                break
                            r -= 1
                            tp += 1
                            if tp > 500:
                                # print(cve, row.url)
                                break                             
                        if row.text[l:r]:
                            df_new.loc[len(df_new)] = [
                                row.url,
                                f'{calc_token(row.text[l:r])}/{token_len}',
                                row.text[l:r]
                            ]
                        l = r
                    if row.text[l:]:
                        df_new.loc[len(df_new)] = [
                            row.url,
                            f'{calc_token(row.text[l:])}/{token_len}',
                            row.text[l:]
                        ]
                else:
                    df_new.loc[len(df_new)] = [row.url, f'{token_len}/{token_len}', row.text]
            if len(df_new) > 0:
                df_new.to_csv(
                    f'{self.embedding_text_dir}/{cve}.csv',
                    index = False, 
                    quotechar = '"',
                    quoting = csv.QUOTE_ALL
                )
            else:
                invalid += 1
        print(invalid, 'invalid cve')


    def generate_query(self):
        
        print('start generate query')

        new_cve_list = [file.split('.')[0] for file in os.listdir(self.embedding_text_dir) if file not in ['.DS_Store']]

        df = pd.DataFrame({
            'cve': [],
            'text': []
        })
        # for file in tqdm.tqdm(list(random.sample(new_cve_list, 5))):
        for file in tqdm.tqdm(new_cve_list):
            cve = file.split('.')[0]
            cpe_info = 'Affected Software: '
            for product in self.cve_data_all[cve]['cpe_product']:
                cpe_info += product + ' '
            original_description = self.cve_data_all[cve]['original_description']
            text = f'{cve}\nDescription\n{original_description}\n{cpe_info}'
            df.loc[len(df)] = [cve, text]
        df.to_csv(
            f'{self.module_root_path}/query.csv',
            index = False, 
            quotechar = '"',
            quoting = csv.QUOTE_ALL
        )
        print('end generate query, size:', len(df))


    def text_to_vector(self):
        
        def get_embedding(text: str):
            return client.embeddings.create(input = [text], model = 'text-embedding-3-small').data[0].embedding
        
        def query_text_to_vector(cve_list: list):
            for cve in tqdm.tqdm(cve_list):
                row = df_query.index[df_query['cve'] == cve]
                tp = df_query.loc[row, 'vector']
                if tp.values == [None]:
                    df_query.loc[row, 'vector'] = str(get_embedding(str(df_query.loc[row].text)))
                else:
                    print('skip')
                

        print('start embedding')

        load_dotenv()
        client = OpenAI()

        df_query = pd.read_csv(f'{self.module_root_path}/query.csv')
        if len(df_query.columns) == 2:
            print('start embedding query')
            df_query['vector'] = None
            cve_list = list(df_query.cve)
            multi_thread(cve_list, query_text_to_vector)

            df_query.to_csv(
                f'{self.module_root_path}/query.csv',
                index = False, 
                quotechar = '"',
                quoting = csv.QUOTE_ALL
            )
            print('end embedding query')
        else:
            print('already embedding query')

        def cve_text_to_vector(cve_list: list):
            for cve in tqdm.tqdm(cve_list):
                target_path = f'{self.embedding_result_dir}/{cve}.csv'
                if os.path.exists(target_path):
                    continue

                df = pd.read_csv(f'{self.embedding_text_dir}/{cve}.csv')
                df['vector'] = df.text.apply(lambda x: get_embedding(x))
                df.to_csv(
                    target_path,
                    index = False, 
                    quotechar = '"',
                    quoting = csv.QUOTE_ALL
                )

        print('start embedding cve')

        new_cve_list = [file.split('.')[0] for file in os.listdir(self.embedding_text_dir) if file not in ['.DS_Store']]
        print(len(new_cve_list))
        multi_thread(new_cve_list, cve_text_to_vector)

        print('start embedding cve')
        print('end embedding')


    def calc_similarity(self):
        print('start calc similarity')

        df_query = pd.read_csv(f'{self.module_root_path}/query.csv')
        tp = {}
        for _, row in df_query.iterrows():
            tp[row.cve] = ast.literal_eval(row.vector)

        for file in tqdm.tqdm(os.listdir(self.embedding_result_dir)):
            if file in ['.DS_Store']: continue
            
            cve = file.split('.')[0]
            df = pd.read_csv(f'{self.embedding_result_dir}/{cve}.csv')
            query_vector = tp[cve]

            df['similarity'] = df.vector.apply(lambda x: cosine_similarity(ast.literal_eval(x), query_vector))
            df = df.sort_values('similarity', ascending = False)
            df.to_csv(
                f'{self.embedding_result_dir}/{cve}.csv',
                index = False, 
                quotechar = '"',
                quoting = csv.QUOTE_ALL
            )
        print('end calc similarity')


    def generate_complete_prompt(self):

        def select_knowledge_block(cve: str, token_used: int):
            res = ''
            df = pd.read_csv(f'{self.embedding_result_dir}/{cve}.csv')
            for _, row in df.iterrows():
                if row.similarity >= 0.458:
                    if calc_token(res + row.text, 'gpt-3.5-turbo') <= self.gpt_max_token - token_used:
                        res += row.text + '\n\n'
                    else:
                        break
            return res

        for file in tqdm.tqdm(os.listdir(self.embedding_result_dir)):
            if file in ['.DS_Store']: continue

            cve = file.split('.')[0]
            with open(f'{self.module_root_path}/prompt_complete_template', 'r') as f:
                prompt = f.read()
            prompt = prompt.replace('{cve}', cve)
            prompt = prompt.replace('{original description}', self.cve_data_all[cve]['original_description'])
            
            knowledge = select_knowledge_block(cve, calc_token(prompt, 'gpt-3.5-turbo'))
            if len(knowledge) < 75:
                continue
            prompt = prompt.replace('{reference information}', knowledge)
            save_text(f'{self.completion_prompt_dir}/{cve}', prompt)


    def count_prompt(self):
        total = 0
        for cve in tqdm.tqdm(os.listdir(self.completion_prompt_dir)):
            if cve in ['.DS_Store']: continue
            with open(f'{self.completion_prompt_dir}/{cve}', 'r') as f:
                token = calc_token(f.read(), 'gpt-3.5-turbo')
                total += token
                if token > self.gpt_max_token:
                    print(cve, token)
        print('total token:', total)


    def complete_description(self):

        def complete(cve_list: list):
            for cve in tqdm.tqdm(cve_list):
                if os.path.exists(f'{self.completion_result_dir}/{cve}'):
                    continue
                with open(f'{self.completion_prompt_dir}/{cve}') as f:
                    prompt = f.read()
                try:
                    res = query_openai(prompt)
                    self.cve_data_all[cve]['complete_description'] = res
                    save_text(f'{self.completion_result_dir}/{cve}', res)
                except Exception as e:
                    save_text(f'{self.completion_result_dir}/error_list', f'{cve}\n\n{e}', 'a')
                
        
        cve_list = os.listdir(self.completion_prompt_dir)
        if '.DS_Store' in cve_list:
            cve_list.remove('.DS_Store')
        print(len(cve_list))
        multi_thread(cve_list, complete)

        save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
        save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)
    

    def re_scrapy(self, path):
        if not os.path.exists(path):
            return
        
        df = pd.read_csv(path)
        # print(len(df))
        for _, row in tqdm.tqdm(df.iterrows()):
            if row.domain not in ['seclists.org']: continue
            df_cve = pd.read_csv(f'{self.scrapy_result_dir}/{row.cve}.csv')
            res = self.scrapy_single_url(row.cve, row.url, save_to_error_list = (path != self.error_list))
            if res[2] == 0:
                df_cve.loc[df_cve['url'] == row.url] = res
                df_cve.to_csv(
                    f'{self.scrapy_result_dir}/{row.cve}.csv',
                    index = False, 
                    quotechar = '"',
                    quoting = csv.QUOTE_ALL
                )
                row.text = 'done'
                print(f'{row.cve}, {row.url} update success, len: {len(res)}')
            else:
                print(f'{row.cve}, {row.url} updated failure')
        df = df.drop(df[df['text'] == 'done'].index)
        print(len(df))
        df.to_csv(
            path,
            index = False, 
            quotechar = '"',
            quoting = csv.QUOTE_ALL
        )


    def count_scrapy_result(self):
        df_long_text = pd.DataFrame({
                'cve': [],
                'url': [],
                'domain': [],
                'text': []
        })
        df_short_text = pd.DataFrame({
            'cve': [],
            'url': [],
            'domain': [],
            'text': []
        })
        length_list = []
        
        for cve in tqdm.tqdm(self.cve_list):
            # print(cve)
            df = pd.read_csv(f'{self.scrapy_result_dir}/{cve}.csv')
            for _, row in df.iterrows():
                # print(row.url)
                if isinstance(row.text, str):
                    length = len(row.text)
                    if 0 < length < 150:
                        df_short_text.loc[len(df_short_text)] = [cve, row.url, row.domain, row.text]
                    else:
                        length_list.append(calc_token(row.text))
                        if length > 50000:
                            df_long_text.loc[len(df_long_text)] = [cve, row.url, row.domain, row.text]

        df_long_text.to_csv(
            self.long_text,
            index = False, 
            quotechar = '"',
            quoting = csv.QUOTE_ALL
        )
        df_short_text.to_csv(
            self.short_text,
            index = False, 
            quotechar = '"',
            quoting = csv.QUOTE_ALL
        )

        print(count_range(length_list, [150, 300, 500, 1000, 1800, 5000, 10000, 20000]))


    def count_total_token(self):
        total = 0
        for cve in tqdm.tqdm(os.listdir(self.embedding_text_dir)):
            if cve in ['.DS_Store']: continue
            sum = 0
            df = pd.read_csv(f'{self.embedding_text_dir}/{cve}')
            for text in df.text:
                # if isinstance(text, str):
                sum += calc_token(text)
            total += sum
        print(total)


    def count_similarity(self, interval: list):
        similarity_all = []
        for file in os.listdir(self.embedding_result_dir):
        # for file in tqdm.tqdm(os.listdir(self.embedding_result_dir)):
            if file in ['.DS_Store']: continue

            df = pd.read_csv(f'{self.embedding_result_dir}/{file}')
            for _, row in df.iterrows():
                # if 0.1 <= row.similarity < 0.15:
                if 0.7 < row.similarity:
                    print(file, row.url)
            similarity = list(df.similarity)
            similarity_all += similarity
            # print(similarity)
        # print(len(similarity_all))
        print(count_range(similarity_all, interval))