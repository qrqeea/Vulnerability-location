import tqdm
import random
from util.io_util import *
from util.github_util import *


class CommitCollection:

    def __init__(self, project_root_path: str, module_root_path: str, repo_data_path: str, cve_data_all: dict):
        self.project_root_path = project_root_path
        self.module_root_path = module_root_path
        self.repo_data_path = repo_data_path
        self.cve_data_all = cve_data_all

        os.makedirs(self.module_root_path, exist_ok = True)
        os.makedirs(self.repo_data_path, exist_ok = True)

        # 这个模块的target cve list是上一步收集到repo的cve
        self.cve_list = [k for k, v in self.cve_data_all.items() if 'collected_repo' in v]
        print(f'cve count: {len(self.cve_list)}')     # 8971

        repo_all = set()
        for _, v in self.cve_data_all.items():
            if 'collected_repo' in v:
                repo_all |= set(v['collected_repo'])
        self.repo_all = repo_all
        print(f'repo count: {len(self.repo_all)}')  # 2722


    def start(self):
        repo_all_branch = self.get_repo_all_branch()
        data = self.get_latest_commit_before_date_all(repo_all_branch)
        repo_latest_sha_before_date = self.convert_data(data)
        self.check(repo_latest_sha_before_date)
        self.select_commit(repo_latest_sha_before_date)
        # self.get_specified_file_list()
        # self.check_commit_accuracy()
        

    def get_latest_commit_before_date_all(self, repo_all_branch: dict):
        # 以repo为key，搜索所有分支在某个日期前的最新commit
        path = f'{self.repo_data_path}/repo_latest_sha_before_date(set)'
        if os.path.exists(f'{path}.pkl'):
            return load_pickle(f'{path}.pkl')
        
        def get_latest_commit_before_date_all_sub(to_scrapy_list_sub: list, token: str):
            for repo, date, branch in tqdm.tqdm(to_scrapy_list_sub):
                commit_sha = get_latest_commit_before_date(repo, date, token, branch)
                if commit_sha:
                    repo_latest_sha_before_date[repo][date].add(commit_sha)

        print('start search repo\'s latest_commit_before_date')
        
        if os.path.exists(f'{path}.pkl'):
            repo_latest_sha_before_date = load_pickle(f'{path}.pkl')
            to_scrapy_list = load_pickle(f'{self.module_root_path}/to_scrapy_list.pkl')
        else:
            repo_latest_sha_before_date = {}
            to_scrapy_list = set()
            for cve in self.cve_list:
                date = self.cve_data_all[cve]['published_date']
                for repo in self.cve_data_all[cve]['collected_repo']:
                    if repo not in repo_latest_sha_before_date:
                        repo_latest_sha_before_date[repo] = {}
                    if date not in repo_latest_sha_before_date[repo]:
                        repo_latest_sha_before_date[repo][date] = set()
                    # if repo_latest_sha_before_date[repo][date]: continue
                    for branch in repo_all_branch[repo]:
                        to_scrapy_list.add((repo, date, branch))
            save_text(f'{self.module_root_path}/to_scrapy_list', to_scrapy_list)
            save_pickle(f'{self.module_root_path}/to_scrapy_list.pkl', to_scrapy_list)
        
        print(f'to_scrapy_list size: {len(to_scrapy_list)}')
        # multi_thread(random.sample(list(to_scrapy_list), 10), get_latest_commit_before_date_all_sub, tokens = github_tokens)
        multi_thread(list(to_scrapy_list)[60000:150000], get_latest_commit_before_date_all_sub, tokens = github_tokens)
        
        save_pickle(f'{path}.pkl', repo_latest_sha_before_date)
    
        print('end search repo\'s latest_commit_before_date')
        return repo_latest_sha_before_date


    def convert_data(self, data: dict):
        repo_latest_sha_before_date = {}
        for repo, v in data.items():
            if repo not in repo_latest_sha_before_date:
                repo_latest_sha_before_date[repo] = {}
            for date, shas in v.items():
                if shas:
                    repo_latest_sha_before_date[repo][date] = sorted(list(shas), key = lambda x: x[1], reverse = True)
            if not repo_latest_sha_before_date[repo]:
                del repo_latest_sha_before_date[repo]
        path = f'{self.repo_data_path}/repo_latest_sha_before_date'
        save_json(f'{path}.json', repo_latest_sha_before_date)
        save_pickle(f'{path}.pkl', repo_latest_sha_before_date)
    

    def check(self, repo_latest_sha_before_date: dict):
        # 和原来的方法找到的commit比较，理论上应该完全覆盖原来的
        for cve in self.cve_list:
            v = self.cve_data_all[cve]
            date = v['published_date']
            if 'collected_commit' not in v: continue

            for repo, commit in v['collected_commit']:
                if repo not in repo_latest_sha_before_date:
                    print(f'repo {repo} not exist')
                    continue
                if date not in repo_latest_sha_before_date[repo]:
                    print(f'{repo} {date} not exist')
                    continue
                flag = False
                for commit2, _ in repo_latest_sha_before_date[repo][date]:
                    if commit2 == commit:
                        flag = True
                        break
                if not flag:
                    print(f'not found', cve, repo, date)


    def select_commit(self, data: dict):
        collected_commits = {}
        for cve in tqdm.tqdm(self.cve_list):
            date = self.cve_data_all[cve]['published_date']
            for repo in self.cve_data_all[cve]['collected_repo']:
                if data[repo][date]:
                    if cve not in collected_commits:
                        collected_commits[cve] = []
                    collected_commits[cve].append(data[repo][date][0])  # 选择最接近漏洞披露日期的commit
                # else:
                #     print('not found', cve, repo, date)
        save_json(f'{self.module_root_path}/collected_commits.json', collected_commits)
        save_pickle(f'{self.module_root_path}/collected_commits.pkl', collected_commits)

        for cve, commits in collected_commits:
            self.cve_data_all[cve]['collected_commit'] = commits
        
        save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
        save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)
    

    def get_specified_file_list(self):
        def get_specified_file_list_sub(cve_list_sub: list, token: str):
            for (repo, sha) in tqdm.tqdm(cve_list_sub):
                if repo not in repo_file_list_table:
                    repo_file_list_table[repo] = {}
                if sha not in repo_file_list_table[repo]:
                    repo_file_list_table[repo][sha] = get_file_list(repo, sha, token)


        data = load_pickle(f'{self.module_root_path}/collected_commits.pkl')
        if os.path.exists(f'{self.module_root_path}/repo_file_list_table.pkl'):
            repo_file_list_table = load_pickle(f'{self.module_root_path}/repo_file_list_table.pkl')
        else:
            repo_file_list_table = {}

        data_list = list({
            (repo, sha)
            for v in data.values()
            for (repo, sha) in v
        })
        print(len(data_list))
        multi_thread(data_list, get_specified_file_list_sub, tokens = github_tokens)

        save_json(f'{self.module_root_path}/repo_file_list_table.json', repo_file_list_table)
        save_pickle(f'{self.module_root_path}/repo_file_list_table.pkl', repo_file_list_table)


    def check_commit_accuracy(self):
        count = 0
        total_count = 0
        correct_cve_list = set()
        repo_file_list_table = load_pickle(f'{self.module_root_path}/repo_file_list_table.pkl')

        for cve in tqdm.tqdm(self.cve_list):
            collected_commit = self.cve_data_all[cve].get('collected_commit')
            if not collected_commit: continue
            total_count += 1

            flag = False
            file_ans_list = self.cve_data_all[cve]['files']
            for repo, sha in collected_commit:
                if flag:
                    break

                file_list = repo_file_list_table[repo][sha]
                for file, _ in file_list:
                    # if any(file_ans in file for file_ans in file_ans_list):
                    #     flag = True
                    #     break
                    if any(file_ans.lower() in file.lower() for file_ans in file_ans_list):
                    # if any(file.lower() in file_ans.lower() for file_ans in file_ans_list):
                    # if any((file_ans.lower() in file.lower() or file.lower() in file_ans.lower()) for file_ans in file_ans_list):
                        # print(cve)
                        correct_cve_list.add(cve)
                        flag = True
                        break
            if flag:
                count += 1
                self.cve_data_all[cve]['collected_commit_correction'] = True
            else:
                self.cve_data_all[cve]['collected_commit_correction'] = False
        
        save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
        save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)

        save_text(f'{self.module_root_path}/correct_commit_cve_list', correct_cve_list)
        print(f'accuracy: {count/total_count}, {count}/{total_count}')


    def get_repo_all_branch(self):

        def get_repo_all_branch_sub(repo_list_sub: list, token: str):
            for repo in tqdm.tqdm(repo_list_sub):
                all_branch = get_all_branch(repo, token)
                repo_all_branch[repo] = all_branch

        print('start search repo\'s all branch')

        path = f'{self.repo_data_path}/repo_all_branch'
        repo_all_branch = load_pickle(f'{path}.pkl') if os.path.exists(f'{path}.pkl') else {}
        
        rest_repo = list(set(self.repo_all) - set(repo_all_branch.keys()))
        print(f'rest repo count: {len(rest_repo)}')
        
        multi_thread(rest_repo, get_repo_all_branch_sub, tokens = github_tokens)

        save_json(f'{path}.json', repo_all_branch)
        save_pickle(f'{path}.pkl', repo_all_branch)

        print('end search repo\'s all branch\n')
        return repo_all_branch