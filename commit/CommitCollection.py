import tqdm
import random
import difflib
from util.io_util import *
from util.github_util import *
from util.general_util import rule_based_filtering


class CommitCollection:

    def __init__(self, project_root_path: str, module_root_path: str, repo_data_path: str, gt_content_path: str, specified_repo_path: str, cve_data_all: dict):
        self.project_root_path = project_root_path
        self.module_root_path = module_root_path
        self.repo_data_path = repo_data_path
        self.gt_content_path = gt_content_path
        self.specified_repo_path = specified_repo_path
        self.cve_data_all = cve_data_all

        os.makedirs(self.module_root_path, exist_ok = True)
        os.makedirs(self.repo_data_path, exist_ok = True)

        # 这个模块的target cve list是上一步收集到repo的cve
        self.cve_list = [k for k, v in self.cve_data_all.items() if 'collected_repo' in v]
        print(f'cve count: {len(self.cve_list)}')

        repo_all = set()
        for _, v in self.cve_data_all.items():
            if 'collected_repo' in v:
                repo_all |= set(v['collected_repo'])
        self.repo_all = repo_all
        print(f'repo count: {len(self.repo_all)}')


    def start(self):
        repo_all_branch = self.get_repo_all_branch()
        data = self.get_latest_commit_before_date_all(repo_all_branch)
        repo_latest_sha_before_date = self.convert_data(data)
        collected_commits = self.select_commit(repo_latest_sha_before_date)
        self.check_commit_accuracy(collected_commits)

        # self.manual_update()


    def manual_update(self):
        manual_update = load_json(f'{self.module_root_path}/manual_update.json')
        print(len(manual_update))
        correct_commits = load_pickle(f'{self.module_root_path}/correct_commits.pkl')
        print(len(correct_commits))
        for cve, v in manual_update.items():
            if cve not in correct_commits:
                correct_commits[cve] = {}
            for repo, file in v.items():
                correct_commits[cve][repo] = file
        print(len(correct_commits))
        save_json(f'{self.module_root_path}/correct_commits.json', correct_commits)
        save_pickle(f'{self.module_root_path}/correct_commits.pkl', correct_commits)


    def get_repo_all_branch(self):
        def get_repo_all_branch_sub(repo_list_sub: list, token: str):
            for repo in tqdm.tqdm(repo_list_sub):
                all_branch = get_all_branch(repo, token)
                repo_all_branch[repo] = all_branch

        path = f'{self.repo_data_path}/repo_all_branch'
        repo_all_branch = load_pickle(f'{path}.pkl') if os.path.exists(f'{path}.pkl') else {}
        
        rest_repo = list(set(self.repo_all) - set(repo_all_branch.keys()))
        if rest_repo:
            print(f'start search repo\'s all branch, rest repo count: {len(rest_repo)}')
            multi_thread(rest_repo, get_repo_all_branch_sub, tokens = github_tokens)
            save_json(f'{path}.json', repo_all_branch)
            save_pickle(f'{path}.pkl', repo_all_branch)
            print('end search repo\'s all branch\n')
        
        return repo_all_branch        


    def get_latest_commit_before_date_all(self, repo_all_branch: dict):
        # 以repo为key，搜索所有分支在某个日期前的最新commit
        if not any('adjusted_date' in v for v in self.cve_data_all.values()):
            for cve in self.cve_list:
                published_date = self.cve_data_all[cve]['published_date']
                year1 = int(published_date[:4])
                year2 = int(cve[4:8])
                if abs(year1 - year2) > 1:
                    self.cve_data_all[cve]['adjusted_date'] = f'{year2}-06-30T00:00Z'
                else:
                    self.cve_data_all[cve]['adjusted_date'] = published_date
            save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
            save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)
        
        def get_latest_commit_before_date_all_sub(to_scrapy_list_sub: list, token: str):
            for repo, date in tqdm.tqdm(to_scrapy_list_sub):
                res = set()
                for branch in repo_all_branch[repo]:
                    commit_sha = get_latest_commit_before_date(repo, date, token, branch)
                    if commit_sha:
                        res.add(commit_sha)
                repo_latest_sha_before_date[repo][date] = res
                save_pickle(f'{path}.pkl', repo_latest_sha_before_date)

        path = f'{self.repo_data_path}/repo_latest_sha_before_date(set)'
        repo_latest_sha_before_date = load_pickle(f'{path}.pkl') if os.path.exists(f'{path}.pkl') else {}
        
        to_scrapy_list = set()
        for cve in self.cve_list:
            dates = {self.cve_data_all[cve]['adjusted_date'], self.cve_data_all[cve]['published_date']}
            for date in dates:
                for repo in self.cve_data_all[cve]['collected_repo']:
                    if repo not in repo_latest_sha_before_date:
                        repo_latest_sha_before_date[repo] = {}
                    if date in repo_latest_sha_before_date[repo]:
                        continue
                    to_scrapy_list.add((repo, date))
        if to_scrapy_list:
            # save_text(f'{self.module_root_path}/commit_to_scrapy_list', to_scrapy_list)
            # save_pickle(f'{self.module_root_path}/commit_to_scrapy_list.pkl', to_scrapy_list)
            print(f'start search repo\'s latest_commit_before_date, to_scrapy_list size: {len(to_scrapy_list)}')
            multi_thread(list(to_scrapy_list), get_latest_commit_before_date_all_sub, tokens = github_tokens)
            save_pickle(f'{path}.pkl', repo_latest_sha_before_date)
            print('end search repo\'s latest_commit_before_date')
        
        return repo_latest_sha_before_date


    def convert_data(self, data: dict):
        path = f'{self.repo_data_path}/repo_latest_sha_before_date'
        if os.path.exists(f'{path}.pkl'):
            return load_pickle(f'{path}.pkl')

        repo_latest_sha_before_date = {}
        for repo, v in data.items():
            if repo not in repo_latest_sha_before_date:
                repo_latest_sha_before_date[repo] = {}
            for date, shas in v.items():
                if shas:
                    repo_latest_sha_before_date[repo][date] = sorted(list(shas), key = lambda x: x[1], reverse = True)
            if not repo_latest_sha_before_date[repo]:
                del repo_latest_sha_before_date[repo]
        save_json(f'{path}.json', repo_latest_sha_before_date)
        save_pickle(f'{path}.pkl', repo_latest_sha_before_date)
        
        return repo_latest_sha_before_date


    def select_commit(self, data: dict):
        path = f'{self.module_root_path}/collected_commits'
        if os.path.exists(f'{path}.pkl'):
            return load_pickle(f'{path}.pkl')
        
        collected_commits = {}
        for cve in tqdm.tqdm(self.cve_list):
            adjusted_date = self.cve_data_all[cve]['adjusted_date']
            published_date = self.cve_data_all[cve]['published_date']
            collected_repo = self.cve_data_all[cve]['collected_repo'].copy()
            # 检查收集到的repo在指定日期前是否有commit，如果没有就把repo删掉
            for repo in collected_repo:
                if repo not in data or (adjusted_date not in data[repo] and published_date not in data[repo]):
                    self.cve_data_all[cve]['collected_repo'].remove(repo)
            if not self.cve_data_all[cve]['collected_repo']:
                del self.cve_data_all[cve]['collected_repo']
                continue
            
            # 到这里一定有commit，但是commit可能为空
            for repo in self.cve_data_all[cve]['collected_repo']:
                if adjusted_date in data[repo]:
                    if cve not in collected_commits:
                        collected_commits[cve] = []
                    collected_commits[cve].append((repo, data[repo][adjusted_date][0][0]))
                    continue
                if published_date in data[repo]:
                    if cve not in collected_commits:
                        collected_commits[cve] = []
                    collected_commits[cve].append((repo, data[repo][published_date][0][0]))
        
        repo_file_list = self.get_specified_file_list(collected_commits)
        # check一下commit是否为空，标准是rule_based_filtering筛选后有文件
        to_delete_list = []
        for cve, v in tqdm.tqdm(collected_commits.items()):
            for repo, sha in v:
                if not any(not isdir and rule_based_filtering(file) for file, isdir in repo_file_list[repo][sha]):
                    to_delete_list.append((cve, repo, sha))
        print(f'to_delete_list size: {len(to_delete_list)}')
        # print(to_delete_list)
        print(f'before: {len(collected_commits)}')
        for cve, repo, sha in to_delete_list:
            collected_commits[cve].remove((repo, sha))
            self.cve_data_all[cve]['collected_repo'].remove(repo)
            if not collected_commits[cve]:
                del collected_commits[cve]
            if not self.cve_data_all[cve]['collected_repo']:
                del self.cve_data_all[cve]['collected_repo']
                
        print(f'after: {len(collected_commits)}')
                
        save_json(f'{path}.json', collected_commits)
        save_pickle(f'{path}.pkl', collected_commits)
        
        for cve, v in self.cve_data_all.items():
            if 'collected_commit' in v:
                del self.cve_data_all[cve]['collected_commit']
            if cve in collected_commits:
                self.cve_data_all[cve]['collected_commit'] = collected_commits[cve]
        
        save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
        save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)
        
        return collected_commits


    def get_specified_file_list(self, collected_commits: dict):
        path = f'{self.repo_data_path}/repo_file_list'
        if os.path.exists(f'{path}.pkl'):
            return load_pickle(f'{path}.pkl')

        def get_specified_file_list_sub(cve_list_sub: list, token: str):
            for (repo, sha) in tqdm.tqdm(cve_list_sub):
                if repo not in repo_file_list:
                    repo_file_list[repo] = {}
                if sha not in repo_file_list[repo]:
                    repo_file_list[repo][sha] = get_file_list(repo, sha, token)

        repo_file_list = load_pickle(f'{path}.pkl') if os.path.exists(f'{path}.pkl') else {}

        to_scrapy_list = list({
            (repo, sha)
            for v in collected_commits.values()
            for (repo, sha) in v
            if not (repo in repo_file_list and sha in repo_file_list[repo] and repo_file_list[repo][sha])
        })
        if len(to_scrapy_list) != 0:
            print(f'start scrapy repo_file_list, to_scrapy_list size: {len(to_scrapy_list)}')
            multi_thread(to_scrapy_list, get_specified_file_list_sub, tokens = github_tokens)
            save_json(f'{path}.json', repo_file_list)
            save_pickle(f'{path}.pkl', repo_file_list)
            print(f'end scrapy repo_file_list')
        
        return repo_file_list


    def get_gt_file_path(self, cve: str, repo: str):
        dir = f'{self.gt_content_path}/{cve}'
        if os.path.exists(dir):
            for file in os.listdir(dir):
                if file.startswith(repo.replace('/', '\\')):
                    return f'{self.gt_content_path}/{cve}/{file}'
        return None


    def check_commit_accuracy(self, collected_commits: dict):
        self.repo_file_list = load_pickle(f'{self.repo_data_path}/repo_file_list.pkl')
        self.collected_commits = collected_commits
        cve_list_check_by_content = self.check_same_repo_and_file()
        self.check_different_repo_and_same_file(cve_list_check_by_content)


    def check_same_repo_and_file(self):
        # 先遍历一轮，筛选repo和file相同的CVE
        res_path = f'{self.module_root_path}/cve_list_check_by_content'
        if os.path.exists(f'{res_path}.pkl'):
            return load_pickle(f'{res_path}.pkl')
        else:
            print(f'check size: {len(self.collected_commits)}')
            correct_commits = {}
            total_count = len(self.collected_commits)
            rest_cve_list = []
            for cve, collected_commit in tqdm.tqdm(self.collected_commits.items()):
                for repo, sha in collected_commit:
                    for repo_ans, vul_file in self.cve_data_all[cve]['vulnerability_files'].items():
                        if repo.lower() != repo_ans.lower():
                            continue
                        vul_file = vul_file[0].lower()
                        for file, _ in self.repo_file_list[repo][sha]:
                            if vul_file == file.lower():
                                if cve not in correct_commits:
                                    correct_commits[cve] = {}
                                correct_commits[cve][repo] = file
                                break
                if not (cve in correct_commits and len(correct_commits[cve]) == len(self.collected_commits[cve])):
                    rest_cve_list.append(cve)
            save_text(f'{res_path}', rest_cve_list)
            save_pickle(f'{res_path}.pkl', rest_cve_list)
            
            save_json(f'{self.module_root_path}/correct_commits_1.json', correct_commits)
            save_pickle(f'{self.module_root_path}/correct_commits_1.pkl', correct_commits)
        
            print(f'{len(correct_commits)}/{total_count} cve with same repo and file')
            return rest_cve_list        


    def check_different_repo_and_same_file(self, cve_list: list):
        correct_commits = load_pickle(f'{self.module_root_path}/correct_commits_1.pkl')
        problem_cve_list = []       # 有问题的CVE，等之后有时间把这些CVE重跑一下，maybe数据能提升一些
        for cve in tqdm.tqdm(cve_list):
            gt_files_path = self.get_gt_files_path(cve)
            if not gt_files_path:
                problem_cve_list.append((cve, 'no gt'))
                continue
            
            for repo, sha in self.collected_commits[cve]:
                if cve in correct_commits and repo in correct_commits[cve]:     # 已经找到答案
                    continue
                threshold = 0.486
                to_check_files_path = self.get_same_files_path(cve, repo, sha)
                if not to_check_files_path:
                    threshold = 0.75
                    to_check_files_path = self.get_similar_files_path(cve, repo, sha)
                threshold = 0.8
                if not to_check_files_path:     # commit为空，应该在collected中删掉这些
                    save_text(f'{self.module_root_path}/to_delete_commit', (cve, repo, sha), 'a')
                    problem_cve_list.append((cve, 'empty commit'))
                    continue
                try:
                    max_simi, ans = 0, ''
                    for to_check_file_path in to_check_files_path:
                        content = load_file(to_check_file_path)
                        if len(content) > 300000:
                            print(f'1, {cve}, {repo}')
                            print(f'to_check_file_path: {to_check_file_path}')
                            print(f'gt_files_path: {gt_files_path}')
                            problem_cve_list.append((cve, f'large content1: {to_check_file_path}'))
                            continue
                        for gt_file_path in gt_files_path:
                            gt_content = load_file(gt_file_path)
                            if len(gt_content) > 300000:
                                print(f'2, {cve}, {repo}')
                                print(f'to_check_file_path: {to_check_file_path}')
                                print(f'gt_file_path: {gt_file_path}')
                                problem_cve_list.append((cve, f'large content2: {gt_file_path}'))
                                continue
                            simi = difflib.SequenceMatcher(None, gt_content, content).ratio()
                            if simi > max_simi:
                                max_simi = simi
                                ans = to_check_file_path
                    if max_simi > threshold:
                        if cve not in correct_commits:
                            correct_commits[cve] = {}
                        repo_updated = repo.replace('/', '—')
                        tp = len(f'{self.specified_repo_path}/{cve}/{repo_updated}/')
                        correct_commits[cve][repo] = ans[tp:]
                except Exception as e:
                    print(f'error587, {e}')     # 有一些类似链接的文件夹，在repo_file_list中isdir为false
                    problem_cve_list.append((cve, f'exception: {e}'))
                    continue

        save_json(f'{self.module_root_path}/correct_commits.json', correct_commits)
        save_pickle(f'{self.module_root_path}/correct_commits.pkl', correct_commits)

        save_text(f'{self.module_root_path}/problem_cve_list', problem_cve_list)
        save_pickle(f'{self.module_root_path}/problem_cve_list.pkl', problem_cve_list)

    
    def get_gt_files_path(self, cve):
        gt_files_dir = f'{self.gt_content_path}/{cve}'
        if not os.path.exists(gt_files_dir):
            print(f'error433, {gt_files_dir} not exist')
            return []
        gt_files_path = [
            f'{gt_files_dir}/{file}'
            for file in os.listdir(gt_files_dir)
            if file not in '.DS_Store'
        ]
        if not gt_files_path:
            print(f'error433, {gt_files_dir} empty')
        return gt_files_path


    def get_similar_files_path(self, cve, repo, sha):
        vul_files = {
            file.lower()
            for file in self.cve_data_all[cve]['file_list']
        }
        file_name_similarity = [
            (file, difflib.SequenceMatcher(None, vul_file, file.lower()).ratio())
            for file, isdir in self.repo_file_list[repo][sha]
            if not isdir and rule_based_filtering(file)
            for vul_file in vul_files
        ]
        file_name_similarity = sorted(file_name_similarity, key = lambda x: x[1], reverse = True)
        res = [ file
            for file, similarity in file_name_similarity[:5]
            if similarity > 0.8
        ]
        if not res:
            if file_name_similarity:
                res.append(file_name_similarity[0][0])
            else:
                return []
        repo_updated = repo.replace('/', '—')
        repo_dir = f'{self.specified_repo_path}/{cve}/{repo_updated}'
        if not os.path.exists(repo_dir):
            print(f'error333, {repo_dir} not exist')
            return []
        return [
            f'{repo_dir}/{file}'
            for file in res
        ]
            
    
    def get_same_files_path(self, cve, repo, sha):
        vul_files = {
            file.lower()
            for file in self.cve_data_all[cve]['file_list']
        }
        res = []
        for file, _ in self.repo_file_list[repo][sha]:
            if file.lower() in vul_files:
                res.append(file)
        if not res:
            return []

        repo_updated = repo.replace('/', '—')
        repo_dir = f'{self.specified_repo_path}/{cve}/{repo_updated}'
        if not os.path.exists(repo_dir):
            print(f'error233, {repo_dir} not exist')
            return []
        return [
            f'{repo_dir}/{file}'
            for file in res
        ]