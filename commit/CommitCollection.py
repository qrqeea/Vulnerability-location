import tqdm
import random
import difflib
from util.io_util import *
from util.github_util import *


class CommitCollection:

    def __init__(self, project_root_path: str, module_root_path: str, repo_data_path: str, gt_content_path: str, cve_data_all: dict):
        self.project_root_path = project_root_path
        self.module_root_path = module_root_path
        self.gt_content_path = gt_content_path
        self.repo_data_path = repo_data_path
        self.cve_data_all = cve_data_all

        os.makedirs(self.module_root_path, exist_ok = True)
        os.makedirs(self.repo_data_path, exist_ok = True)

        self.file_content_dir = f'{self.module_root_path}/file_content'
        os.makedirs(self.file_content_dir, exist_ok = True)

        # 这个模块的target cve list是上一步收集到repo的cve
        self.cve_list = [k for k, v in self.cve_data_all.items() if 'collected_repo' in v]
        print(f'cve count: {len(self.cve_list)}')

        repo_all = set()
        for _, v in self.cve_data_all.items():
            if 'collected_repo' in v:
                repo_all |= set(v['collected_repo'])
        self.repo_all = repo_all
        print(f'repo count: {len(self.repo_all)}')


    def start(self):
        repo_all_branch = self.get_repo_all_branch()
        data = self.get_latest_commit_before_date_all(repo_all_branch)
        repo_latest_sha_before_date = self.convert_data(data)
        self.select_commit(repo_latest_sha_before_date)


    def get_repo_all_branch(self):
        path = f'{self.repo_data_path}/repo_all_branch'
        if os.path.exists(f'{path}.pkl'):
            return load_pickle(f'{path}.pkl')

        def get_repo_all_branch_sub(repo_list_sub: list, token: str):
            for repo in tqdm.tqdm(repo_list_sub):
                all_branch = get_all_branch(repo, token)
                repo_all_branch[repo] = all_branch

        print('start search repo\'s all branch')

        repo_all_branch = load_pickle(f'{path}.pkl') if os.path.exists(f'{path}.pkl') else {}
        
        rest_repo = list(set(self.repo_all) - set(repo_all_branch.keys()))
        print(f'rest repo count: {len(rest_repo)}')
        
        multi_thread(rest_repo, get_repo_all_branch_sub, tokens = github_tokens)

        save_json(f'{path}.json', repo_all_branch)
        save_pickle(f'{path}.pkl', repo_all_branch)

        print('end search repo\'s all branch\n')
        return repo_all_branch        


    def get_latest_commit_before_date_all(self, repo_all_branch: dict):
        # 以repo为key，搜索所有分支在某个日期前的最新commit
        path = f'{self.repo_data_path}/repo_latest_sha_before_date(set)'
        if os.path.exists(f'{path}.pkl'):
            return load_pickle(f'{path}.pkl')

        if not any('adjusted_date' in v for v in self.cve_data_all.values()):
            for cve in self.cve_list:
                published_date = self.cve_data_all[cve]['published_date']
                year1 = int(published_date[:4])
                year2 = int(cve[4:8])
                if abs(year1 - year2) > 1:
                    self.cve_data_all[cve]['adjusted_date'] = f'{year2}-06-30T00:00Z'
                else:
                    self.cve_data_all[cve]['adjusted_date'] = published_date
            save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
            save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)
        
        def get_latest_commit_before_date_all_sub(to_scrapy_list_sub: list, token: str):
            for repo, date, branch in tqdm.tqdm(to_scrapy_list_sub):
                commit_sha = get_latest_commit_before_date(repo, date, token, branch)
                if commit_sha:
                    repo_latest_sha_before_date[repo][date].add(commit_sha)

        print('start search repo\'s latest_commit_before_date')
        
        repo_latest_sha_before_date = load_pickle(f'{path}.pkl') if os.path.exists(f'{path}.pkl') else {}
        to_scrapy_list = set()
        for cve in self.cve_list:
            dates = {self.cve_data_all[cve]['adjusted_date'], self.cve_data_all[cve]['published_date']}
            for date in dates:
                for repo in self.cve_data_all[cve]['collected_repo']:
                    if repo not in repo_latest_sha_before_date:
                        repo_latest_sha_before_date[repo] = {}
                    if date not in repo_latest_sha_before_date[repo]:
                        repo_latest_sha_before_date[repo][date] = set()
                    if repo_latest_sha_before_date[repo][date]: continue
                    for branch in repo_all_branch[repo]:
                        to_scrapy_list.add((repo, date, branch))
        save_text(f'{self.module_root_path}/commit_to_scrapy_list', to_scrapy_list)
        save_pickle(f'{self.module_root_path}/commit_to_scrapy_list.pkl', to_scrapy_list)
        
        print(f'to_scrapy_list size: {len(to_scrapy_list)}')
        # multi_thread(random.sample(list(to_scrapy_list), 10), get_latest_commit_before_date_all_sub, tokens = github_tokens)
        multi_thread(list(to_scrapy_list), get_latest_commit_before_date_all_sub, tokens = github_tokens)
        
        save_pickle(f'{path}.pkl', repo_latest_sha_before_date)
    
        print('end search repo\'s latest_commit_before_date')
        return repo_latest_sha_before_date


    def convert_data(self, data: dict):
        path = f'{self.repo_data_path}/repo_latest_sha_before_date'
        if os.path.exists(f'{path}.pkl'):
            return load_pickle(f'{path}.pkl')

        repo_latest_sha_before_date = {}
        for repo, v in data.items():
            if repo not in repo_latest_sha_before_date:
                repo_latest_sha_before_date[repo] = {}
            for date, shas in v.items():
                if shas:
                    repo_latest_sha_before_date[repo][date] = sorted(list(shas), key = lambda x: x[1], reverse = True)
            if not repo_latest_sha_before_date[repo]:
                del repo_latest_sha_before_date[repo]
        save_json(f'{path}.json', repo_latest_sha_before_date)
        save_pickle(f'{path}.pkl', repo_latest_sha_before_date)
        
        return repo_latest_sha_before_date


    def select_commit(self, data: dict):
        path = f'{self.module_root_path}/correct_commits'
        if os.path.exists(f'{path}.json'):
            return
        
        collected_commits = {}
        for cve in tqdm.tqdm(self.cve_list):
            adjusted_date = self.cve_data_all[cve]['adjusted_date']
            published_date = self.cve_data_all[cve]['published_date']
            for repo in self.cve_data_all[cve]['collected_repo']:
                if repo not in data: continue
                if adjusted_date in data[repo] and data[repo][adjusted_date]:
                    if cve not in collected_commits:
                        collected_commits[cve] = []
                    collected_commits[cve].append((repo, data[repo][adjusted_date][0][0]))
                    continue
                if published_date in data[repo] and data[repo][published_date]:
                    if cve not in collected_commits:
                        collected_commits[cve] = []
                    collected_commits[cve].append((repo, data[repo][published_date][0][0]))
        save_json(f'{self.module_root_path}/collected_commits.json', collected_commits)
        save_pickle(f'{self.module_root_path}/collected_commits.pkl', collected_commits)
        
        self.get_specified_file_list()
        correct_commits = self.check_commit_accuracy(collected_commits)
        save_json(f'{path}.json', correct_commits)
        save_pickle(f'{path}.pkl', correct_commits)

        for cve, commit in correct_commits.items():
            self.cve_data_all[cve]['collected_commit'] = commit
        
        save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
        save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)
    

    def get_specified_file_list(self):
        path = f'{self.repo_data_path}/repo_file_list'
        if os.path.exists(f'{path}.pkl'):
            return

        def get_specified_file_list_sub(cve_list_sub: list, token: str):
            for (repo, sha) in tqdm.tqdm(cve_list_sub):
                if repo not in repo_file_list:
                    repo_file_list[repo] = {}
                if sha not in repo_file_list[repo]:
                    repo_file_list[repo][sha] = get_file_list(repo, sha, token)

        data = load_pickle(f'{self.module_root_path}/collected_commits.pkl')
        repo_file_list = load_pickle(f'{path}.pkl') if os.path.exists(f'{path}.pkl') else {}

        data_list = list({
            (repo, sha)
            for v in data.values()
            for (repo, sha) in v
            if not (repo in repo_file_list and sha in repo_file_list[repo])
        })
        print(len(data_list))
        if len(data_list) != 0:
            multi_thread(data_list, get_specified_file_list_sub, tokens = github_tokens)
            save_json(f'{path}.json', repo_file_list)
            save_pickle(f'{path}.pkl', repo_file_list)


    def check_commit_accuracy(self, collected_commits: dict):
        def scrapy_file_content(to_scrapy_list_sub: list, token: str):
            for cve, repo, sha, file in tqdm.tqdm(to_scrapy_list_sub):
                dir = f'{self.file_content_dir}/{cve}'
                tp1 = repo.replace('/', '\\')
                tp2 = file.replace('/', '\\')
                save_path = f'{dir}/{tp1}____{tp2}'
                if os.path.exists(save_path):
                    continue
                res = get_file_content(repo, sha, file, token)
                if res:
                    os.makedirs(dir, exist_ok = True)
                    save_text(save_path, res)

        correct_commits = {}
        total_count = len(collected_commits)
        repo_file_list = load_pickle(f'{self.repo_data_path}/repo_file_list.pkl')

        if os.path.exists(f'{self.module_root_path}/rest_cve_list_1531.pkl'):
            rest_cve_list = load_pickle(f'{self.module_root_path}/rest_cve_list_1531.pkl')
        else:
            print('start check same repo and same file')     # 先遍历一轮，筛选repo和file相同的CVE
            rest_cve_list = []
            for cve, collected_commit in tqdm.tqdm(collected_commits.items()):
                for repo, sha in collected_commit:
                    if repo in self.cve_data_all[cve]['vulnerability_files']:
                        vul_file = self.cve_data_all[cve]['vulnerability_files'][repo][0].lower()
                        for file, _ in repo_file_list[repo][sha]:
                            if vul_file == file.lower():
                                correct_commits[cve] = (repo, sha, file)
                                break
                if cve not in correct_commits:
                    rest_cve_list.append(cve)
            print(f'{len(correct_commits)}/{total_count} cve with same repo and file')
            save_text(f'{self.module_root_path}/rest_cve_list_{len(rest_cve_list)}', rest_cve_list)
            save_pickle(f'{self.module_root_path}/rest_cve_list_{len(rest_cve_list)}.pkl', rest_cve_list)
            print('end check same repo and same file')

        print('start check different repo and same file')
        cnt = 0
        # to_scrapy_list = load_pickle('tp.pkl') if os.path.exists('tp.pkl') else []
        # multi_thread(to_scrapy_list, scrapy_file_content, tokens = github_tokens)

        for cve in tqdm.tqdm(rest_cve_list.copy()):
            for repo, sha in collected_commits[cve]:
                if cve in correct_commits: break
                for repo_ans, vul_files in self.cve_data_all[cve]['vulnerability_files'].items():
                    vul_file = vul_files[0].lower()
                    file = ''
                    for tp, _ in repo_file_list[repo][sha]:
                        if tp.lower() == vul_file:
                            file = tp
                            break
                    if not file: continue   # 没找到同名文件
                    gt_file = ''
                    for tp in os.listdir(f'{self.gt_content_path}/{cve}'):
                        if tp.startswith(repo_ans.replace('/', '\\')):
                            gt_file = tp
                            break
                    if not gt_file: continue    # 没有gt file content
                    dir = f'{self.file_content_dir}/{cve}'
                    tp1 = repo.replace('/', '\\')
                    tp2 = file.replace('/', '\\')
                    save_path = f'{dir}/{tp1}____{tp2}'
                    if os.path.exists(save_path):
                        content = load_file(save_path)
                    else:       # 没有file content
                        # to_scrapy_list.append((cve, repo, sha, file))
                        continue
                    gt_content = load_file(f'{self.gt_content_path}/{cve}/{gt_file}')
                    simi = difflib.SequenceMatcher(None, gt_content, content).ratio()
                    if simi > 0.486:
                        correct_commits[cve] = (repo, sha, file)
                        break
            if cve in correct_commits:
                cnt += 1
                rest_cve_list.remove(cve)
        # save_pickle('tp.pkl', to_scrapy_list)
        print(f'{cnt}/{total_count} cve with different repo and same file')
        save_text(f'{self.module_root_path}/rest_cve_list_{len(rest_cve_list)}', rest_cve_list)
        save_pickle(f'{self.module_root_path}/rest_cve_list_{len(rest_cve_list)}.pkl', rest_cve_list)
        print('end check different repo and same file')

        print('start check different file')
        to_scrapy_list = []
        for cve in tqdm.tqdm(rest_cve_list):
            for repo, sha in collected_commits[cve]:
                similarity_list = [
                    (file, difflib.SequenceMatcher(None, file.lower(), vul_file.lower()).ratio())
                    for file, isdir in repo_file_list[repo][sha]
                    if not isdir
                    for vul_file in self.cve_data_all[cve]['file_list']
                ]
                similarity_list = sorted(similarity_list, key = lambda x: x[1], reverse = True)
                to_scrapy_list.append((cve, repo, sha, similarity_list[0][0]))
        print(f'to_scrapy_list size: {len(to_scrapy_list)}')
        
        print('start scrapy file content')
        multi_thread(to_scrapy_list, scrapy_file_content, tokens = github_tokens)
        print('end scrapy file content')

        print('start check by file content')
        gt_not_found_list = []
        candidate_not_found_list = []
        similarity_table_path = f'{self.module_root_path}/similarity_table'
        similarity_table = load_pickle(f'{similarity_table_path}.pkl') if os.path.exists(f'{similarity_table_path}.pkl') else {}

        for cve in tqdm.tqdm(rest_cve_list.copy()):
            if cve not in similarity_table or not similarity_table[cve]:
                similarity_table[cve] = {}
                gt_content_path = f'{self.gt_content_path}/{cve}'
                file_content_path = f'{self.file_content_dir}/{cve}'
                if not os.path.exists(gt_content_path):
                    gt_not_found_list.append(cve)
                    continue
                if not os.path.exists(file_content_path):
                    candidate_not_found_list.append(cve)
                    continue
                file_ans_list = [
                    file
                    for file in os.listdir(gt_content_path) if file not in ['.DS_Store']
                ]
                file_candidate_list = [
                    file
                    for file in os.listdir(file_content_path) if file not in ['.DS_Store']
                ]
                max_simi = 0
                selected_file = ''
                for file_candidate in file_candidate_list:
                    for file_ans in file_ans_list:
                        candidate_content = load_file(f'{file_content_path}/{file_candidate}')
                        gt_content = load_file(f'{gt_content_path}/{file_ans}')
                        simi = difflib.SequenceMatcher(None, candidate_content, gt_content).ratio()
                        if simi > max_simi:
                            max_simi = simi
                            selected_file = file_candidate
                similarity_table[cve]['simi'] = max_simi
                similarity_table[cve]['repo'] = selected_file.split('____')[0].replace('\\', '/')
                similarity_table[cve]['file'] = selected_file.split('____')[1].replace('\\', '/')
        
            if similarity_table[cve]['simi'] > 0.75:
                rest_cve_list.remove(cve)
                repo, sha = list(
                    filter(lambda x: x[0] == similarity_table[cve]['repo'], collected_commits[cve])
                )[0]
                correct_commits[cve] = (repo, sha, similarity_table[cve]['file'])

        save_json(f'{similarity_table_path}.json', similarity_table)
        save_pickle(f'{similarity_table_path}.pkl', similarity_table)

        # save_text(f'{self.module_root_path}/incorrrect_cve_list', rest_cve_list)
        if gt_not_found_list:
            save_text(f'{self.module_root_path}/gt_not_found_cve_list', gt_not_found_list)
        if candidate_not_found_list:
            save_text(f'{self.module_root_path}/candidate_not_found_list', candidate_not_found_list)
        print('end check by file content')

        correct_cnt = total_count - len(rest_cve_list)
        print(f'{len(gt_not_found_list) + len(candidate_not_found_list)} lack content')
        print('{:.2f}%'.format(correct_cnt / total_count * 100), f'({correct_cnt}/{total_count})')

        return correct_commits