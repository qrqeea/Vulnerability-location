import tqdm
import random
import difflib
from util.io_util import *
from util.github_util import *


class CommitCollection:

    def __init__(self, project_root_path: str, module_root_path: str, repo_data_path: str, gt_content_path: str, cve_data_all: dict):
        self.project_root_path = project_root_path
        self.module_root_path = module_root_path
        self.gt_content_path = gt_content_path
        self.repo_data_path = repo_data_path
        self.cve_data_all = cve_data_all

        os.makedirs(self.module_root_path, exist_ok = True)
        os.makedirs(self.repo_data_path, exist_ok = True)

        self.file_content_dir = f'{self.module_root_path}/file_content'
        os.makedirs(self.file_content_dir, exist_ok = True)

        # 这个模块的target cve list是上一步收集到repo的cve
        self.cve_list = [k for k, v in self.cve_data_all.items() if 'collected_repo' in v]
        # print(f'cve count: {len(self.cve_list)}')

        repo_all = set()
        for _, v in self.cve_data_all.items():
            if 'collected_repo' in v:
                repo_all |= set(v['collected_repo'])
        self.repo_all = repo_all
        # print(f'repo count: {len(self.repo_all)}')


    def start(self):
        repo_all_branch = self.get_repo_all_branch()
        data = self.get_latest_commit_before_date_all(repo_all_branch)
        repo_latest_sha_before_date = self.convert_data(data)
        self.select_commit(repo_latest_sha_before_date)
        self.get_specified_file_list()
        self.check_commit_accuracy()


    def get_repo_all_branch(self):
        path = f'{self.repo_data_path}/repo_all_branch'
        if os.path.exists(f'{path}.pkl'):
            return load_pickle(f'{path}.pkl')

        def get_repo_all_branch_sub(repo_list_sub: list, token: str):
            for repo in tqdm.tqdm(repo_list_sub):
                all_branch = get_all_branch(repo, token)
                repo_all_branch[repo] = all_branch

        print('start search repo\'s all branch')

        repo_all_branch = load_pickle(f'{path}.pkl') if os.path.exists(f'{path}.pkl') else {}
        
        rest_repo = list(set(self.repo_all) - set(repo_all_branch.keys()))
        print(f'rest repo count: {len(rest_repo)}')
        
        multi_thread(rest_repo, get_repo_all_branch_sub, tokens = github_tokens)

        save_json(f'{path}.json', repo_all_branch)
        save_pickle(f'{path}.pkl', repo_all_branch)

        print('end search repo\'s all branch\n')
        return repo_all_branch        


    def get_latest_commit_before_date_all(self, repo_all_branch: dict):
        # 以repo为key，搜索所有分支在某个日期前的最新commit
        path = f'{self.repo_data_path}/repo_latest_sha_before_date(set)'
        if os.path.exists(f'{path}.pkl'):
            return load_pickle(f'{path}.pkl')

        if not any('adjusted_date' in v for v in self.cve_data_all.values()):
            for cve in self.cve_list:
                published_date = self.cve_data_all[cve]['published_date']
                year1 = int(published_date[:4])
                year2 = int(cve[4:8])
                if abs(year1 - year2) > 1:
                    self.cve_data_all[cve]['adjusted_date'] = f'{year2}-06-30T00:00Z'
                else:
                    self.cve_data_all[cve]['adjusted_date'] = published_date
            save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
            save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)
        
        def get_latest_commit_before_date_all_sub(to_scrapy_list_sub: list, token: str):
            for repo, date, branch in tqdm.tqdm(to_scrapy_list_sub):
                commit_sha = get_latest_commit_before_date(repo, date, token, branch)
                if commit_sha:
                    repo_latest_sha_before_date[repo][date].add(commit_sha)

        print('start search repo\'s latest_commit_before_date')
        
        repo_latest_sha_before_date = load_pickle(f'{path}.pkl') if os.path.exists(f'{path}.pkl') else {}
        to_scrapy_list = set()
        for cve in self.cve_list:
            dates = {self.cve_data_all[cve]['adjusted_date'], self.cve_data_all[cve]['published_date']}
            for date in dates:
                for repo in self.cve_data_all[cve]['collected_repo']:
                    if repo not in repo_latest_sha_before_date:
                        repo_latest_sha_before_date[repo] = {}
                    if date not in repo_latest_sha_before_date[repo]:
                        repo_latest_sha_before_date[repo][date] = set()
                    if repo_latest_sha_before_date[repo][date]: continue
                    for branch in repo_all_branch[repo]:
                        to_scrapy_list.add((repo, date, branch))
        save_text(f'{self.module_root_path}/to_scrapy_list', to_scrapy_list)
        save_pickle(f'{self.module_root_path}/to_scrapy_list.pkl', to_scrapy_list)
        
        print(f'to_scrapy_list size: {len(to_scrapy_list)}')
        # multi_thread(random.sample(list(to_scrapy_list), 10), get_latest_commit_before_date_all_sub, tokens = github_tokens)
        multi_thread(list(to_scrapy_list), get_latest_commit_before_date_all_sub, tokens = github_tokens)
        
        save_pickle(f'{path}.pkl', repo_latest_sha_before_date)
    
        print('end search repo\'s latest_commit_before_date')
        return repo_latest_sha_before_date


    def convert_data(self, data: dict):
        path = f'{self.repo_data_path}/repo_latest_sha_before_date'
        if os.path.exists(f'{path}.pkl'):
            return load_pickle(f'{path}.pkl')

        repo_latest_sha_before_date = {}
        for repo, v in data.items():
            if repo not in repo_latest_sha_before_date:
                repo_latest_sha_before_date[repo] = {}
            for date, shas in v.items():
                if shas:
                    repo_latest_sha_before_date[repo][date] = sorted(list(shas), key = lambda x: x[1], reverse = True)
            if not repo_latest_sha_before_date[repo]:
                del repo_latest_sha_before_date[repo]
        save_json(f'{path}.json', repo_latest_sha_before_date)
        save_pickle(f'{path}.pkl', repo_latest_sha_before_date)
        
        return repo_latest_sha_before_date


    def select_commit(self, data: dict):
        path = f'{self.module_root_path}/collected_commits'
        if os.path.exists(f'{path}.json'):
            return
        
        collected_commits = {}
        for cve in tqdm.tqdm(self.cve_list):
            adjusted_date = self.cve_data_all[cve]['adjusted_date']
            published_date = self.cve_data_all[cve]['published_date']
            for repo in self.cve_data_all[cve]['collected_repo']:
                if repo not in data: continue
                if adjusted_date in data[repo] and data[repo][adjusted_date]:
                    if cve not in collected_commits:
                        collected_commits[cve] = []
                    collected_commits[cve].append((repo, data[repo][adjusted_date][0][0]))
                    continue
                if published_date in data[repo] and data[repo][published_date]:
                    if cve not in collected_commits:
                        collected_commits[cve] = []
                    collected_commits[cve].append((repo, data[repo][published_date][0][0]))

        self.check_commit_accuracy(collected_commits)
        
        # save_json(f'{path}.json', collected_commits)
        # save_pickle(f'{path}.pkl', collected_commits)

        # for cve in self.cve_data_all:
        #     if 'collected_commit' in self.cve_data_all[cve]:
        #         del self.cve_data_all[cve]['collected_commit']

        # for cve, commits in collected_commits.items():
        #     self.cve_data_all[cve]['collected_commit'] = commits
        
        # save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
        # save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)
    

    def get_specified_file_list(self):
        path = f'{self.repo_data_path}/repo_file_list'
        if os.path.exists(f'{path}.pkl'):
            return load_pickle(f'{path}.pkl')

        def get_specified_file_list_sub(cve_list_sub: list, token: str):
            for (repo, sha) in tqdm.tqdm(cve_list_sub):
                if repo not in repo_file_list:
                    repo_file_list[repo] = {}
                if sha not in repo_file_list[repo]:
                    repo_file_list[repo][sha] = get_file_list(repo, sha, token)

        data = load_pickle(f'{self.module_root_path}/collected_commits.pkl')
        repo_file_list = load_pickle(f'{path}.pkl') if os.path.exists(f'{path}.pkl') else {}

        data_list = list({
            (repo, sha)
            for v in data.values()
            for (repo, sha) in v
            if not (repo in repo_file_list and sha in repo_file_list[repo])
        })
        print(len(data_list))
        multi_thread(data_list, get_specified_file_list_sub, tokens = github_tokens)

        save_json(f'{path}.json', repo_file_list)
        save_pickle(f'{path}.pkl', repo_file_list)


    def check_commit_accuracy(self, collected_commits: dict):
        total_count = len(collected_commits)
        to_scrapy_list_path = f'{self.module_root_path}/to_scrapy_list'
        if os.path.exists(f'{to_scrapy_list_path}.pkl'):
            to_scrapy_list = load_pickle(f'{to_scrapy_list_path}.pkl')
        else:
            to_scrapy_list = []
            repo_file_list = load_pickle(f'{self.repo_data_path}/repo_file_list.pkl')
            print('start check by repo name and file name')
            
            for cve, collected_commit in tqdm.tqdm(collected_commits.items()):
                flag = False
                for repo, sha in collected_commit:
                    if flag: break
                    if repo in self.cve_data_all[cve]['vulnerability_files']:
                        vul_file = self.cve_data_all[cve]['vulnerability_files'][repo][0].lower()
                        if any(vul_file == file.lower() for file, _ in repo_file_list[repo][sha]):
                            flag = True
                if not flag:
                    for repo, sha in collected_commit:
                        similarity_list = [
                            (file, difflib.SequenceMatcher(None, file.lower(), vul_file.lower()).ratio())
                            for file, isdir in repo_file_list[repo][sha]
                            if not isdir
                            for vul_file in self.cve_data_all[cve]['file_list']
                        ]
                        similarity_list = sorted(similarity_list, key = lambda x: x[1], reverse = True)
                        to_scrapy_list.append((cve, repo, sha, similarity_list[0][0]))

            save_text(f'{to_scrapy_list_path}', to_scrapy_list)
            save_pickle(f'{to_scrapy_list_path}.pkl', to_scrapy_list)
            print('end check by repo name and file name')
        incorrect_cve_list = list({ cve
            for cve, _, _, _ in to_scrapy_list
        })
        print(f'{total_count - len(incorrect_cve_list)} cve has the same repo and file')
        
        print('start scrapy file content')
        def scrapy_file_content(to_scrapy_list_sub: list, token: str):
            for cve, repo, sha, file in tqdm.tqdm(to_scrapy_list_sub):
                dir = f'{self.file_content_dir}/{cve}'
                tp1 = repo.replace('/', '\\')
                tp2 = file.replace('/', '\\')
                save_path = f'{dir}/{tp1}____{tp2}'
                if os.path.exists(save_path):
                    continue
                res = get_file_content(repo, sha, file, token)
                if res:
                    os.makedirs(dir, exist_ok = True)
                    save_text(save_path, res)

        multi_thread(to_scrapy_list, scrapy_file_content, tokens = github_tokens)
        print('end scrapy file content')

        print('start check by file content')
        gt_not_found_list = []
        candidate_not_found_list = []
        similarity_table_path = f'{self.module_root_path}/similarity_table'
        if os.path.exists(f'{similarity_table_path}.pkl'):
            similarity_table = load_pickle(f'{similarity_table_path}.pkl')
        else:
            similarity_table = {}
        for cve in tqdm.tqdm(incorrect_cve_list.copy()):
            if cve not in similarity_table or not similarity_table[cve]:
                similarity_table[cve] = {}
                gt_content_path = f'{self.gt_content_path}/{cve}'
                file_content_path = f'{self.file_content_dir}/{cve}'
                if not os.path.exists(gt_content_path):
                    gt_not_found_list.append(cve)
                    continue
                if not os.path.exists(file_content_path):
                    candidate_not_found_list.append(cve)
                    continue
                file_ans_list = [
                    file
                    for file in os.listdir(gt_content_path) if file not in ['.DS_Store']
                ]
                file_candidate_list = [
                    file
                    for file in os.listdir(file_content_path) if file not in ['.DS_Store']
                ]
                max_simi = 0
                selected_file = ''
                for file_candidate in file_candidate_list:
                    for file_ans in file_ans_list:
                        candidate_content = load_file(f'{file_content_path}/{file_candidate}')
                        gt_content = load_file(f'{gt_content_path}/{file_ans}')
                        simi = difflib.SequenceMatcher(None, candidate_content, gt_content).ratio()
                        if simi > max_simi:
                            max_simi = simi
                            selected_file = file_candidate
                similarity_table[cve]['simi'] = max_simi
                similarity_table[cve]['repo'] = selected_file.split('____')[0].replace('\\', '/')
                similarity_table[cve]['file'] = selected_file.split('____')[1].replace('\\', '/')
        
            if similarity_table[cve]['simi'] > 0.8:
                incorrect_cve_list.remove(cve)

        save_json(f'{similarity_table_path}.json', similarity_table)
        save_pickle(f'{similarity_table_path}.pkl', similarity_table)

        save_text(f'{self.module_root_path}/incorrrect_cve_list', incorrect_cve_list)
        if gt_not_found_list:
            save_text(f'{self.module_root_path}/gt_not_found_cve_list', gt_not_found_list)
        if candidate_not_found_list:
            save_text(f'{self.module_root_path}/candidate_not_found_list', candidate_not_found_list)
        print('end check by file content')
        
        correct_cnt = total_count - len(incorrect_cve_list)
        print(f'{len(gt_not_found_list) + len(candidate_not_found_list)} lack content')
        print('{:.2f}%'.format(correct_cnt / total_count * 100), f'({correct_cnt}/{total_count})')


        # TODO 保存到cve_data_all
        # save_json(f'{self.project_root_path}/cve_data_all.json', self.cve_data_all)
        # save_pickle(f'{self.project_root_path}/cve_data_all.pkl', self.cve_data_all)