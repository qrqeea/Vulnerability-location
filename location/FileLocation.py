import os
import sys
import csv
import ast
import tqdm
import random
import pandas as pd
from util.io_util import *
from util.general_util import *
from util.similarity_util import *

# language_map = {
    # 'c': 'location/extract_code/c.js'
    # 'cpp': 'location/extract_code/cpp.js',
    # 'java': 'location/extract_code/java.js',
    # 'js': 'location/extract_code/js.js',
# }

class FileLocation:

    def __init__(self, module_root_path: str, repo_path: str, cve_data_all: dict, candidates: dict):
        self.module_root_path = module_root_path
        self.repo_path = repo_path
        self.cve_data_all = cve_data_all
        self.candidates = candidates

        os.makedirs(self.module_root_path, exist_ok = True)

        self.prompt_dir = f'{self.module_root_path}/prompt'
        os.makedirs(self.prompt_dir, exist_ok = True)

        self.result_dir = f'{self.module_root_path}/result'
        os.makedirs(self.result_dir, exist_ok = True)

        self.embedding_text = f'{self.module_root_path}/embedding_text'
        os.makedirs(self.embedding_text, exist_ok = True)
        
        self.embedding_result = f'{self.module_root_path}/embedding_result'
        os.makedirs(self.embedding_result, exist_ok = True)

        self.cve_list = list(candidates.keys())
        print(len(self.cve_list))
        save_text(f'{self.module_root_path}/cve_list', self.cve_list)
        save_pickle(f'{self.module_root_path}/cve_list.pkl', self.cve_list)


    def start(self):
        # self.split_file_content()
        # self.text_to_vector()
        self.calc_similarity()
        # self.generate_prompt()

    
    def split_file_content(self, block_max_token = 2048):
        for cve, files in tqdm.tqdm(self.candidates.items()):
            repo = self.cve_data_all[cve]['collected_commit'][0].replace('/', '—')
            path = f'{self.repo_path}/{cve}/{repo}'
            if not os.path.exists(path):
                print(cve, repo, 'not exist')
                continue
            df = pd.DataFrame({
                'file': [],
                'token_len': [],
                'text': []
            })
            for file in files:
                if not os.path.exists(f'{path}/{file}'):
                    print(cve, repo, file, 'not found')
                    continue
                try:
                    content = load_file(f'{path}/{file}')
                    text_len = len(content)
                    token_len = calc_token(content)
                    if token_len > block_max_token:
                        block_size = math.ceil(token_len / block_max_token)
                        block_len = int(text_len / block_size)
                        l = 0
                        for i in range(1, block_size):
                            r = block_len * i
                            while r > l and calc_token(content[l:r]) > block_max_token:
                                r -= 50
                            if content[l:r]:
                                df.loc[len(df)] = [
                                    file,
                                    f'{calc_token(content[l:r])}/{token_len}',
                                    content[l:r]
                                ]
                            l = r
                        if content[l:]:
                            df.loc[len(df)] = [
                                file,
                                f'{calc_token(content[l:])}/{token_len}',
                                content[l:]
                            ]
                    else:
                        df.loc[len(df)] = [file, f'{token_len}/{token_len}', content]
                except Exception as e:
                    print(cve, repo, file, 'load failure', e)
                    continue
            df.to_csv(
                f'{self.embedding_text}/{cve}.csv',
                index = False,
                quotechar = '"',
                quoting = csv.QUOTE_ALL
            )


    def text_to_vector(self):
        
        def get_embedding(text: str):
            return client.embeddings.create(input = [text], model = 'text-embedding-3-small').data[0].embedding     

        load_dotenv()
        client = OpenAI()

        def text_to_vector_sub(cve_list_sub: list):
            # for cve in tqdm.tqdm(cve_list_sub):
            for cve in tqdm.tqdm(random.sample(cve_list_sub, 1)):
                target_path = f'{self.embedding_result}/{cve}.csv'
                if os.path.exists(target_path):
                    continue

                df = pd.read_csv(f'{self.embedding_text}/{cve}.csv')
                df['vector'] = df.text.apply(lambda x: get_embedding(x))
                df.to_csv(
                    target_path,
                    index = False, 
                    quotechar = '"',
                    quoting = csv.QUOTE_ALL
                )

        rest_cve = list(
            {file.split('.')[0] for file in os.listdir(self.embedding_text) if file not in ['.DS_Store']} -
            {file.split('.')[0] for file in os.listdir(self.embedding_result) if file not in ['.DS_Store']}
        )
        print(len(rest_cve))

        print('start embedding')
        multi_thread(rest_cve, text_to_vector_sub, chunk_size = 500)
        print('end embedding')


    def calc_similarity(self):
        print('start calc similarity')

        df_query = pd.read_csv('/Volumes/NVD/experiment_data/completion/query_embedding_result.csv')
        tp = {}
        for _, row in df_query.iterrows():
            tp[row.cve] = ast.literal_eval(row.vector)

        for file in tqdm.tqdm(os.listdir(self.embedding_result)):
            if file in ['.DS_Store']: continue
            
            cve = file.split('.')[0]
            df = pd.read_csv(f'{self.embedding_result}/{cve}.csv')
            if len(df.columns) == 5:
                continue
            if cve not in tp:
                continue
            query_vector = tp[cve]
            
            df['similarity'] = df.vector.apply(lambda x: cosine_similarity(ast.literal_eval(x), query_vector))
            df = df.sort_values('similarity', ascending = False)
            df.to_csv(
                f'{self.embedding_result}/{cve}.csv',
                index = False, 
                quotechar = '"',
                quoting = csv.QUOTE_ALL
            )
        print('end calc similarity')


    def generate_prompt(self):
        prompt_template = load_file(f'{self.module_root_path}/prompt_location_vul_file')
        for cve, files in self.candidates.items():
            # if len(files > 5):
            #     continue
            repo = self.cve_data_all[cve]['collected_commit'][0].replace('/', '—')
            path = f'{self.repo_path}/{cve}/{repo}'
            if not os.path.exists(path):
                continue
            index = 1
            tp = ''
            for file in files:
                try:
                    content = load_file(f'{path}/{file}')
                    content = format_text(clear_comment(content), ' ')
                    tp += f'file{index}: "{file}"\n{content}\n\n'
                except Exception as e:
                    # print('error', cve, repo, file)
                    continue
                index += 1
            if not tp:
                continue
            prompt = prompt_template.replace(
                '{description}',
                self.cve_data_all[cve]['complete_description'] if 'complete_description' in self.cve_data_all[cve] else self.cve_data_all[cve]['original_description']
            ).replace('{files}', tp)
            save_text(f'{self.prompt_dir}/{cve}', prompt)


    def count_total_tokens(self):
        tokens = 0
        for cve, files in tqdm.tqdm(self.candidates.items()):
            repo = self.cve_data_all[cve]['collected_commit'][0]
            repo = repo.replace('/', '—')
            path = f'{self.repo_path}/{cve}/{repo}'
            if not os.path.exists(path):
                continue
            if len(files) > 100:
                continue
            for file in files:
                try:
                    content = load_file(f'{path}/{file}')
                    # 删除注释
                    tokens += calc_token(content)
                except Exception as e:
                    # print('error', cve, repo, file)
                    continue
        token_M = int(tokens / 1000000)
        print(f'total token: {token_M}M, price: {token_M * 0.5}$')


    def count_file_cnt(self):
        size_list = []
        cnt = 0
        for cve, files in self.candidates.items():
            size_list.append(len(files))
            cnt += len(files)
        print(cnt)
        print(count_range(size_list, [1, 2, 5, 10, 20, 50, 100, 200, 300, 350, 380]))


    # def extract_candidate_files(self):
    #     for cve, files in tqdm.tqdm(self.candidates.items()):
    #         repo = self.cve_data_all[cve]['collected_commit'][0].replace('/', '—')
    #         path = f'{self.repo_path}/{cve}/{repo}'
    #         if not os.path.exists(path):
    #             print(cve, repo, 'not exist')
    #             continue
    #         dir = f'{self.extract_result_dir}/{cve}'
    #         os.makedirs(dir, exist_ok = True)
    #         for file in files:
    #             if os.path.exists(f'{path}/{file}'):
    #                 try:
    #                     suffix = file.split('/')[-1].split('.')[-1].lower()
    #                     if suffix in language_map:
    #                         js_script = language_map[suffix]
    #                         command = ['node', js_script, f'{path}/{file}']
    #                         print(command)
    #                         result = subprocess.run(command, capture_output = True, text = True)
    #                         tp = file.replace('/', '\\')
    #                         save_path = f'{self.extract_result_dir}/{cve}/{tp}'
    #                         save_text(save_path, result.stdout)
    #                         print(f'{path}/{file}')
    #                         sys.exit()
    #                     else:       # 使用文件原始内容
    #                         pass
    #                 except Exception as e:
    #                     print(f'load {path}/{file} failure', e)
    #             else:
    #                 print(cve, repo, file, 'not exist')