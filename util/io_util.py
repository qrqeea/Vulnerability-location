import os
import json
import math
import pickle
import shutil
import tiktoken
import requests
import threading
import numpy as np
from openai import OpenAI
from dotenv import load_dotenv
from urllib.parse import urlparse

load_dotenv()

def query_openai(prompt: str, model = 'gpt-3.5-turbo-0125'):
    # 'gpt-4-turbo'

    client = OpenAI(api_key = os.environ.get('OPENAI_API_KEY'))

    chat_completion = client.chat.completions.create(
        messages = [
            {
                'role': 'user',
                'content': prompt,
            }
        ],
        model = model,
        temperature = 0
    )
    # print(chat_completion.choices[0].message.content)
    return chat_completion.choices[0].message.content


def get_domain(url):
    parsed_url = urlparse(url)
    return parsed_url.netloc


def copy_file(src_file, dest_file):
    shutil.copy(src_file, dest_file)

 
def load_pickle(file_path):
    with open(file_path, 'rb') as f:
        data = pickle.load(f)
        return data


def save_pickle(path, data):
    with open(path, 'wb') as f:
        pickle.dump(data, f)


def save_text(path, data, mode = 'w'):
    with open(path, mode) as f:
        if isinstance(data, dict):
            if isinstance(list(data.values())[0], (set, list)):
                for k, v in data.items():
                    print(k, file = f)
                    for item in v:
                        print(item, file = f)
            else:    
                for k, v in data.items():
                    print(k, ': ', v, file = f)
        elif isinstance(data, (set, list)):
            for i in data:
                print(i, file = f)
        else:
            print(data, file = f)


def save_json(path, data):
    with open(path, 'w') as f:
        print(json.dumps(data), file = f)


def load_json(path, mode = 'r', encoding = 'utf-8'):
    with open(path, mode, encoding = encoding) as f:
        return json.load(f)

def load_file(path, mode = 'r'):
    with open(path, mode, encoding = 'utf-8', errors = 'ignore') as f:
        return f.read()


def count_range(data: list, interval: list):
    res = {}
    for i in range(len(interval)):
        if i == 0:
            res[f'1-{interval[0]}'] = 0
        if i == len(interval) - 1:
            res[f'>{interval[-1]}'] = 0
            continue
        else:
            l = interval[i] + 1
            r = interval[i + 1]
            res[f'{l}-{r}'] = 0

    for item in data:
        for i, value in enumerate(interval):
            if (i == 0 and item <= value) or (i == len(interval) - 1) or (item <= interval[i + 1]):
                if i == 0 and item <= value:
                    res[f'1-{value}'] += 1
                elif i == len(interval) - 1:
                    res[f'>{value}'] += 1
                else:
                    res[f'{value + 1}-{interval[i + 1]}'] += 1
                break
    return res


def calc_token(text, model = 'text-embedding-3-small'):
    # 'text-embedding-3-small'
    enc = tiktoken.get_encoding('cl100k_base')
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))


def format_text(text: str, separator = '\n'):
    lines = text.split('\n')

    # 去除每行前后的空格，并过滤掉空行
    stripped_lines = [line.strip() for line in lines if line.strip()]

    result = separator.join(stripped_lines)
    return result


def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def multi_thread(data_list: list, target, chunk_size = 300, tokens = None):
    if tokens:
        chunk_size = max(1, math.ceil(len(data_list) / len(tokens)))
    chunks = [data_list[i:i + chunk_size] for i in range(0, len(data_list), chunk_size)]

    threads = []
    for i in range(len(chunks)):
        if tokens:
            args = (chunks[i], tokens[i],)
        else:
            args = (chunks[i],)
        thread = threading.Thread(target = target, args = args)
        threads.append(thread)
    
    for thread in threads:
        thread.start()

    for thread in threads:
        thread.join()


def get_redirected_url(url):
    response = requests.head(url, allow_redirects = True)
    return response.url


def clear_comment(content: str):
    while True:
        l = content.find('/*')
        r = content.find('*/')
        if l != -1 and r != -1 and r > l:
            content = content[:l] + content[r + 2:]
        else:
            return content


if __name__ == '__main__':
    def f(data_list: list, res):
        for i in data_list:
            res[f'{i}'] = i

    res = {}
    multi_thread([1,2,3,4,5], f, res = res)
    print(res)