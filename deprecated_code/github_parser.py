import tqdm
from Post_loader import *
from github import Github
import datetime
import argparse


def parse_repo_from_commit_vbom():
    # Loads CVE and associated GitHub commit URLs from a pickle file
    commit_file = PostLoader().load_pickle('/output_vbom/ini_file/cve_github_commit.pkl')

    # Initializes the GitHub API client with a token
    g = Github('your github token')

    # Initializes a dictionary to store the mapping of CVEs to repository objects
    store = {}

    # Iterates over each CVE and associated commit URLs
    for idx, (k, v) in enumerate(tqdm.tqdm(commit_file.items())):
        # Initializes a list for storing repositories for each CVE
        store[k] = []
        # Creates a set to hold unique repository data
        repo_data = set()
        # Iterates over each commit URL
        for one_commit in list(v): 
            try:
                # Extracts repository name from commit URL and adds it to the set
                repo_data.add(one_commit.split('/commit')[0].split('.com/')[-1])
            except:
                continue
        # Iterates over each unique repository name
        for repo_name in repo_data:
            try:
                # Retrieves the repository object using GitHub API and adds it to the list
                repo = g.get_repo(repo_name)
                store[k].append(repo)
            except Exception as e:
                continue

            try:
                # Checks the GitHub API rate limit
                rate_limit = g.get_rate_limit()
                remaining = rate_limit.raw_data['core']['remaining']
                reset = rate_limit.raw_data['core']['reset']
                # If rate limit is close to being reached, waits until reset time
                if remaining <= 5:
                    print(reset-datetime.datetime.utcnow().timestamp())
                    time.sleep(reset-datetime.datetime.utcnow().timestamp())
            except:
                # Handles any exception related to rate limit and waits for a minute
                print('abnormal no limit')
                time.sleep(60)
    # Saves the CVE to repository mapping to a pickle file
    PostWriter('/output_vbom/parsed_file/cve_github_commit_repo_extra.pkl').write_pickle(store)


def parse_supply_commit_pull_issues():
    """
    This function tries to get commit/issue/pull info from urls
    """
    # format: {cve_id:{'commits':set(urls), 'issues':set(urls), 'pulls':set(urls)}}
    record = PostLoader().load_pickle('/output/parsed_file/multi_supply_commit_pull_issue.pkl')

    g = Github('your github token')
    parse_count = 0

    parsed_issue_commit = {}

    for k, v in tqdm.tqdm(record.items()):

        parsed_issue_commit[k] = {'cve':k, 'commits':set(), 'pulls':set(), 'issues':set()}
        for one_commit_url in v['commits']:
            commit_id = one_commit_url.split('/')[-1]  # Extracts commit ID from the URL
            try:
                repo_name = one_commit_url.split('/commits')[0].split('repos/')[-1]
                repo = g.get_repo(repo_name)
                commit = repo.get_commit(sha=commit_id)
                parsed_issue_commit[k]['commits'].add(commit)
            except Exception as e:
                print(e)
                print(commit_id, k)
                print(3)
                continue

            parse_count += 1
            if parse_count % 28 == 0 and parse_count != 0:
                time.sleep(60)
            if parse_count % 998 == 0 and parse_count != 0:
                time.sleep(3600)
            if parse_count % 100 == 0 and parse_count != 0:
                PostWriter('/output/parsed_file/link_pull_issue_commit_1.pkl').write_pickle(parsed_issue_commit)

        for one_pull_url in v['pulls']:
            number = one_pull_url.split('/')[-1]
            try:
                repo_name = '/'.join(one_pull_url.split('/')[-4:-2]).split('.')[0]
                repo = g.get_repo(repo_name)
                pull = repo.get_pull(int(number))
                parsed_issue_commit[k]['pulls'].add(pull)
            except Exception as e:
                print(e)
                print(number, k)
                print(3)
                continue
            parse_count += 1
            if parse_count % 28 == 0 and parse_count != 0:
                time.sleep(60)
            if parse_count % 998 == 0 and parse_count != 0:
                time.sleep(3600)
            if parse_count % 100 == 0 and parse_count != 0:
                PostWriter('/output/parsed_file/link_pull_issue_commit_1.pkl').write_pickle(parsed_issue_commit)

        for one_issue_url in v['issues']:
            number = one_issue_url.split('/')[-1].split('#')[0]
            try:
                repo_name = '/'.join(one_issue_url.split('/')[-4:-2]).split('.')[0]
                repo = g.get_repo(repo_name)
                issue = repo.get_issue(int(number))
                parsed_issue_commit[k]['issues'].add(issue)
            except Exception as e:
                print(e)
                print(number, k)
                print(3)
                continue
            parse_count += 1
            if parse_count % 28 == 0 and parse_count != 0:
                time.sleep(60)
            if parse_count % 998 == 0 and parse_count != 0:
                time.sleep(3600)
            if parse_count % 100 == 0 and parse_count != 0:
                PostWriter('/output/parsed_file/link_pull_issue_commit_1.pkl').write_pickle(parsed_issue_commit)
    PostWriter('/output/parsed_file/link_pull_issue_commit_1.pkl').write_pickle(parsed_issue_commit)
    print(1)


def parse_commits_from_pull():
    """
    This function tries to get commit info from pull
    """
    # format: {cve_id: set(list of tuples (repo_name, pull_url))}
    pull_dict = PostLoader().load_pickle('/output_vbom/ini_file/cve_github_pull_multi_supply.pkl')
    repo_file = PostLoader().load_pickle('/output_vbom/parsed_file/cve_github_pull_issues_repo_multi.pkl')  # parsed repos

    # Dictionary to store repo objects indexed by CVE and repo name
    repo_dict = {}
    for k, v in repo_file.items():
        for one_v in v:
            repo_dict[k, one_v.full_name] = one_v

    g = Github('your github token')
    store = {}
    for k, v in tqdm.tqdm(pull_dict.items()):
        for one_pull in v:
            try:
                key = (k, one_pull[0])
                if repo_dict.get(key) is not None:

                    try:
                        rate_limit = g.get_rate_limit()
                        remaining = rate_limit.raw_data['core']['remaining']
                        reset = rate_limit.raw_data['core']['reset']
                        if remaining <= 25:
                            timestamp = time.time()
                            wait_second = int(reset - timestamp)
                            if wait_second <= 0:
                                continue
                            print('wait ', wait_second)
                            time.sleep(wait_second + 2)
                    except:
                        print('abnormal no limit')
                        time.sleep(60)
                    repo = repo_dict[key]
                    number = one_pull[1].split('/')[-1]
                    linked_pull_request = repo.get_pull(int(number))
                    linked_commits = linked_pull_request.get_commits()
                    linked_commits = list(linked_commits)
                    store[k, one_pull[1]] = linked_commits
            except:
                print('error')
                continue
    PostWriter('/output_vbom/parsed_file/cve_github_pull_multi_commits_supply.pkl').write_pickle(store)


def parse_concurrent_date2():
    """
    This function tries to find commit info from 90 days before the nvd disclosure dates
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("--index", default='1', type=str, )
    parser.add_argument("--git_token", default='your github token', type=str, )
    args = parser.parse_args()

    # format: {cve_id:[list of repo names]}
    file_r = PostLoader().load_pickle('/output/date_need_parse_allrepo_' + args.index + '_gpt3.pkl')
    date_file = PostLoader().load_pickle('/output/nvd_pub_dict_2022_07.pkl')  # dict of nvd disclosure dates
    # example: cve_id: {'cves': ['CVE-1999-0001'], 'publishedDate': datetime.datetime(1999, 12, 30, 5, 0)}
    g = Github(args.git_token)
    store = {}

    for k, v in tqdm.tqdm(file_r.items()):
        store[k] = []
        if date_file.get(k) is None:
            continue
        date_data = date_file[k]['publishedDate']
        for one_repo in v:
            try:
                rate_limit = g.get_rate_limit()
                remaining = rate_limit.raw_data['core']['remaining']
                reset = rate_limit.raw_data['core']['reset']
                if remaining <= 5:
                    timestamp = time.time()
                    wait_second = int(reset - timestamp)
                    if wait_second > 0:
                        print('wait ', wait_second)
                        time.sleep(wait_second + 2)
                        PostWriter('/output/nvd_match_repo_date_all_plat_' + args.index + '_gpt3.pkl').write_pickle(
                            store)
            except:
                print('abnormal no limit')
                time.sleep(60)


            try:
                repo = g.get_repo(one_repo)
            except Exception as e:
                print(e)
                if hasattr(e, 'status') and e.status == 403:
                    time.sleep(2)
                continue

            try:
                rate_limit = g.get_rate_limit()
                remaining = rate_limit.raw_data['core']['remaining']
                reset = rate_limit.raw_data['core']['reset']
                if remaining <= 5:
                    timestamp = time.time()
                    wait_second = int(reset - timestamp)
                    if wait_second > 0:
                        print('wait ', wait_second)
                        time.sleep(wait_second + 2)
                        PostWriter('/output/nvd_match_repo_date_all_plat_' + args.index + '_gpt3.pkl').write_pickle(
                            store)
            except:
                print('abnormal no limit')
                time.sleep(60)

            try:
                commit_list = list(repo.get_commits(since=date_data-datetime.timedelta(days=90), until=date_data))
                if len(commit_list) > 0:
                    commit_list = [(one_ele.sha, one_ele.last_modified) for one_ele in commit_list]
                    store[k].append((repo.full_name, commit_list))
            except Exception as e:
                print(e)
                if hasattr(e, 'status') and e.status == 403:
                    time.sleep(2)
    PostWriter('/output/nvd_match_repo_date_all_plat_' + args.index + '_gpt3f.pkl').write_pickle(store)


def parse_repo_files():
    """
    This function tries to parse file paths and names given the selected commit
    """
    def parse_file_names(tree, repo):
        store = []
        file_store = []
        for one_tree in tree.tree:
            try:
                if one_tree.type == 'tree':

                    try:
                        rate_limit = g.get_rate_limit()
                        remaining = rate_limit.raw_data['core']['remaining']
                        reset = rate_limit.raw_data['core']['reset']
                        if remaining <= 5:
                            PostWriter('/output/nvd_match_repo_files_all_plat_sum_' + args.index + '_gpt3_onlydate1_2.pkl').write_pickle(
                                store_dict)
                            timestamp = time.time()
                            wait_second = int(reset - timestamp)
                            if wait_second <= 0:
                                continue
                            print('wait ', wait_second)
                            time.sleep(wait_second + 2)
                    except:
                        print('abnormal no limit')
                        time.sleep(60)
                    tree_sha = one_tree.sha
                    sub_trees = repo.get_git_tree(tree_sha)
                    tmp_store, tmp_file = parse_file_names(sub_trees, repo)
                    store += [one_tree.path + '/' + one_subtree for one_subtree in tmp_store]
                    file_store += tmp_file
                else:
                    store.append(one_tree.path)
                    file_store.append(one_tree.sha)
            except Exception as e:
                if hasattr(e, 'status') and e.status == 403:
                    time.sleep(2)
                print(e)
        return store, file_store

    parser = argparse.ArgumentParser()
    parser.add_argument("--index", default='0', type=str,)
    parser.add_argument("--git_token", default='your github token', type=str, )
    args = parser.parse_args()

    # format: {(cve_id, repo_name):[repo, commit_sha]}
    file_r = PostLoader().load_pickle('/output/tree_need_parse_all_plat_sum_' + args.index + '_gpt3_onlydate1.pkl')
    g = Github(args.git_token)

    store_dict = {}

    for (k, v) in tqdm.tqdm(file_r):
        try:

            try:
                rate_limit = g.get_rate_limit()
                remaining = rate_limit.raw_data['core']['remaining']
                reset = rate_limit.raw_data['core']['reset']
                if remaining <= 5:
                    PostWriter('/output/nvd_match_repo_files_all_plat_sum_' + args.index + '_gpt3_onlydate1_2.pkl').write_pickle(
                        store_dict)
                    timestamp = time.time()
                    wait_second = int(reset - timestamp)
                    if wait_second <= 0:
                        continue
                    print('wait ', wait_second)
                    time.sleep(wait_second + 2)
            except:
                print('abnormal no limit')
                time.sleep(60)

            repo_full_name = k
            repo = g.get_repo(full_name_or_id=repo_full_name)

            try:
                rate_limit = g.get_rate_limit()
                remaining = rate_limit.raw_data['core']['remaining']
                reset = rate_limit.raw_data['core']['reset']
                if remaining <= 5:
                    PostWriter('/output/nvd_match_repo_files_all_plat_sum_' + args.index + '_gpt3_onlydate1_2.pkl').write_pickle(
                        store_dict)
                    timestamp = time.time()
                    wait_second = int(reset - timestamp)
                    if wait_second <= 0:
                        continue
                    print('wait ', wait_second)
                    time.sleep(wait_second + 2)
            except:
                print('abnormal no limit')
                time.sleep(60)
            trees_main = repo.get_git_tree(v)
            store_result, file_store = parse_file_names(trees_main, repo)

            store_dict[(k, v)] = [store_result, file_store]
        except Exception as e:
            if hasattr(e, 'status') and e.status == 403:
                time.sleep(2)
            print(e)
    PostWriter('/output/nvd_match_repo_files_all_plat_sum_' + args.index + '_gpt3_onlydate1_2.pkl').write_pickle(store_dict)


def parse_content_from_trees():
    """
    This function tries to parse code contents given the file path and name
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("--index", default='0', type=str, )
    parser.add_argument("--git_token", default='your github token', type=str, )
    args = parser.parse_args()

    # format: [[repo_name, sha, [list of file path&name]], ...]
    file_r = PostLoader().load_pickle('/output/gt_need_parse_gpt3_' + args.index + '.pkl')
    store = {}
    file_r = {(one_line[0], one_line[1]): one_line[2] for one_line in file_r}

    store_idx = 0
    sub_count = 0

    g = Github(args.git_token)
    for idx, (k, v) in enumerate(tqdm.tqdm(file_r.items())):
        for one_path in v:
            try:

                try:
                    rate_limit = g.get_rate_limit()
                    remaining = rate_limit.raw_data['core']['remaining']
                    reset = rate_limit.raw_data['core']['reset']
                    if remaining <= 5:
                        timestamp = time.time()
                        wait_second = int(reset - timestamp)
                        if wait_second <= 0:
                            continue
                        print('wait ', wait_second)
                        time.sleep(wait_second + 2)
                except:
                    print('abnormal no limit')
                    time.sleep(60)

                target_repo = g.get_repo(full_name_or_id=k[0])

                try:
                    rate_limit = g.get_rate_limit()
                    remaining = rate_limit.raw_data['core']['remaining']
                    reset = rate_limit.raw_data['core']['reset']
                    if remaining <= 5:
                        timestamp = time.time()
                        wait_second = int(reset - timestamp)
                        if wait_second <= 0:
                            continue
                        print('wait ', wait_second)
                        time.sleep(wait_second + 2)
                except:
                    print('abnormal no limit')
                    time.sleep(60)

                content = target_repo.get_contents(path=one_path, ref=k[1])
                store[(k[0], k[1], one_path)] = content
                sub_count += 1
                if sub_count > 3000:
                    PostWriter('/output/nvd_match_repo_tress_accurate_content_gpt3_all2_' + args.index + '_' + str(
                        store_idx) + '.pkl').write_pickle(
                        store)
                    store = {}
                    store_idx += 1
                    sub_count = 0
            except Exception as e:
                if hasattr(e, 'status') and e.status == 403:
                    time.sleep(2)
                print(one_path)
                print(e)

        # Conditional saving logic
        if sub_count > 0:
            PostWriter('/output/nvd_match_repo_gt_gpt3_all2_' + args.index + '_' + str(
                store_idx) + '.pkl').write_pickle(
                store)
            store = {}
            store_idx += 1
            sub_count = 0
    PostWriter('/output/nvd_match_repo_gt_gpt3_all2_'+args.index+'_'+str(store_idx)+'f.pkl').write_pickle(store)
